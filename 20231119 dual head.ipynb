{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.frame_level import FrameLevelLexicalAccess, \\\n",
    "    LexicalAccessConfig, LexicalAccessDataCollator\n",
    "from src.models.transformer import drop_wav2vec_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_target_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_init(\n",
    "        model_name_or_path,\n",
    "        config, \n",
    "        word_vocabulary, word_representations,\n",
    "        device=\"cpu\"):\n",
    "    def model_init(trial):\n",
    "        encoder = transformers.Wav2Vec2Model.from_pretrained(\n",
    "            model_name_or_path, config=config.encoder_config).to(device)\n",
    "        model = FrameLevelLexicalAccess(\n",
    "            config, word_vocabulary, word_representations,\n",
    "            encoder=encoder).to(device)\n",
    "\n",
    "        model.freeze_feature_extractor()\n",
    "\n",
    "        if hasattr(config, \"drop_layers\"):\n",
    "            model.encoder = drop_wav2vec_layers(model.encoder, config.drop_layers)\n",
    "\n",
    "        if getattr(config, \"reinit_feature_extractor_weights\", False):\n",
    "            model.encoder.feature_extractor.apply(lambda x: model.encoder._init_weights(x))\n",
    "        if getattr(config, \"reinit_encoder_weights\", False):\n",
    "            model.encoder.encoder.apply(lambda x: model.encoder._init_weights(x))\n",
    "\n",
    "        # Freeze all model weights.\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        return model\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\n",
    "    \"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(\"./data/timit_phoneme/\")\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare semantic representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9d35eac52f4d309c7748cb7f9e6c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b587490f7bb048a0a79caab43bd84c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'em\",\n",
       " 'a',\n",
       " 'abbreviate',\n",
       " 'abdomen',\n",
       " 'abides',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'abolish',\n",
       " 'aborigine']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = set()\n",
    "def update_all_words(item):\n",
    "    all_words.update(set(item[\"word_detail\"][\"utterance\"]))\n",
    "    return None\n",
    "dataset.map(update_all_words)\n",
    "\n",
    "all_words = sorted(all_words)\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_representations = np.random.randint(\n",
    "    0, 2, (len(all_words), regressor_target_size))\n",
    "word_representations = torch.tensor(word_representations, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6102, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder_config = transformers.AutoConfig.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\")\n",
    "model_config = LexicalAccessConfig(\n",
    "    encoder_config=encoder_config,\n",
    "    num_labels=tokenizer.vocab_size,\n",
    "    regressor_target_size=regressor_target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "model = transformers.Wav2Vec2ForCTC.from_pretrained(\"charsiu/en_w2v2_ctc_libris_and_cv\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = make_model_init(\n",
    "    \"charsiu/en_w2v2_ctc_libris_and_cv\", model_config,\n",
    "    all_words, word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = LexicalAccessDataCollator(\n",
    "    processor=processor,\n",
    "    model=model_init(None),\n",
    "    padding=True,\n",
    "    num_labels=tokenizer.vocab_size,\n",
    "    regression_target_size=regressor_target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'attention_mask', 'target_mask', 'classifier_labels', 'regressor_targets'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = collator(train_dataset.select(range(2)))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 145])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 145, 42])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.classifier_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255, 42])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.classifier_labels[batch.target_mask == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LexicalAccessOutput(loss=tensor(0.4624, grad_fn=<AddBackward0>), logits=tensor([[[ 0.0461, -0.0082,  0.0013,  ...,  0.0240,  0.0794,  0.0870],\n",
       "         [ 0.0331, -0.0169, -0.0312,  ...,  0.0065,  0.0973,  0.1045],\n",
       "         [ 0.0216,  0.0022, -0.0298,  ...,  0.0186,  0.0992,  0.1192],\n",
       "         ...,\n",
       "         [ 0.0845,  0.0304,  0.0342,  ..., -0.0199,  0.1108,  0.0880],\n",
       "         [ 0.0815,  0.0461,  0.0265,  ..., -0.0231,  0.1007,  0.0762],\n",
       "         [ 0.0775,  0.0114,  0.0233,  ..., -0.0559,  0.1135,  0.0677]],\n",
       "\n",
       "        [[ 0.0515, -0.0086,  0.0080,  ...,  0.0091,  0.0878,  0.0757],\n",
       "         [ 0.0401, -0.0052, -0.0073,  ...,  0.0305,  0.0862,  0.0913],\n",
       "         [ 0.0216, -0.0027, -0.0022,  ...,  0.0187,  0.0945,  0.0952],\n",
       "         ...,\n",
       "         [ 0.0416,  0.0233, -0.0533,  ...,  0.0270,  0.0944,  0.1274],\n",
       "         [ 0.0262,  0.0157, -0.0940,  ...,  0.0487,  0.1132,  0.1303],\n",
       "         [ 0.0377,  0.0161, -0.0760,  ...,  0.0071,  0.0888,  0.1490]]],\n",
       "       grad_fn=<AddBackward0>), semantic=tensor([[[ 0.0695,  0.0705, -0.0747,  ..., -0.0068,  0.0739, -0.0489],\n",
       "         [ 0.0766,  0.0843, -0.0669,  ..., -0.0085,  0.0786, -0.0683],\n",
       "         [ 0.0690,  0.1179, -0.0986,  ..., -0.0070,  0.0795, -0.0538],\n",
       "         ...,\n",
       "         [ 0.0024,  0.1514, -0.0579,  ..., -0.0195,  0.0823, -0.0442],\n",
       "         [-0.0208,  0.1326, -0.0582,  ..., -0.0088,  0.0932, -0.0725],\n",
       "         [-0.0234,  0.1409, -0.0442,  ..., -0.0085,  0.0855, -0.0512]],\n",
       "\n",
       "        [[ 0.0690,  0.0747, -0.0713,  ..., -0.0209,  0.0742, -0.0499],\n",
       "         [ 0.0773,  0.1062, -0.0561,  ..., -0.0268,  0.0829, -0.0664],\n",
       "         [ 0.0886,  0.1293, -0.0647,  ...,  0.0037,  0.0528, -0.0815],\n",
       "         ...,\n",
       "         [ 0.0846,  0.1374, -0.0974,  ...,  0.0134,  0.0705, -0.1001],\n",
       "         [ 0.0954,  0.1218, -0.0905,  ...,  0.0312,  0.1096, -0.0703],\n",
       "         [ 0.0601,  0.1152, -0.1001,  ..., -0.0039,  0.0545, -0.0843]]],\n",
       "       grad_fn=<AddBackward0>), wav2vec2_hidden_states=None, wav2vec2_attentions=None, rnn_hidden_states=tensor([[[ 0.0262,  0.0002,  0.0072,  ..., -0.0027,  0.0234, -0.0253],\n",
       "         [ 0.0514,  0.0184, -0.0019,  ..., -0.0408,  0.0242, -0.0380],\n",
       "         [ 0.0657,  0.0200, -0.0226,  ..., -0.0862,  0.0297, -0.0379],\n",
       "         ...,\n",
       "         [ 0.0326, -0.0096, -0.0050,  ...,  0.0330,  0.0946,  0.0421],\n",
       "         [ 0.0457, -0.0102, -0.0045,  ...,  0.0295,  0.0969,  0.0266],\n",
       "         [ 0.0471, -0.0057, -0.0073,  ...,  0.0355,  0.1011,  0.0261]],\n",
       "\n",
       "        [[ 0.0127,  0.0046,  0.0134,  ..., -0.0044,  0.0216, -0.0022],\n",
       "         [ 0.0353,  0.0075, -0.0095,  ..., -0.0592,  0.0190, -0.0103],\n",
       "         [ 0.0506,  0.0192, -0.0403,  ..., -0.1081,  0.0107, -0.0242],\n",
       "         ...,\n",
       "         [ 0.0682,  0.0233, -0.0247,  ..., -0.1485,  0.0017, -0.0854],\n",
       "         [ 0.0726,  0.0240, -0.0362,  ..., -0.1539,  0.0084, -0.0857],\n",
       "         [ 0.0618,  0.0175, -0.0493,  ..., -0.0992,  0.0310, -0.0782]]],\n",
       "       grad_fn=<TransposeBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(loss_alpha=0.3, **batch)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
