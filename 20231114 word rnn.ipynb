{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.frame_level import FrameLevelRNNClassifier\n",
    "from models.transformer import TilingWordFeatureExtractor2, DataCollator, drop_wav2vec_layers\n",
    "from utils.timit import group_phonetic_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner model and corpus prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "model = transformers.Wav2Vec2ForCTC.from_pretrained(\"charsiu/en_w2v2_ctc_libris_and_cv\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = datasets.load_dataset(\"timit_asr\", data_dir=\"/userdata/jgauthier/data/TIMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO cross-check resulting mappings with cmudict\n",
    "TIMIT_MAPPING = {\n",
    "    'ax': 'AH',\n",
    "    'ax-h': 'AH',\n",
    "    'axr': 'ER',\n",
    "    'dx': 'T',\n",
    "    'el': ['AH', 'L'],\n",
    "    'em': ['AH', 'M'],\n",
    "    'en': ['AH', 'N'],\n",
    "    'eng': ['IH', 'NG'],\n",
    "    'hv': 'HH',\n",
    "    'ix': 'IH',\n",
    "    'nx': ['N', 'T'],\n",
    "    'q': 'T',\n",
    "    'pau': '[SIL]',\n",
    "    'epi': '[SIL]',\n",
    "    'ux': 'UW'\n",
    "}\n",
    "\n",
    "VOCAB = set(tokenizer.get_vocab().keys())\n",
    "for src, tgt in TIMIT_MAPPING.items():\n",
    "    if isinstance(tgt, str):\n",
    "        tgt = [tgt]\n",
    "    for t in tgt:\n",
    "        assert t in VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f82b08dcd86467997673cfb275fe0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f0abb0b9bc4cec9d34dd06d60711df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TIMIT_IGNORE = [\"h#\"]\n",
    "TIMIT_JOIN_CLOSURES = ['bcl', 'dcl', 'gcl', 'kcl', 'pcl', 'tcl']\n",
    "\n",
    "\n",
    "def map_timit_phone_to_cmudict_phoneme(phone):\n",
    "    if phone in TIMIT_IGNORE:\n",
    "        return []\n",
    "    elif phone in TIMIT_MAPPING:\n",
    "        ret = TIMIT_MAPPING[phone]\n",
    "        if isinstance(ret, str):\n",
    "            ret = [ret]\n",
    "        return ret\n",
    "    elif not phone.upper() in VOCAB:\n",
    "        raise ValueError(f\"Invalid phone {phone.upper()}\")\n",
    "    return [phone.upper()]\n",
    "\n",
    "\n",
    "def map_timit_to_cmudict(timit_item):\n",
    "    phonemes = []\n",
    "\n",
    "    i = 0\n",
    "    phonetic_detail = timit_item[\"phonetic_detail\"]\n",
    "    num_phones = len(phonetic_detail[\"start\"])\n",
    "    while i < num_phones:\n",
    "        phone = phonetic_detail[\"utterance\"][i]\n",
    "        start = phonetic_detail[\"start\"][i]\n",
    "        stop = phonetic_detail[\"stop\"][i]\n",
    "\n",
    "        if phone in TIMIT_IGNORE:\n",
    "            i += 1\n",
    "        elif phone in TIMIT_JOIN_CLOSURES:\n",
    "            release_phone = phone[:-len(\"cl\")]\n",
    "            if phonetic_detail[\"utterance\"][i + 1] == release_phone:\n",
    "                phoneme_start = start\n",
    "                phoneme_end = phonetic_detail[\"stop\"][i + 1]\n",
    "                phoneme_label = map_timit_phone_to_cmudict_phoneme(release_phone)\n",
    "\n",
    "                for phoneme in phoneme_label:\n",
    "                    phonemes.append((phoneme_start, phoneme_end, phoneme))\n",
    "                i += 2\n",
    "            else:\n",
    "                phoneme_label = map_timit_phone_to_cmudict_phoneme(release_phone)\n",
    "                for phoneme in phoneme_label:\n",
    "                    phonemes.append((start, stop, phoneme))\n",
    "                i += 1\n",
    "        else:\n",
    "            for phoneme in map_timit_phone_to_cmudict_phoneme(phone):\n",
    "                phonemes.append((start, stop, phoneme))\n",
    "            i += 1\n",
    "\n",
    "    return phonemes\n",
    "\n",
    "\n",
    "def add_phonemic_detail(item):\n",
    "    phonemes = map_timit_to_cmudict(item)\n",
    "    # TODO group by word\n",
    "\n",
    "    starts, stops, utterances = zip(*phonemes)\n",
    "    item[\"phonemic_detail\"] = {\n",
    "        \"start\": starts,\n",
    "        \"stop\": stops,\n",
    "        \"utterance\": utterances\n",
    "    }\n",
    "\n",
    "    return item\n",
    "\n",
    "    \n",
    "corpus = corpus.map(add_phonemic_detail, batched=False, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(processor, drop_phones=None):\n",
    "    corpus = datasets.load_dataset(\"timit_asr\", data_dir=\"/userdata/jgauthier/data/TIMIT\")\n",
    "\n",
    "    corpus = corpus.map(add_phonemic_detail, batched=False)\n",
    "\n",
    "    # Compute phonetic and phonemic details grouped by word span\n",
    "    corpus = corpus.map(group_phonetic_detail, batched=False,\n",
    "                        fn_kwargs=dict(drop_phones=drop_phones))\n",
    "    corpus = corpus.map(group_phonetic_detail, batched=False,\n",
    "                        fn_kwargs=dict(key=\"phonemic_detail\"))\n",
    "    \n",
    "    def prepare_audio(batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        return batch\n",
    "    corpus = corpus.map(prepare_audio)\n",
    "\n",
    "    twfe = TilingWordFeatureExtractor2(tokenizer, item_key=\"word_phonemic_detail\")\n",
    "    def add_features(example):\n",
    "        example[\"phone_targets\"] = twfe(example)\n",
    "        return example\n",
    "    corpus = corpus.map(add_features, load_from_cache_file=False)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(processor, corpus_path=\"timit_phoneme_corpus\"):\n",
    "    if not Path(corpus_path).exists():\n",
    "        corpus = prepare_corpus(processor)\n",
    "        corpus.save_to_disk(corpus_path)\n",
    "    else:\n",
    "        corpus = datasets.load_from_disk(corpus_path)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_split = corpus[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "train_corpus, eval_corpus = corpora_split[\"train\"], corpora_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_init(model_name_or_path, config, device=\"cpu\"):\n",
    "    def model_init(trial):\n",
    "        model = FrameLevelRNNClassifier.from_pretrained(\n",
    "            model_name_or_path, config=config).to(device)\n",
    "\n",
    "        model.freeze_feature_extractor()\n",
    "\n",
    "        if hasattr(config, \"drop_layers\"):\n",
    "            model.wav2vec2 = drop_wav2vec_layers(model.wav2vec2, config.drop_layers)\n",
    "\n",
    "        # Freeze all model weights.\n",
    "        for param in model.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        return model\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: transformers.EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    label_mask, labels = p.label_ids\n",
    "\n",
    "    def evaluate_label(j):\n",
    "        preds_j = preds[:, :, j]\n",
    "        labels_j = labels[:, :, j]\n",
    "\n",
    "        preds_j = preds_j[label_mask == 1]\n",
    "        labels_j = labels_j[label_mask == 1]\n",
    "        if labels_j.std() == 0:\n",
    "            # Only one class. Quit\n",
    "            return None\n",
    "        return roc_auc_score(labels_j, preds_j)\n",
    "\n",
    "    roc_auc_scores = [evaluate_label(j) for j in range(preds.shape[-1])]\n",
    "    return {\"roc_auc\": np.mean([score for score in roc_auc_scores if score is not None])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"charsiu/en_w2v2_ctc_libris_and_cv\"\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=tokenizer.vocab_size)\n",
    "setattr(config, \"classifier_bias\", True)\n",
    "setattr(config, \"rnn_hidden_size\", 128)\n",
    "setattr(config, \"rnn_num_layers\", 2)\n",
    "setattr(config, \"drop_layers\", 6)\n",
    "model_init = make_model_init(model_name_or_path, config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FrameLevelRNNClassifier were not initialized from the model checkpoint at charsiu/en_w2v2_ctc_libris_and_cv and are newly initialized: ['rnn.bias_ih_l1', 'classifier.weight', 'rnn.bias_hh_l0', 'classifier.bias', 'rnn.weight_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.weight_hh_l1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "coll = DataCollator(processor=processor, model=model_init(None), padding=True,\n",
    "                    num_labels=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of FrameLevelRNNClassifier were not initialized from the model checkpoint at charsiu/en_w2v2_ctc_libris_and_cv and are newly initialized: ['rnn.bias_ih_l1', 'classifier.weight', 'rnn.bias_hh_l0', 'classifier.bias', 'rnn.weight_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.weight_hh_l1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"out/rnn/rnn{config.rnn_num_layers}_hidden{config.rnn_hidden_size}_drop{config.drop_layers}\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=50,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=2,\n",
    "    learning_rate=1e-2,\n",
    "    save_total_limit=5,\n",
    "    use_cpu=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"label_mask\", \"labels\"],\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None, model_init=model_init,\n",
    "    data_collator=coll,\n",
    "    args=training_args,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_corpus,\n",
    "    eval_dataset=eval_corpus,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FrameLevelRNNClassifier were not initialized from the model checkpoint at charsiu/en_w2v2_ctc_libris_and_cv and are newly initialized: ['rnn.bias_ih_l1', 'classifier.weight', 'rnn.bias_hh_l0', 'classifier.bias', 'rnn.weight_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.weight_hh_l1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6697, 'learning_rate': 0.009996923076923077, 'epoch': 0.02}\n",
      "{'loss': 0.3622, 'learning_rate': 0.009993846153846155, 'epoch': 0.03}\n",
      "{'loss': 0.2862, 'learning_rate': 0.009990769230769232, 'epoch': 0.05}\n",
      "{'loss': 0.3042, 'learning_rate': 0.009987692307692307, 'epoch': 0.06}\n",
      "{'loss': 0.297, 'learning_rate': 0.009984615384615385, 'epoch': 0.08}\n",
      "{'loss': 0.2864, 'learning_rate': 0.009981538461538462, 'epoch': 0.09}\n",
      "{'loss': 0.2755, 'learning_rate': 0.009978461538461538, 'epoch': 0.11}\n",
      "{'loss': 0.2858, 'learning_rate': 0.009975384615384615, 'epoch': 0.12}\n",
      "{'loss': 0.2848, 'learning_rate': 0.009972307692307694, 'epoch': 0.14}\n",
      "{'loss': 0.2698, 'learning_rate': 0.00996923076923077, 'epoch': 0.15}\n",
      "{'loss': 0.2767, 'learning_rate': 0.009966153846153845, 'epoch': 0.17}\n",
      "{'loss': 0.2667, 'learning_rate': 0.009963076923076923, 'epoch': 0.18}\n",
      "{'loss': 0.2752, 'learning_rate': 0.00996, 'epoch': 0.2}\n",
      "{'loss': 0.265, 'learning_rate': 0.009956923076923077, 'epoch': 0.22}\n",
      "{'loss': 0.2801, 'learning_rate': 0.009953846153846155, 'epoch': 0.23}\n",
      "{'loss': 0.2704, 'learning_rate': 0.009950769230769232, 'epoch': 0.25}\n",
      "{'loss': 0.274, 'learning_rate': 0.009947692307692308, 'epoch': 0.26}\n",
      "{'loss': 0.2626, 'learning_rate': 0.009944615384615385, 'epoch': 0.28}\n",
      "{'loss': 0.2747, 'learning_rate': 0.009941538461538462, 'epoch': 0.29}\n",
      "{'loss': 0.2738, 'learning_rate': 0.009938461538461538, 'epoch': 0.31}\n",
      "{'loss': 0.2775, 'learning_rate': 0.009935384615384617, 'epoch': 0.32}\n",
      "{'loss': 0.2644, 'learning_rate': 0.009932307692307693, 'epoch': 0.34}\n",
      "{'loss': 0.273, 'learning_rate': 0.00992923076923077, 'epoch': 0.35}\n",
      "{'loss': 0.2602, 'learning_rate': 0.009926153846153847, 'epoch': 0.37}\n",
      "{'loss': 0.2686, 'learning_rate': 0.009923076923076923, 'epoch': 0.38}\n",
      "{'eval_loss': 0.26618024706840515, 'eval_roc_auc': 0.6198665890185966, 'eval_runtime': 13.7037, 'eval_samples_per_second': 33.713, 'eval_steps_per_second': 4.232, 'epoch': 0.38}\n",
      "{'loss': 0.2755, 'learning_rate': 0.00992, 'epoch': 0.4}\n",
      "{'loss': 0.2669, 'learning_rate': 0.009916923076923077, 'epoch': 0.42}\n",
      "{'loss': 0.268, 'learning_rate': 0.009913846153846155, 'epoch': 0.43}\n",
      "{'loss': 0.2598, 'learning_rate': 0.009910769230769232, 'epoch': 0.45}\n",
      "{'loss': 0.263, 'learning_rate': 0.009907692307692308, 'epoch': 0.46}\n",
      "{'loss': 0.2524, 'learning_rate': 0.009904615384615385, 'epoch': 0.48}\n",
      "{'loss': 0.252, 'learning_rate': 0.009901538461538462, 'epoch': 0.49}\n",
      "{'loss': 0.2696, 'learning_rate': 0.009898461538461538, 'epoch': 0.51}\n",
      "{'loss': 0.2633, 'learning_rate': 0.009895384615384617, 'epoch': 0.52}\n",
      "{'loss': 0.2674, 'learning_rate': 0.009892307692307693, 'epoch': 0.54}\n",
      "{'loss': 0.2638, 'learning_rate': 0.00988923076923077, 'epoch': 0.55}\n",
      "{'loss': 0.2517, 'learning_rate': 0.009886153846153847, 'epoch': 0.57}\n",
      "{'loss': 0.2508, 'learning_rate': 0.009883076923076923, 'epoch': 0.58}\n",
      "{'loss': 0.2677, 'learning_rate': 0.00988, 'epoch': 0.6}\n",
      "{'loss': 0.2529, 'learning_rate': 0.009876923076923077, 'epoch': 0.62}\n",
      "{'loss': 0.25, 'learning_rate': 0.009873846153846155, 'epoch': 0.63}\n",
      "{'loss': 0.2432, 'learning_rate': 0.009870769230769232, 'epoch': 0.65}\n",
      "{'loss': 0.2691, 'learning_rate': 0.009867692307692308, 'epoch': 0.66}\n",
      "{'loss': 0.256, 'learning_rate': 0.009864615384615385, 'epoch': 0.68}\n",
      "{'loss': 0.2618, 'learning_rate': 0.009861538461538462, 'epoch': 0.69}\n",
      "{'loss': 0.2601, 'learning_rate': 0.009858461538461538, 'epoch': 0.71}\n",
      "{'loss': 0.255, 'learning_rate': 0.009855384615384617, 'epoch': 0.72}\n",
      "{'loss': 0.2591, 'learning_rate': 0.009852307692307693, 'epoch': 0.74}\n",
      "{'loss': 0.26, 'learning_rate': 0.00984923076923077, 'epoch': 0.75}\n",
      "{'loss': 0.25, 'learning_rate': 0.009846153846153846, 'epoch': 0.77}\n",
      "{'eval_loss': 0.25143757462501526, 'eval_roc_auc': 0.6704771115895358, 'eval_runtime': 13.7881, 'eval_samples_per_second': 33.507, 'eval_steps_per_second': 4.207, 'epoch': 0.77}\n",
      "{'loss': 0.259, 'learning_rate': 0.009843076923076923, 'epoch': 0.78}\n",
      "{'loss': 0.2565, 'learning_rate': 0.00984, 'epoch': 0.8}\n",
      "{'loss': 0.2565, 'learning_rate': 0.009836923076923076, 'epoch': 0.82}\n",
      "{'loss': 0.2462, 'learning_rate': 0.009833846153846155, 'epoch': 0.83}\n",
      "{'loss': 0.2519, 'learning_rate': 0.009830769230769231, 'epoch': 0.85}\n",
      "{'loss': 0.2559, 'learning_rate': 0.009827692307692308, 'epoch': 0.86}\n",
      "{'loss': 0.2524, 'learning_rate': 0.009824615384615385, 'epoch': 0.88}\n",
      "{'loss': 0.2512, 'learning_rate': 0.009821538461538461, 'epoch': 0.89}\n",
      "{'loss': 0.2529, 'learning_rate': 0.009818461538461538, 'epoch': 0.91}\n",
      "{'loss': 0.2573, 'learning_rate': 0.009815384615384616, 'epoch': 0.92}\n",
      "{'loss': 0.2468, 'learning_rate': 0.009812307692307693, 'epoch': 0.94}\n",
      "{'loss': 0.259, 'learning_rate': 0.00980923076923077, 'epoch': 0.95}\n",
      "{'loss': 0.2548, 'learning_rate': 0.009806153846153846, 'epoch': 0.97}\n",
      "{'loss': 0.2528, 'learning_rate': 0.009803076923076923, 'epoch': 0.98}\n",
      "{'loss': 0.2403, 'learning_rate': 0.0098, 'epoch': 1.0}\n",
      "{'loss': 0.2514, 'learning_rate': 0.009796923076923076, 'epoch': 1.02}\n",
      "{'loss': 0.2586, 'learning_rate': 0.009793846153846155, 'epoch': 1.03}\n",
      "{'loss': 0.249, 'learning_rate': 0.009790769230769231, 'epoch': 1.05}\n",
      "{'loss': 0.2538, 'learning_rate': 0.009787692307692308, 'epoch': 1.06}\n",
      "{'loss': 0.2597, 'learning_rate': 0.009784615384615385, 'epoch': 1.08}\n",
      "{'loss': 0.2469, 'learning_rate': 0.009781538461538461, 'epoch': 1.09}\n",
      "{'loss': 0.248, 'learning_rate': 0.009778461538461538, 'epoch': 1.11}\n",
      "{'loss': 0.2509, 'learning_rate': 0.009775384615384616, 'epoch': 1.12}\n",
      "{'loss': 0.2577, 'learning_rate': 0.009772307692307693, 'epoch': 1.14}\n",
      "{'loss': 0.262, 'learning_rate': 0.00976923076923077, 'epoch': 1.15}\n",
      "{'eval_loss': 0.2485247254371643, 'eval_roc_auc': 0.6904089814304534, 'eval_runtime': 13.1053, 'eval_samples_per_second': 35.253, 'eval_steps_per_second': 4.426, 'epoch': 1.15}\n",
      "{'loss': 0.256, 'learning_rate': 0.009766153846153846, 'epoch': 1.17}\n",
      "{'loss': 0.2598, 'learning_rate': 0.009763076923076923, 'epoch': 1.18}\n",
      "{'loss': 0.2535, 'learning_rate': 0.00976, 'epoch': 1.2}\n",
      "{'loss': 0.2533, 'learning_rate': 0.009756923076923078, 'epoch': 1.22}\n",
      "{'loss': 0.2518, 'learning_rate': 0.009753846153846155, 'epoch': 1.23}\n",
      "{'loss': 0.2471, 'learning_rate': 0.009750769230769231, 'epoch': 1.25}\n",
      "{'loss': 0.261, 'learning_rate': 0.009747692307692308, 'epoch': 1.26}\n",
      "{'loss': 0.2463, 'learning_rate': 0.009744615384615385, 'epoch': 1.28}\n",
      "{'loss': 0.2497, 'learning_rate': 0.009741538461538461, 'epoch': 1.29}\n",
      "{'loss': 0.246, 'learning_rate': 0.009738461538461538, 'epoch': 1.31}\n",
      "{'loss': 0.2513, 'learning_rate': 0.009735384615384616, 'epoch': 1.32}\n",
      "{'loss': 0.2486, 'learning_rate': 0.009732307692307693, 'epoch': 1.34}\n",
      "{'loss': 0.2417, 'learning_rate': 0.00972923076923077, 'epoch': 1.35}\n",
      "{'loss': 0.2507, 'learning_rate': 0.009726153846153846, 'epoch': 1.37}\n",
      "{'loss': 0.2448, 'learning_rate': 0.009723076923076923, 'epoch': 1.38}\n",
      "{'loss': 0.2353, 'learning_rate': 0.00972, 'epoch': 1.4}\n",
      "{'loss': 0.2424, 'learning_rate': 0.009716923076923078, 'epoch': 1.42}\n",
      "{'loss': 0.2518, 'learning_rate': 0.009713846153846155, 'epoch': 1.43}\n",
      "{'loss': 0.2332, 'learning_rate': 0.009710769230769231, 'epoch': 1.45}\n",
      "{'loss': 0.2524, 'learning_rate': 0.009707692307692308, 'epoch': 1.46}\n",
      "{'loss': 0.2431, 'learning_rate': 0.009704615384615384, 'epoch': 1.48}\n",
      "{'loss': 0.2528, 'learning_rate': 0.009701538461538461, 'epoch': 1.49}\n",
      "{'loss': 0.256, 'learning_rate': 0.009698461538461538, 'epoch': 1.51}\n",
      "{'loss': 0.2384, 'learning_rate': 0.009695384615384616, 'epoch': 1.52}\n",
      "{'loss': 0.2401, 'learning_rate': 0.009692307692307693, 'epoch': 1.54}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
