{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare state space trajectories for a lexical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a word-level equivalence dataset regardless of model, so that we can look up cohort facts\n",
    "equiv_dataset_path = \"data/timit_equiv_phoneme_within_word_prefix_6_1.pkl\"\n",
    "timit_corpus_path = \"data/timit_syllables\"\n",
    "\n",
    "out = \"out/state_space_specs/all_words.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(equiv_dataset_path, \"rb\") as f:\n",
    "    equiv_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_corpus = datasets.load_from_disk(timit_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(type(label) == tuple for label in equiv_dataset.class_labels), \"Assumes dataset with word prefix labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare cohort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_frames_by_item = equiv_dataset.hidden_state_dataset.frames_by_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_spans_by_word = defaultdict(list)\n",
    "cuts_df = []\n",
    "\n",
    "def process_item(item, idx):\n",
    "    # How many frames do we have stored for this item?\n",
    "    start_frame, stop_frame = equiv_frames_by_item[idx]\n",
    "    num_frames = stop_frame - start_frame\n",
    "\n",
    "    compression_ratio = num_frames / len(item[\"input_values\"])\n",
    "\n",
    "    for i, word_detail in enumerate(item[\"word_syllable_detail\"]):\n",
    "        if not word_detail:\n",
    "            continue\n",
    "\n",
    "        word_start_frame = start_frame + int(word_detail[0][\"start\"] * compression_ratio)\n",
    "        word_stop_frame = start_frame + int(word_detail[-1][\"stop\"] * compression_ratio)\n",
    "        word = item[\"word_detail\"][\"utterance\"][i]\n",
    "\n",
    "        instance_idx = len(frame_spans_by_word[word])\n",
    "        frame_spans_by_word[word].append((word_start_frame, word_stop_frame))\n",
    "\n",
    "        for syllable in word_detail:\n",
    "            cuts_df.append({\n",
    "                \"label\": word,\n",
    "                \"instance_idx\": instance_idx,\n",
    "                \"level\": \"syllable\",\n",
    "                \"description\": tuple(syllable[\"phones\"]),\n",
    "                \"onset_frame_idx\": start_frame + int(syllable[\"start\"] * compression_ratio),\n",
    "                \"offset_frame_idx\": start_frame + int(syllable[\"stop\"] * compression_ratio),\n",
    "                \"item_idx\": idx,\n",
    "            })\n",
    "\n",
    "        for phoneme in item[\"word_phonemic_detail\"][i]:\n",
    "            cuts_df.append({\n",
    "                \"label\": word,\n",
    "                \"instance_idx\": instance_idx,\n",
    "                \"level\": \"phoneme\",\n",
    "                \"description\": phoneme[\"phone\"],\n",
    "                \"onset_frame_idx\": start_frame + int(phoneme[\"start\"] * compression_ratio),\n",
    "                \"offset_frame_idx\": start_frame + int(phoneme[\"stop\"] * compression_ratio),\n",
    "                \"item_idx\": idx,\n",
    "            })\n",
    "\n",
    "timit_corpus[\"train\"].map(process_item, with_indices=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: we should have Q assignments for the final frame\n",
    "Q_assignments = {word: [equiv_dataset.Q[end].item() for start, end in spans]\n",
    "                 for word, spans in frame_spans_by_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_assignments_flat = np.array(list(itertools.chain.from_iterable(Q_assignments.values())))\n",
    "(Q_assignments_flat >= 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(frame_spans_by_word.keys())\n",
    "spans = list(frame_spans_by_word.values())\n",
    "\n",
    "spec = StateSpaceAnalysisSpec(\n",
    "    total_num_frames=equiv_dataset.hidden_state_dataset.num_frames,\n",
    "    labels=words,\n",
    "    target_frame_spans=spans,\n",
    "    cuts=pd.DataFrame(cuts_df).set_index([\"label\", \"instance_idx\", \"level\"]).sort_index(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out, \"wb\") as f:\n",
    "    pickle.dump(spec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find word cohorts with interesting overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timit_word_to_phon = {}\n",
    "\n",
    "def process_item(item):\n",
    "    for word, word_phons in zip(item[\"word_detail\"][\"utterance\"], item[\"word_phonemic_detail\"]):\n",
    "        if len(word_phons) == 0:\n",
    "            continue\n",
    "\n",
    "        timit_word_to_phon[word] = tuple(phone[\"phone\"] for phone in word_phons)\n",
    "timit_corpus.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "shared_suffixes, shared_suffix_words = Counter(), defaultdict(set)\n",
    "for w1, w2 in itertools.combinations(timit_word_to_phon.keys(), 2):\n",
    "    phons1, phons2 = timit_word_to_phon[w1], timit_word_to_phon[w2]\n",
    "    if len(phons1) > k and len(phons2) > k and phons1[-k:] == phons2[-k:]:\n",
    "        shared_suffixes[phons1[-k:]] += 1\n",
    "        shared_suffix_words[phons1[-k:]].add(w1)\n",
    "        shared_suffix_words[phons1[-k:]].add(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(key, shared_suffix_words[key]) for key, count in shared_suffixes.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_overlap_words = set(itertools.chain.from_iterable([shared_suffix_words[key] for key, count in shared_suffixes.most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "shared_prefixes, shared_prefix_words = Counter(), defaultdict(set)\n",
    "for w1, w2 in itertools.combinations(timit_word_to_phon.keys(), 2):\n",
    "    phons1, phons2 = timit_word_to_phon[w1], timit_word_to_phon[w2]\n",
    "    if len(phons1) > k and len(phons2) > k and phons1[:k] == phons2[:k]:\n",
    "        shared_prefixes[phons1[:k]] += 1\n",
    "        shared_prefix_words[phons1[:k]].add(w1)\n",
    "        shared_prefix_words[phons1[:k]].add(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_overlap_words = set(itertools.chain.from_iterable([shared_prefix_words[key] for key, count in shared_prefixes.most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_overlap_words = suffix_overlap_words & prefix_overlap_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_overlap_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source some prefix overlaps and suffix overlaps for each case\n",
    "complex_cohort_set = {word: (list(shared_prefix_words[timit_word_to_phon[word][:k]]),\n",
    "                             list(shared_suffix_words[timit_word_to_phon[word][-k:]]))\n",
    "                      for word in multiple_overlap_words}\n",
    "complex_cohort_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"complex_cohort_set.json\", \"w\") as f:\n",
    "    json.dump(complex_cohort_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
