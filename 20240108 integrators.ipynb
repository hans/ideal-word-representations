{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0148, -0.0528, -0.0368,  0.0270]])\n",
      "tensor([[-0.0148, -0.0528, -0.0368,  0.0270]])\n",
      "//\n",
      "tensor([[-0.0667, -0.0486, -0.0589,  0.0061]])\n",
      "tensor([[-0.0667, -0.0486, -0.0589,  0.0061]])\n",
      "//\n",
      "tensor([[-0.0149, -0.0511, -0.0373,  0.0261]])\n",
      "tensor([[-0.0149, -0.0511, -0.0373,  0.0261]])\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # TODO padding is on the LEFT here, make sure RNN reflects this\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ContrastiveEmbeddingObjective(nn.Module):\n",
    "    def __init__(self, tau=0.1):\n",
    "        super(ContrastiveEmbeddingObjective, self).__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, embeddings, pos_embeddings, neg_embeddings):\n",
    "        pos_dist = Fn.cosine_similarity(embeddings, pos_embeddings, dim=1)\n",
    "        neg_dist = Fn.cosine_similarity(embeddings, neg_embeddings, dim=1)\n",
    "\n",
    "        pos_loss = -torch.log(torch.exp(pos_dist / self.tau)).mean()\n",
    "        neg_loss = -torch.log(torch.exp(-neg_dist / self.tau)).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "class ContrastiveEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, tau=0.1):\n",
    "        super(ContrastiveEmbeddingModel, self).__init__()\n",
    "        self.rnn = RNNModel(input_dim, hidden_dim, output_dim)\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, batch):\n",
    "        example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths = batch\n",
    "        embeddings, pos_embeddings, neg_embeddings = self.compute_batch_embeddings(example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths)\n",
    "        return ContrastiveEmbeddingObjective(tau=self.tau)(embeddings, pos_embeddings, neg_embeddings)\n",
    "        \n",
    "    def compute_batch_embeddings(self, example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths):\n",
    "        # Compute embeddings\n",
    "        embeddings = self.rnn(example_batch, example_lengths)\n",
    "        pos_embeddings = self.rnn(pos_batch, pos_lengths)\n",
    "        neg_embeddings = self.rnn(neg_batch, neg_lengths)\n",
    "\n",
    "        # Gather final embedding of each sequence\n",
    "        embeddings = torch.gather(embeddings, 1, (example_lengths - 1).reshape(-1, 1, 1).expand(-1, 1, embeddings.shape[-1])).squeeze(1)\n",
    "        pos_embeddings = torch.gather(pos_embeddings, 1, (pos_lengths - 1).reshape(-1, 1, 1).expand(-1, 1, pos_embeddings.shape[-1])).squeeze(1)\n",
    "        neg_embeddings = torch.gather(neg_embeddings, 1, (neg_lengths - 1).reshape(-1, 1, 1).expand(-1, 1, neg_embeddings.shape[-1])).squeeze(1)\n",
    "\n",
    "        return embeddings, pos_embeddings, neg_embeddings\n",
    "\n",
    "\n",
    "def get_sequence(F, start_index, end_index, max_length):\n",
    "    if end_index - start_index + 1 > max_length:\n",
    "        start_index = end_index - max_length + 1\n",
    "    sequence = F[start_index:end_index + 1]\n",
    "    \n",
    "    # Pad on right if necessary\n",
    "    if len(sequence) < max_length:\n",
    "        pad_size = max_length - len(sequence)\n",
    "        padding = torch.zeros(pad_size, F.shape[1])\n",
    "        sequence = torch.cat((sequence, padding), dim=0)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def prepare_batches(F, Q, S, max_length, batch_size=32):\n",
    "    dataset = []\n",
    "    assert F.shape[0] == Q.shape[0] == S.shape[0]\n",
    "    n_F = F.size(0)\n",
    "\n",
    "    lengths = torch.arange(n_F) - S\n",
    "    lengths = torch.minimum(lengths, torch.tensor(max_length))\n",
    "    # TODO this is just a hack\n",
    "    lengths[lengths == 0] = 1\n",
    "\n",
    "    for i in range(n_F):\n",
    "        pos_indices = (Q == Q[i]).nonzero(as_tuple=True)[0]\n",
    "        neg_indices = (Q != Q[i]).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(pos_indices) > 1 and len(neg_indices) > 0:\n",
    "            pos_indices = pos_indices[pos_indices != i]\n",
    "            pos_idx = random.choice(pos_indices)\n",
    "            neg_idx = random.choice(neg_indices)\n",
    "\n",
    "            # Extract sequences\n",
    "            example_seq = get_sequence(F, S[i], i, max_length)\n",
    "            pos_seq = get_sequence(F, S[pos_idx], pos_idx, max_length)\n",
    "            neg_seq = get_sequence(F, S[neg_idx], neg_idx, max_length)\n",
    "\n",
    "            dataset.append((example_seq, lengths[i],\n",
    "                            pos_seq, lengths[pos_idx],\n",
    "                            neg_seq, lengths[neg_idx]))\n",
    "\n",
    "    return DataLoader(TensorDataset(\n",
    "        # example frames and lengths\n",
    "        torch.stack([x[0] for x in dataset]), \n",
    "        torch.stack([x[1] for x in dataset]), \n",
    "\n",
    "        # positive frames and lengths\n",
    "        torch.stack([x[2] for x in dataset]),\n",
    "        torch.stack([x[3] for x in dataset]),\n",
    "\n",
    "        # negative frames and lengths\n",
    "        torch.stack([x[4] for x in dataset]),\n",
    "        torch.stack([x[5] for x in dataset])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def compute_batched_rnn_loss(model, data_loader, tau=0.1):\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        loss = model(batch)\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / total_batches\n",
    "\n",
    "\n",
    "# Example usage\n",
    "n_F, d = 100, 4  # Example dimensions\n",
    "F = torch.randn(n_F, d) * 3  # Random frame features\n",
    "Q = torch.randint(0, 10, (n_F,))  # Random frame matches\n",
    "S = torch.maximum(torch.tensor(0), torch.arange(n_F) - torch.randint(1, 10, (n_F,)))  # Random span indices\n",
    "max_length = 20  # Maximum sequence length for RNN\n",
    "\n",
    "# rnn_model = RNNModel(input_dim=d, hidden_dim=256, output_dim=d)\n",
    "model = ContrastiveEmbeddingModel(input_dim=d, hidden_dim=256, output_dim=d, tau=0.1)\n",
    "# data_loader = prepare_batches(F, Q, S, max_length, batch_size=32)\n",
    "# loss = compute_batched_rnn_loss(model, data_loader)\n",
    "# print(loss)\n",
    "\n",
    "# Build a test batch\n",
    "sample_idx, pos_sample_idx, neg_sample_idx = 37, 23, 85\n",
    "sample_length, pos_sample_length, neg_sample_length = 10, 8, 12\n",
    "example_batch = get_sequence(F, S[sample_idx], sample_idx, max_length).unsqueeze(0)\n",
    "pos_sample_batch = get_sequence(F, S[pos_sample_idx], pos_sample_idx, max_length).unsqueeze(0)\n",
    "neg_sample_batch = get_sequence(F, S[neg_sample_idx], neg_sample_idx, max_length).unsqueeze(0)\n",
    "example_lengths = torch.tensor([sample_length])\n",
    "pos_sample_lengths = torch.tensor([pos_sample_length])\n",
    "neg_sample_lengths = torch.tensor([neg_sample_length])\n",
    "with torch.no_grad():\n",
    "    embeddings, pos_embeddings, neg_embeddings = model.compute_batch_embeddings(example_batch, example_lengths, pos_sample_batch, pos_sample_lengths, neg_sample_batch, neg_sample_lengths)\n",
    "# Manually compute embeddings\n",
    "def compute_embedding_single(model, x, length):\n",
    "    return model.rnn.fc(model.rnn.rnn(x[:, :length, :])[0][:, -1, :])\n",
    "with torch.no_grad():\n",
    "    embeddings_manual = compute_embedding_single(model, example_batch, example_lengths)\n",
    "    pos_embeddings_manual = compute_embedding_single(model, pos_sample_batch, pos_sample_lengths)\n",
    "    neg_embeddings_manual = compute_embedding_single(model, neg_sample_batch, neg_sample_lengths)\n",
    "\n",
    "    # Check that the embeddings are the same\n",
    "    print(embeddings)\n",
    "    print(embeddings_manual)\n",
    "    print(\"//\")\n",
    "    print(pos_embeddings)\n",
    "    print(pos_embeddings_manual)\n",
    "    print(\"//\")\n",
    "    print(neg_embeddings)\n",
    "    print(neg_embeddings_manual)\n",
    "    torch.testing.assert_close(embeddings, embeddings_manual)\n",
    "    torch.testing.assert_close(pos_embeddings, pos_embeddings_manual)\n",
    "    torch.testing.assert_close(neg_embeddings, neg_embeddings_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.timit import load_or_prepare_timit_corpus\n",
    "from src.models.frame_level import LexicalAccessDataCollator\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = transformers.Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_or_prepare_timit_corpus(\"data/timit_phoneme\", \"data/timit_raw\",\n",
    "                                       processor)\n",
    "\n",
    "def add_indices(item, idx):\n",
    "    item[\"idx\"] = idx\n",
    "    return item\n",
    "dataset = dataset.map(add_indices, batched=True, batch_size=2000, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237fba9351374da2ab6e739c7a7d02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute frame-level encodings for each example\n",
    "flat_idxs = []\n",
    "frame_states = []\n",
    "\n",
    "def collate(features):\n",
    "    input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "    # For classification\n",
    "    # label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "    ret = processor.pad(\n",
    "        input_features,\n",
    "        padding=True,\n",
    "        max_length=None,\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    if \"idx\" in features[0]:\n",
    "        ret[\"idx\"] = torch.tensor([feature[\"idx\"] for feature in features])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "dev_dataset = dataset[\"train\"].select(range(5))\n",
    "dataloader = DataLoader(dev_dataset, batch_size=8, collate_fn=collate, shuffle=False)\n",
    "\n",
    "# Collect an index of frames in the dataset by phoneme within matched word type.\n",
    "# We will use this to equivalence-class the model frames later on.\n",
    "phoneme_within_word: dict[tuple[str, int], list[tuple[int, int]]] = {}\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    idxs = batch.pop(\"idx\")\n",
    "    with torch.no_grad():\n",
    "        output = model(output_hidden_states=True, **batch)\n",
    "\n",
    "    # num_layers * batch_size * sequence_length * hidden_size\n",
    "    batch_hidden = torch.stack(output.hidden_states)\n",
    "    \n",
    "    # Extract just non-padding values for each example and concatenate\n",
    "    batch_num_samples = batch.attention_mask.sum(1)\n",
    "    batch_num_frames = model._get_feat_extract_output_lengths(batch_num_samples).tolist()\n",
    "    \n",
    "    for i, (idx, num_frames) in enumerate(zip(idxs, batch_num_frames)):\n",
    "        frame_states.append(batch_hidden[:, i, :num_frames, :])\n",
    "        flat_idxs.extend([(idx, i) for i in range(num_frames)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file',\n",
       " 'audio',\n",
       " 'text',\n",
       " 'phonetic_detail',\n",
       " 'word_detail',\n",
       " 'dialect_region',\n",
       " 'sentence_type',\n",
       " 'speaker_id',\n",
       " 'id',\n",
       " 'phonemic_detail',\n",
       " 'word_phonetic_detail',\n",
       " 'word_phonemic_detail',\n",
       " 'input_values',\n",
       " 'phone_targets',\n",
       " 'idx']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dev_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'phone': 'SH', 'start': 3050, 'stop': 4559},\n",
       "  {'phone': 'IH', 'start': 4559, 'stop': 5723}],\n",
       " [{'phone': 'HH', 'start': 5723, 'stop': 6642},\n",
       "  {'phone': 'EH', 'start': 6642, 'stop': 8772},\n",
       "  {'phone': 'D', 'start': 8772, 'stop': 9190},\n",
       "  {'phone': 'JH', 'start': 9190, 'stop': 10337}],\n",
       " [{'phone': 'JH', 'start': 9190, 'stop': 10337},\n",
       "  {'phone': 'IH', 'start': 10337, 'stop': 11517}],\n",
       " [{'phone': 'D', 'start': 11517, 'stop': 12640},\n",
       "  {'phone': 'AH', 'start': 12640, 'stop': 14714},\n",
       "  {'phone': 'K', 'start': 14714, 'stop': 16334}],\n",
       " [{'phone': 'S', 'start': 16334, 'stop': 18088},\n",
       "  {'phone': 'UW', 'start': 18088, 'stop': 20417},\n",
       "  {'phone': 'T', 'start': 20417, 'stop': 21199}],\n",
       " [{'phone': 'AH', 'start': 21199, 'stop': 22560},\n",
       "  {'phone': 'N', 'start': 21199, 'stop': 22560}],\n",
       " [{'phone': 'G', 'start': 22560, 'stop': 23271},\n",
       "  {'phone': 'R', 'start': 23271, 'stop': 24229},\n",
       "  {'phone': 'IH', 'start': 24229, 'stop': 25566},\n",
       "  {'phone': 'S', 'start': 25566, 'stop': 27156},\n",
       "  {'phone': 'IH', 'start': 27156, 'stop': 28064}],\n",
       " [{'phone': 'W', 'start': 28064, 'stop': 29660},\n",
       "  {'phone': 'AO', 'start': 29660, 'stop': 31719},\n",
       "  {'phone': 'SH', 'start': 31719, 'stop': 33360}],\n",
       " [{'phone': 'W', 'start': 33754, 'stop': 34715},\n",
       "  {'phone': 'AO', 'start': 34715, 'stop': 36080},\n",
       "  {'phone': 'T', 'start': 36080, 'stop': 36326},\n",
       "  {'phone': 'ER', 'start': 36326, 'stop': 37556}],\n",
       " [{'phone': 'AO', 'start': 37556, 'stop': 39561},\n",
       "  {'phone': 'L', 'start': 39561, 'stop': 40313}],\n",
       " [{'phone': 'Y', 'start': 40313, 'stop': 42059},\n",
       "  {'phone': 'IH', 'start': 42059, 'stop': 43479},\n",
       "  {'phone': 'ER', 'start': 43479, 'stop': 44586}]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0][\"word_phonemic_detail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944a8e644284f7c9bbaed1f791e3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute frame-level encodings for each example\n",
    "flat_idxs = []\n",
    "frames_by_item = {}\n",
    "frame_states = []\n",
    "\n",
    "# Collect an index of frames in the dataset by phoneme within matched word type.\n",
    "# We will use this to equivalence-class the model frames later on.\n",
    "phoneme_within_word: dict[tuple[tuple[str, ...], int], list[tuple[int, int]]] = defaultdict(list)\n",
    "phoneme_within_word_prefix: dict[tuple[str, ...], list[tuple[int, int]]] = defaultdict(list)\n",
    "phoneme_within: dict[str, list[tuple[int, int]]] = defaultdict(list)\n",
    "\n",
    "def process(item, idx):\n",
    "    with torch.no_grad():\n",
    "        output = model(output_hidden_states=True,\n",
    "                       input_values=torch.tensor(item[\"input_values\"]).unsqueeze(0))\n",
    "        \n",
    "    # num_layers * sequence_length * hidden_size\n",
    "    batch_hidden = torch.stack(output.hidden_states).squeeze(1)\n",
    "\n",
    "    flat_idx_offset = len(flat_idxs)\n",
    "    flat_idxs.extend([(idx, i) for i in range(batch_hidden.shape[1])])\n",
    "    frames_by_item[idx] = (flat_idx_offset, len(flat_idxs))\n",
    "    frame_states.append(batch_hidden)\n",
    "\n",
    "    # Now align and store frame metadata\n",
    "    compression_ratio = batch_hidden.shape[1] / len(item[\"input_values\"])\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        word_str = tuple(phone[\"phone\"] for phone in word)\n",
    "        word_start = int(word[0][\"start\"] * compression_ratio)\n",
    "        word_end = int(word[-1][\"stop\"] * compression_ratio)\n",
    "\n",
    "        for j, phone in enumerate(word):\n",
    "            word_prefix = word_str[:j + 1]\n",
    "\n",
    "            phone_str = phone[\"phone\"]\n",
    "            phone_start = int(phone[\"start\"] * compression_ratio)\n",
    "            phone_end = int(phone[\"stop\"] * compression_ratio)\n",
    "\n",
    "            for k in range(phone_start, phone_end + 1):\n",
    "                phoneme_within_word[word_str, j].append((idx, k))\n",
    "            for k in range(phone_start, phone_end + 1):\n",
    "                phoneme_within_word_prefix[word_prefix].append((idx, k))\n",
    "            for k in range(phone_start, phone_end + 1):\n",
    "                phoneme_within[phone_str].append((idx, k))\n",
    "\n",
    "    return None\n",
    "\n",
    "dev_dataset = dataset[\"train\"].select(range(5))\n",
    "dev_dataset.map(process, with_indices=True)\n",
    "\n",
    "frame_states = torch.cat(frame_states, dim=1)\n",
    "assert frame_states.shape[1] == len(flat_idxs)\n",
    "\n",
    "# num_frames * num_layers * hidden_size\n",
    "frame_states = frame_states.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_full = torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "flat_idxs_rev = {idx: i for i, idx in enumerate(flat_idxs)}\n",
    "\n",
    "for i, (word, j) in enumerate(phoneme_within_word):\n",
    "    for idx, k in phoneme_within_word[word, j]:\n",
    "        Q_full[flat_idxs_rev[idx, k]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_prefix = torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "flat_idxs_rev = {idx: i for i, idx in enumerate(flat_idxs)}\n",
    "\n",
    "for i, prefix in enumerate(phoneme_within_word_prefix):\n",
    "    for idx, k in phoneme_within_word_prefix[prefix]:\n",
    "        Q_prefix[flat_idxs_rev[idx, k]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_phoneme = torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "flat_idxs_rev = {idx: i for i, idx in enumerate(flat_idxs)}\n",
    "\n",
    "for i, key in enumerate(phoneme_within):\n",
    "    for idx, k in phoneme_within[key]:\n",
    "        Q_phoneme[flat_idxs_rev[idx, k]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067b4442dc0643e38e3f9c6b7767536d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each frame, store the preceding frame at which the word event began.\n",
    "S = torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "\n",
    "def compute_S(item, idx):\n",
    "    flat_idx_offset, flat_idx_end = frames_by_item[idx]\n",
    "    num_frames = flat_idx_end - flat_idx_offset\n",
    "    compression_ratio = num_frames / len(item[\"input_values\"])\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        word_str = tuple(phone[\"phone\"] for phone in word)\n",
    "        word_start = int(word[0][\"start\"] * compression_ratio)\n",
    "        word_end = int(word[-1][\"stop\"] * compression_ratio)\n",
    "\n",
    "        for j in range(word_start, word_end + 1):\n",
    "            S[flat_idx_offset + j] = word_start\n",
    "\n",
    "    return None\n",
    "\n",
    "dev_dataset.map(compute_S, with_indices=True)\n",
    "\n",
    "# If Q is set, then S should be set\n",
    "# (Q != -1) => (S != -1)\n",
    "assert ((Q_phoneme == -1) | (S != -1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(F, Q, S, max_length, batch_size=32):\n",
    "    dataset = []\n",
    "    assert F.shape[0] == Q.shape[0] == S.shape[0]\n",
    "    n_F = F.size(0)\n",
    "\n",
    "    lengths = torch.arange(n_F) - S\n",
    "    lengths = torch.minimum(lengths, torch.tensor(max_length))\n",
    "    # TODO this is just a hack\n",
    "    lengths[lengths == 0] = 1\n",
    "    lengths[S == -1] = -1\n",
    "\n",
    "    for i in range(n_F):\n",
    "        if lengths[i] == -1:\n",
    "            continue\n",
    "\n",
    "        pos_indices = (Q == Q[i]).nonzero(as_tuple=True)[0]\n",
    "        neg_indices = ((Q != -1) & (Q != Q[i])).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(pos_indices) > 1 and len(neg_indices) > 0:\n",
    "            pos_indices = pos_indices[pos_indices != i]\n",
    "            pos_idx = random.choice(pos_indices)\n",
    "            neg_idx = random.choice(neg_indices)\n",
    "\n",
    "            # Extract sequences\n",
    "            example_seq = get_sequence(F, S[i], i, max_length)\n",
    "            pos_seq = get_sequence(F, S[pos_idx], pos_idx, max_length)\n",
    "            neg_seq = get_sequence(F, S[neg_idx], neg_idx, max_length)\n",
    "\n",
    "            dataset.append((example_seq, lengths[i],\n",
    "                            pos_seq, lengths[pos_idx],\n",
    "                            neg_seq, lengths[neg_idx]))\n",
    "\n",
    "    return DataLoader(TensorDataset(\n",
    "        # example frames and lengths\n",
    "        torch.stack([x[0] for x in dataset]), \n",
    "        torch.stack([x[1] for x in dataset]), \n",
    "\n",
    "        # positive frames and lengths\n",
    "        torch.stack([x[2] for x in dataset]),\n",
    "        torch.stack([x[3] for x in dataset]),\n",
    "\n",
    "        # negative frames and lengths\n",
    "        torch.stack([x[4] for x in dataset]),\n",
    "        torch.stack([x[5] for x in dataset])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "output_dim = 32\n",
    "dataloader = prepare_dataloader(frame_states[:, layer, :], Q_full, S, max_length, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_model = ContrastiveEmbeddingModel(input_dim=frame_states.shape[-1], hidden_dim=256,\n",
    "                                     output_dim=output_dim, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2177, -0.0353, -0.2646,  ..., -0.1344,  0.1016, -0.1929],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor(1),\n",
       " tensor([[-0.2177, -0.0353, -0.2646,  ..., -0.1344,  0.1016, -0.1929],\n",
       "         [-0.2657,  0.1511, -0.2867,  ..., -0.0080, -0.1367, -0.1434],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor(1),\n",
       " tensor([[ 0.1642,  0.3462, -0.1093,  ..., -0.1092,  0.0529, -0.0363],\n",
       "         [ 0.2049,  0.3594, -0.1376,  ..., -0.0686,  0.0304, -0.0183],\n",
       "         [-0.0319,  0.1716, -0.1741,  ..., -0.1461,  0.2767, -0.3333],\n",
       "         ...,\n",
       "         [ 0.0324,  0.0435,  0.1996,  ...,  0.5078,  0.0349, -0.0181],\n",
       "         [-0.0628, -0.0257,  0.1752,  ...,  0.4680,  0.0374,  0.3896],\n",
       "         [-0.2725, -0.0850,  0.0609,  ...,  0.0899,  0.1853,  0.3375]]),\n",
       " tensor(20))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.7878, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_model(next(iter(dataloader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
