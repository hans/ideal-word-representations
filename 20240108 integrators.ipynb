{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0595,  0.0202, -0.0344, -0.0358]])\n",
      "tensor([[-0.0595,  0.0202, -0.0344, -0.0358]])\n",
      "//\n",
      "tensor([[-0.0337,  0.0206, -0.0498, -0.0423]])\n",
      "tensor([[-0.0337,  0.0206, -0.0498, -0.0423]])\n",
      "//\n",
      "tensor([[-0.0592,  0.0162, -0.0423, -0.0357]])\n",
      "tensor([[-0.0592,  0.0162, -0.0423, -0.0357]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # TODO padding is on the LEFT here, make sure RNN reflects this\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_sequence(F, start_index, end_index, max_length):\n",
    "    if end_index - start_index + 1 > max_length:\n",
    "        start_index = end_index - max_length + 1\n",
    "    sequence = F[start_index:end_index + 1]\n",
    "    \n",
    "    # Pad on right if necessary\n",
    "    if len(sequence) < max_length:\n",
    "        pad_size = max_length - len(sequence)\n",
    "        padding = torch.zeros(pad_size, F.shape[1])\n",
    "        sequence = torch.cat((sequence, padding), dim=0)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def prepare_batches(F, Q, S, max_length, batch_size=32):\n",
    "    dataset = []\n",
    "    assert F.shape[0] == Q.shape[0] == S.shape[0]\n",
    "    n_F = F.size(0)\n",
    "\n",
    "    lengths = torch.arange(n_F) - S\n",
    "    lengths = torch.minimum(lengths, torch.tensor(max_length))\n",
    "    # TODO this is just a hack\n",
    "    lengths[lengths == 0] = 1\n",
    "\n",
    "    for i in range(n_F):\n",
    "        pos_indices = (Q == Q[i]).nonzero(as_tuple=True)[0]\n",
    "        neg_indices = (Q != Q[i]).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(pos_indices) > 1 and len(neg_indices) > 0:\n",
    "            pos_indices = pos_indices[pos_indices != i]\n",
    "            pos_idx = random.choice(pos_indices)\n",
    "            neg_idx = random.choice(neg_indices)\n",
    "\n",
    "            # Extract sequences\n",
    "            example_seq = get_sequence(F, S[i], i, max_length)\n",
    "            pos_seq = get_sequence(F, S[pos_idx], pos_idx, max_length)\n",
    "            neg_seq = get_sequence(F, S[neg_idx], neg_idx, max_length)\n",
    "\n",
    "            dataset.append((example_seq, lengths[i],\n",
    "                            pos_seq, lengths[pos_idx],\n",
    "                            neg_seq, lengths[neg_idx]))\n",
    "\n",
    "    return DataLoader(TensorDataset(\n",
    "        # example frames and lengths\n",
    "        torch.stack([x[0] for x in dataset]), \n",
    "        torch.stack([x[1] for x in dataset]), \n",
    "\n",
    "        # positive frames and lengths\n",
    "        torch.stack([x[2] for x in dataset]),\n",
    "        torch.stack([x[3] for x in dataset]),\n",
    "\n",
    "        # negative frames and lengths\n",
    "        torch.stack([x[4] for x in dataset]),\n",
    "        torch.stack([x[5] for x in dataset])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def compute_batch_embeddings(example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths):\n",
    "    # Compute embeddings\n",
    "    embeddings = model(example_batch, example_lengths)\n",
    "    pos_embeddings = model(pos_batch, pos_lengths)\n",
    "    neg_embeddings = model(neg_batch, neg_lengths)\n",
    "\n",
    "    # Gather final embedding of each sequence\n",
    "    embeddings = torch.gather(embeddings, 1, (example_lengths - 1).reshape(-1, 1, 1).expand(-1, 1, embeddings.shape[-1])).squeeze(1)\n",
    "\n",
    "    # Last frame's embedding is used for comparison\n",
    "    pos_embeddings = pos_embeddings[:, -1, :]\n",
    "    neg_embeddings = neg_embeddings[:, -1, :]\n",
    "\n",
    "    return embeddings, pos_embeddings, neg_embeddings\n",
    "\n",
    "\n",
    "def compute_batched_rnn_loss(model, data_loader, tau=0.1):\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths in data_loader:\n",
    "        embeddings, pos_embeddings, neg_embeddings = compute_batch_embeddings(example_batch, example_lengths, pos_batch, pos_lengths, neg_batch, neg_lengths)\n",
    "\n",
    "        pos_dist = Fn.cosine_similarity(embeddings, pos_embeddings, dim=1)\n",
    "        neg_dist = Fn.cosine_similarity(embeddings, neg_embeddings, dim=1)\n",
    "\n",
    "        pos_loss = -torch.log(torch.exp(pos_dist / tau)).mean()\n",
    "        neg_loss = -torch.log(torch.exp(-neg_dist / tau)).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / total_batches\n",
    "\n",
    "\n",
    "# Example usage\n",
    "n_F, d = 100, 4  # Example dimensions\n",
    "F = torch.randn(n_F, d) * 3  # Random frame features\n",
    "Q = torch.randint(0, 10, (n_F,))  # Random frame matches\n",
    "S = torch.maximum(torch.tensor(0), torch.arange(n_F) - torch.randint(1, 10, (n_F,)))  # Random span indices\n",
    "max_length = 20  # Maximum sequence length for RNN\n",
    "\n",
    "model = RNNModel(input_dim=d, hidden_dim=256, output_dim=d)\n",
    "# data_loader = prepare_batches(F, Q, S, max_length, batch_size=32)\n",
    "# loss = compute_batched_rnn_loss(model, data_loader)\n",
    "# print(loss)\n",
    "\n",
    "# Build a test batch\n",
    "sample_idx, pos_sample_idx, neg_sample_idx = 37, 23, 85\n",
    "sample_length, pos_sample_length, neg_sample_length = 10, 8, 12\n",
    "example_batch = get_sequence(F, S[sample_idx], sample_idx, max_length).unsqueeze(0)\n",
    "pos_sample_batch = get_sequence(F, S[pos_sample_idx], pos_sample_idx, max_length).unsqueeze(0)\n",
    "neg_sample_batch = get_sequence(F, S[neg_sample_idx], neg_sample_idx, max_length).unsqueeze(0)\n",
    "example_lengths = torch.tensor([sample_length])\n",
    "pos_sample_lengths = torch.tensor([pos_sample_length])\n",
    "neg_sample_lengths = torch.tensor([neg_sample_length])\n",
    "with torch.no_grad():\n",
    "    embeddings, pos_embeddings, neg_embeddings = compute_batch_embeddings(example_batch, example_lengths, pos_sample_batch, pos_sample_lengths, neg_sample_batch, neg_sample_lengths)\n",
    "# Manually compute embeddings\n",
    "def compute_embedding_single(model, x, length):\n",
    "    return model.fc(model.rnn(x[:, :length, :])[0][:, -1, :])\n",
    "with torch.no_grad():\n",
    "    embeddings_manual = compute_embedding_single(model, example_batch, example_lengths)\n",
    "    pos_embeddings_manual = compute_embedding_single(model, pos_sample_batch, pos_sample_lengths)\n",
    "    neg_embeddings_manual = compute_embedding_single(model, neg_sample_batch, neg_sample_lengths)\n",
    "\n",
    "    # Check that the embeddings are the same\n",
    "    print(embeddings)\n",
    "    print(embeddings_manual)\n",
    "    print(\"//\")\n",
    "    print(pos_embeddings)\n",
    "    print(pos_embeddings_manual)\n",
    "    print(\"//\")\n",
    "    print(neg_embeddings)\n",
    "    print(neg_embeddings_manual)\n",
    "    torch.testing.assert_close(embeddings, embeddings_manual)\n",
    "    torch.testing.assert_close(pos_embeddings, pos_embeddings_manual)\n",
    "    torch.testing.assert_close(neg_embeddings, neg_embeddings_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
