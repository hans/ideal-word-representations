{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import transformers\n",
    "\n",
    "from src.datasets.speech_equivalence import \\\n",
    "    SpeechEquivalenceDataset, SpeechHiddenStateDataset, make_timit_equivalence_dataset\n",
    "from src.models.integrator import ContrastiveEmbeddingModel, ContrastiveEmbeddingModelConfig, prepare_dataset\n",
    "from src.utils.timit import load_or_prepare_timit_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "equivalence_classer = \"phoneme_within_word_prefix\"\n",
    "num_frames_per_phoneme = 1\n",
    "\n",
    "layer = 6\n",
    "output_dim = 32\n",
    "\n",
    "equiv_dataset_path = f\"data/timit_equiv_{equivalence_classer}_{num_frames_per_phoneme}.pkl\"\n",
    "output_dir = f\"out/ce_model_{equivalence_classer}_{layer}_{output_dim}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare equivalence class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model: transformers.Wav2Vec2Model = transformers.Wav2Vec2Model.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_or_prepare_timit_corpus(\"data/timit_phoneme\", \"data/timit_raw\",\n",
    "                                       processor)\n",
    "\n",
    "def add_indices(item, idx):\n",
    "    item[\"idx\"] = idx\n",
    "    return item\n",
    "dataset = dataset.map(add_indices, batched=True, batch_size=2000, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = dataset[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a8b9c2a23944f3a0ffe77b1f3eef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting hidden states:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d9a4b5c5e9418fbc8e7e5f4ea73496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing start frames:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "equiv_dataset = make_timit_equivalence_dataset(\n",
    "    f\"timit_phoneme/{equivalence_classer}\",\n",
    "    dev_dataset, model,\n",
    "    equivalence_classer,\n",
    "    num_frames_per_phoneme=num_frames_per_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechEquivalenceDataset(timit_phoneme/phoneme_within_word_prefix, 8879 classes, 31712 instances, with SpeechHiddenStateDataset(facebook/wav2vec2-base, 1000 items, 150670 frames, 13 layers, 768 hidden size))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equiv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute how many positive examples each Q lines up. we want to make sure we have a minimal\n",
    "# number of positive examples for each Q, even the sparse word-level ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a max length that accommodates the majority of the samples, excluding outlier lengths\n",
    "evident_lengths = equiv_dataset.lengths\n",
    "target_length = int(torch.quantile(evident_lengths, 0.95).item())\n",
    "sns.displot(evident_lengths.numpy(), kde=True)\n",
    "target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0312b59f0d3c4c629a12cecf999b58a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userdata/jgauthier/projects/ideal-word-representations/src/models/integrator.py:124\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(F, Q, S, max_length)\u001b[0m\n\u001b[1;32m    118\u001b[0m neg_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(neg_indices)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# TODO ideally we'd have multiple positive/negative examples\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# per example, especially in sparser Q cases.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Extract sequences\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m example_seq \u001b[38;5;241m=\u001b[39m \u001b[43mget_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m pos_seq \u001b[38;5;241m=\u001b[39m get_sequence(F, S[pos_idx], pos_idx, max_length)\n\u001b[1;32m    126\u001b[0m neg_seq \u001b[38;5;241m=\u001b[39m get_sequence(F, S[neg_idx], neg_idx, max_length)\n",
      "File \u001b[0;32m/userdata/jgauthier/projects/ideal-word-representations/src/models/integrator.py:90\u001b[0m, in \u001b[0;36mget_sequence\u001b[0;34m(F, start_index, end_index, max_length)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;241m<\u001b[39m max_length:\n\u001b[1;32m     89\u001b[0m     pad_size \u001b[38;5;241m=\u001b[39m max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)\n\u001b[0;32m---> 90\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((sequence, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = prepare_dataset(equiv_dataset, target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "dataset.save_to_disk(Path(output_dir) / \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_model = ContrastiveEmbeddingModel(\n",
    "    input_dim=equiv_dataset.hidden_state_dataset.hidden_dim,\n",
    "    hidden_dim=32,\n",
    "    output_dim=output_dim, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=5,\n",
    "    logging_steps=10,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "dataset_split = dev_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "trainer = transformers.Trainer(\n",
    "    model=ce_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # data_collator=MyCollator(max_length),\n",
    "    args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1784' max='1784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1784/1784 12:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-0.531900</td>\n",
       "      <td>-0.490023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.553900</td>\n",
       "      <td>-0.592076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>-0.652100</td>\n",
       "      <td>-0.705454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.700300</td>\n",
       "      <td>-0.837613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-1.127100</td>\n",
       "      <td>-0.988161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-1.069200</td>\n",
       "      <td>-1.149871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>-1.306900</td>\n",
       "      <td>-1.319050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-1.071700</td>\n",
       "      <td>-1.492350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>-1.299300</td>\n",
       "      <td>-1.669061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-2.121800</td>\n",
       "      <td>-1.834337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>-2.017100</td>\n",
       "      <td>-1.993588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-2.249800</td>\n",
       "      <td>-2.131993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>-2.580700</td>\n",
       "      <td>-2.253350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-2.284600</td>\n",
       "      <td>-2.354437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>-2.407200</td>\n",
       "      <td>-2.434043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-2.541100</td>\n",
       "      <td>-2.490052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>-2.369100</td>\n",
       "      <td>-2.523082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1784, training_loss=-1.5648202286707447, metrics={'train_runtime': 755.1656, 'train_samples_per_second': 75.586, 'train_steps_per_second': 2.362, 'total_flos': 0.0, 'train_loss': -1.5648202286707447, 'epoch': 2.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
