{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import transformers\n",
    "\n",
    "from src.models.integrator import ContrastiveEmbeddingModel, prepare_dataset\n",
    "from src.utils.timit import load_or_prepare_timit_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:792: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/jgauthier/transformers/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model: transformers.Wav2Vec2Model = transformers.Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_or_prepare_timit_corpus(\"data/timit_phoneme\", \"data/timit_raw\",\n",
    "                                       processor)\n",
    "\n",
    "def add_indices(item, idx):\n",
    "    item[\"idx\"] = idx\n",
    "    return item\n",
    "dataset = dataset.map(add_indices, batched=True, batch_size=2000, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b3730ab81f443fa12068ac0c5d3bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect this many frames per phoneme at maximum\n",
    "num_frames_per_phoneme = 1\n",
    "\n",
    "# Compute frame-level encodings for each example\n",
    "flat_idxs = []\n",
    "frames_by_item = {}\n",
    "frame_states = []\n",
    "\n",
    "# Collect an index of frames in the dataset by phoneme within matched word type.\n",
    "# We will use this to equivalence-class the model frames later on.\n",
    "equivalence_classers = {\n",
    "    \"phoneme_within_word\": lambda word, i: (tuple(phone[\"phone\"] for phone in word), i),\n",
    "    \"phoneme_within_word_prefix\": lambda word, i: tuple(phone[\"phone\"] for phone in word[:i + 1]),\n",
    "    \"phoneme_within\": lambda word, i: word[i][\"phone\"],\n",
    "}\n",
    "frame_groups = {classer: defaultdict(list) for classer in equivalence_classers}\n",
    "\n",
    "def process(item, idx):\n",
    "    with torch.no_grad():\n",
    "        output = model(output_hidden_states=True,\n",
    "                       input_values=torch.tensor(item[\"input_values\"]).unsqueeze(0).to(model.device))\n",
    "        \n",
    "    # num_layers * sequence_length * hidden_size\n",
    "    batch_hidden = torch.stack(output.hidden_states).squeeze(1).cpu()\n",
    "\n",
    "    flat_idx_offset = len(flat_idxs)\n",
    "    flat_idxs.extend([(idx, i) for i in range(batch_hidden.shape[1])])\n",
    "    frames_by_item[idx] = (flat_idx_offset, len(flat_idxs))\n",
    "    frame_states.append(batch_hidden)\n",
    "\n",
    "    # Now align and store frame metadata\n",
    "    compression_ratio = batch_hidden.shape[1] / len(item[\"input_values\"])\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        word_str = tuple(phone[\"phone\"] for phone in word)\n",
    "        word_start = int(word[0][\"start\"] * compression_ratio)\n",
    "        word_end = int(word[-1][\"stop\"] * compression_ratio)\n",
    "\n",
    "        for j, phone in enumerate(word):\n",
    "            word_prefix = word_str[:j + 1]\n",
    "\n",
    "            phone_str = phone[\"phone\"]\n",
    "            phone_start = int(phone[\"start\"] * compression_ratio)\n",
    "            phone_end = int(phone[\"stop\"] * compression_ratio)\n",
    "\n",
    "            ks = list(range(phone_start, phone_end + 1))\n",
    "            if len(ks) > num_frames_per_phoneme:\n",
    "                ks = ks[-num_frames_per_phoneme:]\n",
    "            for k in ks:\n",
    "                for classer, fn in equivalence_classers.items():\n",
    "                    class_key = fn(word, j)\n",
    "                    frame_groups[classer][class_key].append((idx, k))\n",
    "\n",
    "    return None\n",
    "\n",
    "dev_dataset = dataset[\"train\"].select(range(1000))\n",
    "dev_dataset.map(process, with_indices=True)\n",
    "\n",
    "frame_states = torch.cat(frame_states, dim=1)\n",
    "assert frame_states.shape[1] == len(flat_idxs)\n",
    "\n",
    "# num_frames * num_layers * hidden_size\n",
    "frame_states = frame_states.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd8f60081124d45b973c2a167c3f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping by phoneme_within_word:   0%|          | 0/16722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e878a14d1d3c4bc5a38a204c7e1668f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping by phoneme_within_word_prefix:   0%|          | 0/8879 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc58a899074f4cd0ba07688e7fae4036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping by phoneme_within:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qs = {k: torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "      for k in frame_groups.keys()}\n",
    "group_idxs = {k: {class_key: idx for idx, class_key in enumerate(frame_groups[k].keys())}\n",
    "              for k in frame_groups.keys()}\n",
    "flat_idxs_rev = {idx: i for i, idx in enumerate(flat_idxs)}\n",
    "\n",
    "for classer, groups in frame_groups.items():\n",
    "    for class_key, group in tqdm(groups.items(), desc=f\"Grouping by {classer}\"):\n",
    "        for idx, k in group:\n",
    "            Qs[classer][flat_idxs_rev[idx, k]] = group_idxs[classer][class_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd425f4859b64c46865d91bd67c4c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c3a650a3814ab3b25f5f24488f91f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each frame, store the preceding frame at which the word event began.\n",
    "start_reference = {\n",
    "    \"phoneme_within_word\": \"word\",\n",
    "    \"phoneme_within_word_prefix\": \"word\",\n",
    "    \"phoneme_within\": \"phoneme\",\n",
    "}\n",
    "Ss = {k: torch.zeros(len(flat_idxs), dtype=torch.long) - 1\n",
    "      for k in start_reference.values()}\n",
    "\n",
    "def compute_S_word(item, idx):\n",
    "    flat_idx_offset, flat_idx_end = frames_by_item[idx]\n",
    "    num_frames = flat_idx_end - flat_idx_offset\n",
    "    compression_ratio = num_frames / len(item[\"input_values\"])\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        word_str = tuple(phone[\"phone\"] for phone in word)\n",
    "        word_start = int(word[0][\"start\"] * compression_ratio)\n",
    "        word_end = int(word[-1][\"stop\"] * compression_ratio)\n",
    "\n",
    "        for j in range(word_start, word_end + 1):\n",
    "            Ss[\"word\"][flat_idx_offset + j] = flat_idx_offset + word_start\n",
    "\n",
    "    return None\n",
    "\n",
    "def compute_S_phoneme(item, idx):\n",
    "    flat_idx_offset, flat_idx_end = frames_by_item[idx]\n",
    "    num_frames = flat_idx_end - flat_idx_offset\n",
    "    compression_ratio = num_frames / len(item[\"input_values\"])\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        for phone in word:\n",
    "            phone_str = phone[\"phone\"]\n",
    "            phone_start = int(phone[\"start\"] * compression_ratio)\n",
    "            phone_end = int(phone[\"stop\"] * compression_ratio)\n",
    "\n",
    "            for j in range(phone_start, phone_end + 1):\n",
    "                Ss[\"phoneme\"][flat_idx_offset + j] = flat_idx_offset + phone_start\n",
    "\n",
    "    return None\n",
    "\n",
    "start_reference_fns = {\n",
    "    \"word\": compute_S_word,\n",
    "    \"phoneme\": compute_S_phoneme,\n",
    "}\n",
    "for k, fn in start_reference_fns.items():\n",
    "    dev_dataset.map(fn, with_indices=True)\n",
    "\n",
    "# If Q is set, then S should be set\n",
    "# (Q != -1) => (S != -1)\n",
    "# assert ((Q_phoneme == -1) | (S_phoneme != -1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: no crazy long events\n",
    "for start_reference, S in Ss.items():\n",
    "    evident_lengths = torch.arange(len(S)) - S\n",
    "    max_evident_length = evident_lengths[S != -1].max()\n",
    "    assert max_evident_length < 100, f\"{start_reference} has event with length {max_evident_length} frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute how many positive examples each Q lines up. we want to make sure we have a minimal\n",
    "# number of positive examples for each Q, even the sparse word-level ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "output_dim = 32\n",
    "\n",
    "eval_classer = \"phoneme_within_word\" #phoneme_within_word_prefix\"\n",
    "output_dir = f\"out/ce_model_{eval_classer}_{layer}_{output_dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m evident_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(S)) \u001b[38;5;241m-\u001b[39m S\n\u001b[1;32m      6\u001b[0m evident_lengths \u001b[38;5;241m=\u001b[39m evident_lengths[S \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[43mevident_lengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "Q = Qs[eval_classer]\n",
    "S = Ss[start_reference[eval_classer]]\n",
    "\n",
    "# Compute ideal max_length based on S.\n",
    "evident_lengths = torch.arange(len(S)) - S\n",
    "evident_lengths = evident_lengths[S != -1]\n",
    "max_length = evident_lengths.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9,  9,  9,  ..., 88, 88, 88]), 129037)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[S != -1], len(S[S != -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 150573, 150574, 150575])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evident_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150583"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length\n",
    "# TODO max_length is too big -- figure out why tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0312b59f0d3c4c629a12cecf999b58a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/userdata/jgauthier/projects/ideal-word-representations/src/models/integrator.py:124\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(F, Q, S, max_length)\u001b[0m\n\u001b[1;32m    118\u001b[0m neg_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(neg_indices)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# TODO ideally we'd have multiple positive/negative examples\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# per example, especially in sparser Q cases.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Extract sequences\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m example_seq \u001b[38;5;241m=\u001b[39m \u001b[43mget_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m pos_seq \u001b[38;5;241m=\u001b[39m get_sequence(F, S[pos_idx], pos_idx, max_length)\n\u001b[1;32m    126\u001b[0m neg_seq \u001b[38;5;241m=\u001b[39m get_sequence(F, S[neg_idx], neg_idx, max_length)\n",
      "File \u001b[0;32m/userdata/jgauthier/projects/ideal-word-representations/src/models/integrator.py:90\u001b[0m, in \u001b[0;36mget_sequence\u001b[0;34m(F, start_index, end_index, max_length)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;241m<\u001b[39m max_length:\n\u001b[1;32m     89\u001b[0m     pad_size \u001b[38;5;241m=\u001b[39m max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)\n\u001b[0;32m---> 90\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((sequence, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = prepare_dataset(frame_states[:, layer, :], Q, S, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "dataset.save_to_disk(Path(output_dir) / \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_model = ContrastiveEmbeddingModel(\n",
    "    input_dim=frame_states.shape[-1],\n",
    "    hidden_dim=32,\n",
    "    output_dim=output_dim, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=5,\n",
    "    logging_steps=10,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "dataset_split = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "trainer = transformers.Trainer(\n",
    "    model=ce_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # data_collator=MyCollator(max_length),\n",
    "    args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1784' max='1784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1784/1784 12:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-0.531900</td>\n",
       "      <td>-0.490023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.553900</td>\n",
       "      <td>-0.592076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>-0.652100</td>\n",
       "      <td>-0.705454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.700300</td>\n",
       "      <td>-0.837613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-1.127100</td>\n",
       "      <td>-0.988161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-1.069200</td>\n",
       "      <td>-1.149871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>-1.306900</td>\n",
       "      <td>-1.319050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-1.071700</td>\n",
       "      <td>-1.492350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>-1.299300</td>\n",
       "      <td>-1.669061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-2.121800</td>\n",
       "      <td>-1.834337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>-2.017100</td>\n",
       "      <td>-1.993588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-2.249800</td>\n",
       "      <td>-2.131993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>-2.580700</td>\n",
       "      <td>-2.253350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-2.284600</td>\n",
       "      <td>-2.354437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>-2.407200</td>\n",
       "      <td>-2.434043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-2.541100</td>\n",
       "      <td>-2.490052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>-2.369100</td>\n",
       "      <td>-2.523082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1784, training_loss=-1.5648202286707447, metrics={'train_runtime': 755.1656, 'train_samples_per_second': 75.586, 'train_steps_per_second': 2.362, 'total_flos': 0.0, 'train_loss': -1.5648202286707447, 'epoch': 2.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
