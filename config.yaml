datasets:
  timit:
    raw_path: data/timit_raw
  timit-no_repeats:
    raw_path: data/timit_raw


models:

  # discriminative word model
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: discrim-rnn_32
    equivalence: word_broad_10frames

  # feedforward word model
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: ff_32
    equivalence: word_broad_10frames

  # librispeech other models
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: rnn_8-weightdecay0.01
    equivalence: phoneme_10frames
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: rnn_8-weightdecay0.01
    equivalence: syllable_10frames
  # librispeech random models
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: randomrnn_8
    equivalence: random
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: randomrnn_32
    equivalence: random
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: randomff_8
    equivalence: random
  - base_model: w2v2_8    
    dataset: librispeech-train-clean-100
    model: randomff_32
    equivalence: random

  # LS hinge mAP-maximizing phoneme models
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: rnn_8-hinge_ph-mAP1
    equivalence: phoneme_10frames

  # LS hinge mAP-maximizing word models
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: rnn_32-hinge-mAP4
    equivalence: word_broad_10frames

  # LS mAP model but with fixed start frame
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: rnn_32-hinge-mAP4
    equivalence: word_broad_10frames_fixedlen25

  # LS discriminative models
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: discrim-rnn_32-mAP1
    equivalence: word_broad_10frames
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: discrim-rnn_32-mAP2
    equivalence: word_broad_10frames
  - base_model: w2v2_8
    dataset: librispeech-train-clean-100
    model: discrim-rnn_32-mAP3
    equivalence: word_broad_10frames

encoding:
  data:
    - subject: EC152
      blocks:
        - B22

    - subject: EC196
      blocks:
        - B1

    - subject: EC195
      blocks:
        - B1

    - subject: EC183
      blocks:
        - B43

    - subject: EC212
      blocks:
        - B13
      fit_channels:
        start: 0
        end: 384

    # missing TIMIT events?
    # - subject: EC208
    #   data_source: fomo
    #   blocks:
    #     - B1
    - subject: EC260
      blocks:
        - B1

    # remaining Barakeet subjects
    - subject: EC248
      data_source: fomo
      blocks:
        - B1
    - subject: EC243
      blocks:
        - B1
    # todo not available
    # - subject: EC279
    #   blocks:
    #     - B1
    - subject: EC250
      blocks:
        - B1
    - subject: EC270
      blocks:
        - B1
    - subject: EC278
      blocks:
        - B27
      fit_channels:
        start: 0
        end: 168
    - subject: EC253
      data_source: fomo
      blocks:
        - B1

  model_comparisons:
    - model2: biphone_recon-w2v2_8-l2norm
      model1: baseline
    - model2: biphone_pred-w2v2_8-l2norm
      model1: baseline
    - model2: random8-w2v2_8-l2norm
      model1: baseline
    - model2: random32-w2v2_8-l2norm
      model1: baseline

    # # with l2 normalization
    # - model2: ls-word_broad-hinge-w2v2_8-l2norm
    #   model1: baseline
    # - model2: ls-word_broad_fixed-hinge-w2v2_8-l2norm
    #   model1: baseline
    - model2: phoneme-w2v2_8-l2norm
      model1: baseline

    # one impulse per phoneme rather than one impulse per unit
    - model2: ph-ls-word_broad-hinge-w2v2_8-l2norm
      model1: baseline
    - model2: ph-ls-word_broad_fixed-hinge-w2v2_8-l2norm
      model1: baseline
    - model2: ph-syllable-w2v2_8-l2norm
      model1: baseline
    - model2: ph-ls-word_broad-hinge-w2v2_8-discrim1-l2norm
      model1: baseline
    - model2: ph-ls-word_broad-hinge-w2v2_8-discrim2-l2norm
      model1: baseline

    # - model2: word_broad-aniso2-w2v2_8
    #   model1: baseline_aud

  permutation_tests:
    units:
      permutation: permute_units
      num_permutations: 5
    shift:
      permutation: shift
      num_permutations: 5


synthetic_encoding:

  evaluations:

    basic10:
      num_components: 10
      num_embeddings_to_select: 6

      datasets:
        - timit

      subsample_strategies:
        - all
        - multisyllabic
        - monosyllabic
        - multisyllabic-nonfirst_syllable

      target_models:
        - w2v2_0
        - w2v2_1
        - w2v2_2
        - w2v2_3
        - w2v2_4
        - w2v2_5
        - w2v2_6
        - w2v2_7
        - w2v2_8
        - w2v2_9
        - w2v2_10
        - w2v2_11
        - w2v2-large_0
        - w2v2-large_11
        - w2v2-large_20

      models:
        - phoneme
        - next_phoneme
        - biphone_recon
        - biphone_pred
        - syllable
        - word_broad

    basic50:
      num_components: 50
      num_embeddings_to_select: 6

      datasets:
        - timit

      subsample_strategies:
        - all
        - multisyllabic
        - monosyllabic
        - multisyllabic-nonfirst_syllable

      target_models:
        - w2v2_0
        - w2v2_1
        - w2v2_2
        - w2v2_3
        - w2v2_4
        - w2v2_5
        - w2v2_6
        - w2v2_7
        - w2v2_8
        - w2v2_9
        - w2v2_10
        - w2v2_11
        - w2v2-large_0
        - w2v2-large_11
        - w2v2-large_20

      models:
        - phoneme
        - next_phoneme
        - biphone_recon
        - biphone_pred
        - syllable
        - word_broad

    basic128:
      num_components: 128
      num_embeddings_to_select: 6

      datasets:
        - timit

      subsample_strategies:
        - all
        - multisyllabic
        - monosyllabic
        - multisyllabic-nonfirst_syllable

      target_models:
        - w2v2_0
        - w2v2_1
        - w2v2_2
        - w2v2_3
        - w2v2_4
        - w2v2_5
        - w2v2_6
        - w2v2_7
        - w2v2_8
        - w2v2_9
        - w2v2_10
        - w2v2_11
        - w2v2-large_0
        - w2v2-large_11
        - w2v2-large_20

      models:
        - phoneme
        - next_phoneme
        - biphone_recon
        - biphone_pred
        - syllable
        - word_broad