{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "from fastdist import fastdist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_instances_per_word = 100\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_instances_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, (\"mean_within_cut\", \"phoneme\"), keepdims=True)\n",
    "traj_flat, traj_flat_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "cuts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = traj_flat_src[:, 2].max() + 1\n",
    "all_phonemes = sorted(cuts_df.description.unique())\n",
    "phoneme2idx = {p: i for i, p in enumerate(all_phonemes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_src_phoneme_lookup = cuts_df.description.map(phoneme2idx).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_results = {}\n",
    "for frame_idx in trange(max_num_frames):\n",
    "    traj_idxs_i = (traj_flat_src[:, 2] == frame_idx).nonzero()[0]\n",
    "    traj_srcs_i = traj_flat_src[traj_idxs_i]\n",
    "    traj_phoneme_idxs = np.array([traj_src_phoneme_lookup[tuple(src)] for src in traj_srcs_i])\n",
    "    \n",
    "    rsa_i = np.zeros((len(all_phonemes), len(all_phonemes)))\n",
    "    for i, j in tqdm(itertools.product(range(len(all_phonemes)), repeat=2), total=len(all_phonemes)**2):\n",
    "        if j > i: continue\n",
    "        traj_idxs_p1 = traj_idxs_i[traj_phoneme_idxs == i]\n",
    "        traj_idxs_p2 = traj_idxs_i[traj_phoneme_idxs == j]\n",
    "        if len(traj_idxs_p1) > 0 and len(traj_idxs_p2) > 0:\n",
    "            rsa_i[i, j] = cdist(traj_flat[traj_idxs_p1], traj_flat[traj_idxs_p2], metric=metric).mean()\n",
    "\n",
    "    rsa_results[frame_idx] = pd.DataFrame(rsa_i, index=all_phonemes, columns=all_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_results = pd.concat(rsa_results, names=[\"frame_idx\"])\n",
    "rsa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = rsa_results.max().max()\n",
    "vmin = rsa_results.min().min()\n",
    "\n",
    "for frame_idx, rsa_results_i in rsa_results.groupby(\"frame_idx\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(rsa_results_i.droplevel(\"frame_idx\"),\n",
    "                ax=ax, square=True, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"RSA distances at frame {frame_idx}\")\n",
    "    ax.set_xlabel(\"Phoneme 2\")\n",
    "    ax.set_ylabel(\"Phoneme 1\")\n",
    "    \n",
    "    fig.savefig(f\"{output_dir}/rsa_{frame_idx}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
