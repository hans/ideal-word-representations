{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "from fastdist import fastdist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_instances_per_word = 100\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_instances_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, (\"mean_within_cut\", \"phoneme\"), keepdims=True)\n",
    "traj_flat, traj_flat_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/SUBTLEXus74286wordstextversion.txt\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "# merge flattened traj idxs into this cuts_df\n",
    "traj_flat_idxs = pd.Series({tuple(traj_flat_src_i): i for i, traj_flat_src_i in enumerate(traj_flat_src)})\n",
    "traj_flat_idxs.index.names = [\"label_idx\", \"instance_idx\", \"frame_idx\"]\n",
    "cuts_df = pd.merge(cuts_df, traj_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)\n",
    "\n",
    "cuts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata for other groupers\n",
    "cuts_df[\"word_length\"] = cuts_df.groupby([\"label_idx\", \"instance_idx\"]).size()\n",
    "# merge in frequency data\n",
    "cuts_df = pd.merge(cuts_df, word_freq_df[[\"Lg10WF\"]], left_on=\"label\", right_index=True, how=\"left\")\n",
    "cuts_df[\"word_relative_position\"] = cuts_df.groupby([\"label_idx\", \"instance_idx\"]).cumcount() / cuts_df.word_length\n",
    "\n",
    "relative_position_bins = 5\n",
    "cuts_df[\"word_relative_position_bin\"] = pd.cut(cuts_df.word_relative_position, bins=relative_position_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bins = 5\n",
    "cuts_df[\"word_frequency_bin\"] = pd.qcut(cuts_df.Lg10WF, q=frequency_bins, labels=False)\n",
    "\n",
    "word_length_bins = 5\n",
    "cuts_df[\"word_length_bin\"] = pd.qcut(cuts_df.word_length, q=word_length_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cuts_df.traj_flat_idx.isna().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemes = sorted(cuts_df.description.unique())\n",
    "phoneme2idx = {p: i for i, p in enumerate(all_phonemes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupers = {\n",
    "    \"position\": [\"frame_idx\"],\n",
    "    \"position_within_length\": [\"word_length_bin\", \"frame_idx\"],\n",
    "    \"position_within_frequency\": [\"word_frequency_bin\", \"frame_idx\"],\n",
    "    \"relative_position\": [\"word_relative_position_bin\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rsa(grouper_name, grouper, cuts_df, traj_flat):\n",
    "    rsa_results = {}\n",
    "    for group, rows in tqdm(cuts_df.groupby(grouper), leave=False):\n",
    "        rsa_i = {}\n",
    "        # split rows by phoneme label\n",
    "        rows = {phoneme_idx: rows_i for phoneme_idx, rows_i in rows.groupby(rows.description.map(phoneme2idx))}\n",
    "        for i, j in tqdm(itertools.product(range(len(all_phonemes)), repeat=2), total=len(all_phonemes)**2, leave=False):\n",
    "            if j > i: continue\n",
    "            if i not in rows or j not in rows: continue\n",
    "            traj_idxs_p1 = rows[i].traj_flat_idx\n",
    "            traj_idxs_p2 = rows[j].traj_flat_idx\n",
    "            if len(traj_idxs_p1) > 0 and len(traj_idxs_p2) > 0:\n",
    "                rsa_i[all_phonemes[i], all_phonemes[j]] = cdist(traj_flat[traj_idxs_p1], traj_flat[traj_idxs_p2], metric=metric).mean()\n",
    "\n",
    "        rsa_results[group] = rsa_i\n",
    "\n",
    "    # save results csv\n",
    "    rsa_results = pd.concat(\\\n",
    "        {group: pd.Series(rsa_results_i) for group, rsa_results_i in rsa_results.items()},\n",
    "        names=grouper + [\"phoneme1\", \"phoneme2\"]).unstack().sort_index()\n",
    "    rsa_results.to_csv(f\"{output_dir}/rsa_results-{grouper_name}.csv\")\n",
    "\n",
    "    # plot\n",
    "    vmax = rsa_results.max().max()\n",
    "    vmin = rsa_results.min().min()\n",
    "\n",
    "    for group, rsa_results_i in tqdm(rsa_results.groupby(grouper), desc=\"Plotting\", leave=False):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        sns.heatmap(rsa_results_i.droplevel(grouper),\n",
    "                    ax=ax, square=True, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"RSA distances in {grouper_name} at group {group}\")\n",
    "        ax.set_xlabel(\"Phoneme 2\")\n",
    "        ax.set_ylabel(\"Phoneme 1\")\n",
    "        \n",
    "        group_id = \"_\".join(str(g) for g in group)\n",
    "        fig.savefig(f\"{output_dir}/rsa-{grouper_name}-{group_id}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouper_name, grouper in tqdm(groupers.items(), unit=\"grouper\"):\n",
    "    evaluate_rsa(grouper_name, grouper, cuts_df, traj_flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
