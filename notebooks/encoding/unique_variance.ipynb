{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from hydra import compose, initialize_config_dir\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.encoding.ecog import timit as timit_encoding\n",
    "from src.encoding.ecog import get_electrode_df\n",
    "from src.estimate_encoder import prepare_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "encoder_path = \"outputs/encoders/timit/baseline/EC212\"\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize_config_dir(str(Path(encoder_path).resolve() / \".hydra\")):\n",
    "    config = compose(config_name=\"config\")\n",
    "\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.read_csv(Path(encoder_path) / \"scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data should be from the same subject\n",
    "all_subjects = set(data_spec.subject for data_spec in config.data)\n",
    "assert len(all_subjects) == 1, f\"All data should be from the same subject. Got: {all_subjects}\"\n",
    "subject = all_subjects.pop()\n",
    "\n",
    "# Prepare electrode metadata\n",
    "electrode_df = get_electrode_df(config, subject)\n",
    "\n",
    "all_xy = [prepare_xy(config, data_spec) for data_spec in config.data]\n",
    "X, Y, feature_names, feature_shapes, trial_onsets = timit_encoding.concat_xy(all_xy)\n",
    "\n",
    "cv_outer = instantiate(config.cv)\n",
    "cv_inner = instantiate(config.cv)\n",
    "\n",
    "# TODO check match between model sfreq and dataset sfreq\n",
    "\n",
    "# Prepare MNE model kwargs from config\n",
    "if \"model\" in config:\n",
    "    trf_kwargs = OmegaConf.to_object(config.model)\n",
    "else:\n",
    "    # account for legacy config\n",
    "    trf_kwargs = {\n",
    "        \"type\": \"trf\",\n",
    "        \"tmin\": 0.0,\n",
    "        \"tmax\": 0.6,\n",
    "        \"sfreq\": 100,\n",
    "        \"fit_intercept\": False,\n",
    "    }\n",
    "\n",
    "sfreq = trf_kwargs.pop(\"sfreq\")\n",
    "trf_kwargs.pop(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_unique_variance(X, Y, target_feature_set, feature_sets,\n",
    "                             feature_names, feature_shapes,\n",
    "                             cv_outer, cv_inner, sfreq, **trf_kwargs):\n",
    "    # NB we are masking out an entire \"feature set\", which corresponds to \n",
    "    # one or more individual features in the TRF.\n",
    "    # The names of the individual features are formatted as \"{feature_set}_{idx}\"\n",
    "    assert target_feature_set in config.feature_sets.baseline_features\n",
    "\n",
    "    # Get start and end indices of columns corresponding to feature sets\n",
    "    feature_set_indices = np.cumsum([0] + feature_shapes)\n",
    "    feature_set_start_idxs = feature_set_indices[:-1]\n",
    "    feature_set_end_idxs = feature_set_indices[1:]\n",
    "    assert len(feature_set_start_idxs) == len(feature_sets)\n",
    "\n",
    "    # Prepare to mask out the target feature\n",
    "    feature_mask = np.ones(X.shape[1], dtype=bool)\n",
    "    feature_set_idx = feature_sets.index(target_feature_set)\n",
    "    feature_mask[feature_set_start_idxs[feature_set_idx]:feature_set_end_idxs[feature_set_idx]] = False\n",
    "\n",
    "    feature_names_masked = [name for name in feature_names if not name.startswith(target_feature_set)]\n",
    "    feature_shapes_masked = [shape for name, shape in zip(feature_sets, feature_shapes) if not name.startswith(target_feature_set)]\n",
    "    assert sum(feature_shapes_masked) == feature_mask.sum()\n",
    "\n",
    "    _, _, scores, _, _ = timit_encoding.strf_nested_cv(\n",
    "        X[:, feature_mask], Y, feature_names_masked, feature_shapes_masked,\n",
    "        sfreq=sfreq, cv_outer=cv_outer, cv_inner=cv_inner, trf_kwargs=trf_kwargs\n",
    "    )\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        # No models converged. Save dummy outputs.\n",
    "        feature_scores_df = pd.DataFrame(\n",
    "            [(fold, output_dim, np.nan)\n",
    "             for fold in range(cv_outer.get_n_splits())\n",
    "             for output_dim in range(Y.shape[1])],\n",
    "            columns=[\"fold\", \"output_dim\", \"score\"]\n",
    "        )\n",
    "    else:\n",
    "        feature_scores_df = pd.DataFrame(\n",
    "            np.array(scores),\n",
    "            index=pd.Index(list(range(cv_outer.get_n_splits())), name=\"fold\"),\n",
    "            columns=pd.Index(list(range(scores[0].shape[0])), name=\"output_dim\"))\n",
    "        feature_scores_df = feature_scores_df.reset_index().melt(id_vars=\"fold\", var_name=\"output_dim\", value_name=\"score\")\n",
    "    feature_scores_df[\"dropped_feature\"] = target_feature_set\n",
    "    feature_scores_df[\"output_name\"] = feature_scores_df.output_dim.map(dict(enumerate(electrode_df.index)))\n",
    "\n",
    "    return feature_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score_dfs = []\n",
    "for feature_set in tqdm(config.feature_sets.baseline_features, unit=\"feature\"):\n",
    "    feature_score_df = estimate_unique_variance(\n",
    "        X, Y, feature_set, config.feature_sets.baseline_features,\n",
    "        feature_names, feature_shapes,\n",
    "        cv_outer, cv_inner, sfreq, **trf_kwargs)\n",
    "    feature_score_dfs.append(feature_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([baseline_scores] + feature_score_dfs) \\\n",
    "    .set_index([\"dropped_feature\", \"fold\", \"output_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(merged_df, (merged_df.score - merged_df.loc[np.nan].score).rename(\"unique_variance_score\").to_frame(),\n",
    "                    left_index=True, right_index=True)\n",
    "final_df.to_csv(Path(output_dir) / \"unique_variance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
