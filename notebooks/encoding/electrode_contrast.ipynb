{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model comparisons on individual electrodes in order to define qualitative \"contrasts\" for individual electrodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"timit-no_repeats\"\n",
    "study_models = {\n",
    "    \"Random\": \"random32-w2v2_8-l2norm\",\n",
    "    \"Phoneme\": \"phoneme-w2v2_8-l2norm\",\n",
    "    \"Word\": \"ph-ls-word_broad-hinge-w2v2_8-l2norm\",\n",
    "    \"Word discrim2\": \"ph-ls-word_broad-hinge-w2v2_8-discrim2-l2norm\",\n",
    "}\n",
    "ttest_results_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/ttest.csv\"\n",
    "scores_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/scores.csv\"\n",
    "\n",
    "encoder_dirs = list(Path(\"outputs/encoders\").glob(f\"{dataset}/*/*\"))\n",
    "\n",
    "pval_threshold = 1e-4\n",
    "# pval_threshold = 5e-7\n",
    "\n",
    "baseline_model = \"baseline\"\n",
    "\n",
    "contrasts = {\n",
    "    \"word_dominant\": ([\"Word\"], [\"Phoneme\", \"Random\"]),\n",
    "    \"word_discrim_dominant\": ([\"Word discrim2\"], [\"Phoneme\", \"Random\"]),\n",
    "    \"phone_dominant\": ([\"Phoneme\"], [\"Word\", \"Random\"]),\n",
    "    \"random_dominant\": ([\"Random\"], [\"Phoneme\", \"Word\"]),\n",
    "}\n",
    "\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dirs = [Path(p) for p in encoder_dirs]\n",
    "study_model_codes = list(study_models.values())\n",
    "\n",
    "# map to codes\n",
    "contrasts = {contrast_name: ([study_models[model_name] for model_name in positive_model_names],\n",
    "                             [study_models[model_name] for model_name in negative_model_names])\n",
    "             for contrast_name, (positive_model_names, negative_model_names) in contrasts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(scores_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\"]).loc[dataset]\n",
    "if study_model_codes is None:\n",
    "    study_model_codes = sorted(scores_df.index.get_level_values(\"model2\").unique())\n",
    "    study_models = {code: code for code in study_model_codes}\n",
    "else:\n",
    "    scores_df = scores_df.loc[scores_df.index.get_level_values(\"model2\").isin(study_model_codes + [baseline_model])]\n",
    "\n",
    "study_model_code_to_name = {code: name for name, code in study_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df = pd.read_csv(ttest_results_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\", \"output_dim\"]) \\\n",
    "    .loc[dataset].loc[(slice(None), study_model_codes), :]\n",
    "ttest_df[\"log_pval\"] = np.log10(ttest_df[\"pval\"])\n",
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrode_df = pd.concat([pd.read_csv(encoder_dir / \"electrodes.csv\", index_col=[\"electrode_idx\"])\n",
    "                          for encoder_dir in encoder_dirs if \"baseline\" == encoder_dir.parent.name],\n",
    "                         names=[\"subject\"], keys=[encoder_dir.name for encoder_dir in encoder_dirs if \"baseline\" == encoder_dir.parent.name])\n",
    "electrode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PValueContrast:\n",
    "    \"\"\"\n",
    "    Defines a qualitative model contrast based on p-value of ttest.\n",
    "    \"\"\"\n",
    "    def __init__(self, ttest_df, scores_df, electrode_df,\n",
    "                 study_model_codes,\n",
    "                 pval_threshold=1e-4):\n",
    "        self.ttest_df = ttest_df\n",
    "        self.electrode_df = electrode_df\n",
    "        self.study_model_codes = study_model_codes\n",
    "        self.pval_threshold = pval_threshold\n",
    "\n",
    "    def get_contrast_inputs(self):\n",
    "        # get least-significant p-value result per model -- electrode\n",
    "        electrode_pvals = self.ttest_df.loc[(slice(None), slice(None), \"baseline\"), \"log_pval\"].groupby([\"model2\", \"subject\", \"output_dim\"]).max()\n",
    "        # insert zero pvals for missing model--electrode combinations\n",
    "        electrode_pvals = electrode_pvals.reindex(pd.MultiIndex.from_tuples(\n",
    "             [(model, subject, output_dim)\n",
    "              for subject, output_dim in self.electrode_df.index\n",
    "              for model in self.study_model_codes],\n",
    "             names=[\"model2\", \"subject\", \"output_dim\"])) \\\n",
    "                .fillna(0.)\n",
    "\n",
    "        return electrode_pvals\n",
    "    \n",
    "    def get_contrast_outcome(self, inputs, positive_models, negative_models):\n",
    "        outcomes = inputs.groupby([\"subject\", \"output_dim\"]).apply(\n",
    "            lambda xs: xs.loc[positive_models].min() - xs.loc[negative_models].min()) \\\n",
    "            .sort_values(ascending=True) \\\n",
    "            .rename(\"contrast_value\").to_frame()\n",
    "        outcomes[\"positive_pval\"] = inputs.loc[positive_models].groupby([\"subject\", \"output_dim\"]).min()\n",
    "\n",
    "        # add qualitative label\n",
    "        outcomes[\"outcome\"] = None\n",
    "        outcomes.loc[(outcomes[\"positive_pval\"] < -np.log10(self.pval_threshold)) & (outcomes[\"contrast_value\"] <= -1), \"outcome\"] = \"positive\"\n",
    "        outcomes.loc[(outcomes[\"positive_pval\"] < -np.log10(self.pval_threshold)) & (outcomes[\"contrast_value\"] >= 1), \"outcome\"] = \"negative\"\n",
    "        outcomes.loc[(outcomes[\"positive_pval\"] < -np.log10(self.pval_threshold)) & (outcomes[\"contrast_value\"].abs() <= 0.5), \"outcome\"] = \"balanced\"\n",
    "\n",
    "        return outcomes\n",
    "    \n",
    "\n",
    "class R2Contrast:\n",
    "    \"\"\"\n",
    "    Defines a qualitative model contrast based on relative R2 improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, ttest_df, scores_df, electrode_df,\n",
    "                 study_model_codes,\n",
    "                 mode: Literal[\"relative\", \"absolute\"] = \"absolute\",\n",
    "                 r2_threshold=0.1,\n",
    "                 r2_contrast_threshold=0.1):\n",
    "        self.ttest_df = ttest_df\n",
    "        self.scores_df = scores_df\n",
    "        self.electrode_df = electrode_df\n",
    "        self.study_model_codes = study_model_codes\n",
    "\n",
    "        self.mode = mode\n",
    "        self.r2_threshold = r2_threshold\n",
    "        self.r2_contrast_threshold = r2_contrast_threshold\n",
    "\n",
    "    def get_contrast_inputs(self):\n",
    "        r2_comparison = self.scores_df.xs(baseline_model, level=\"model1\")\n",
    "        # r2_comparison = r2_comparison.groupby([\"subject\", \"model2\", \"model\", \"output_dim\"]).score.mean().reset_index()\n",
    "        r2_comparison.loc[r2_comparison.model != baseline_model, \"model\"] = \"full_model\"\n",
    "        r2_comparison = r2_comparison.reset_index().pivot_table(index=[\"subject\", \"model2\", \"output_dim\", \"fold\"], columns=\"model\", values=\"score\")\n",
    "\n",
    "        # avoid using negative values as baseline reference\n",
    "        baseline_reference = r2_comparison[baseline_model]\n",
    "        baseline_relative_reference = baseline_reference[baseline_reference > 0]\n",
    "        r2_comparison[\"absolute_improvement\"] = r2_comparison[\"full_model\"] - baseline_reference.combine(0, max)\n",
    "        r2_comparison[\"relative_improvement\"] = r2_comparison[\"absolute_improvement\"] / baseline_relative_reference\n",
    "\n",
    "        # mean across folds\n",
    "        r2_comparison = r2_comparison.groupby([\"subject\", \"model2\", \"output_dim\"]).mean()\n",
    "        \n",
    "        r2_comparison = r2_comparison.reorder_levels([\"model2\", \"subject\", \"output_dim\"])\n",
    "\n",
    "        return r2_comparison\n",
    "    \n",
    "    def get_contrast_outcome(self, inputs, positive_models, negative_models):\n",
    "        # # compare the minimum relative improvement of positive model set\n",
    "        # # to the maximum relative improvement of negative model set\n",
    "        # outcomes = inputs.relative_improvement.groupby([\"subject\", \"output_dim\"]).apply(\n",
    "        #     lambda xs: xs.loc[positive_models].min() - max(0, xs.loc[negative_models].max())) \\\n",
    "        #     .sort_values(ascending=False) \\\n",
    "        #     .rename(\"contrast_value\").to_frame()\n",
    "        \n",
    "        # NB most stringent test -- we take the MAXIMUM improvement of the negative models\n",
    "        # and the MINIMUM improvement of the positive models\n",
    "        outcomes = pd.DataFrame({\n",
    "            \"positive_r2_relative_improvement\": inputs.loc[positive_models, \"relative_improvement\"].groupby([\"subject\", \"output_dim\"]).min(),\n",
    "            \"positive_r2_absolute_improvement\": inputs.loc[positive_models, \"absolute_improvement\"].groupby([\"subject\", \"output_dim\"]).min(),\n",
    "            \"positive_r2_absolute\": inputs.loc[positive_models, \"full_model\"].groupby([\"subject\", \"output_dim\"]).min(),\n",
    "\n",
    "            \"negative_r2_relative_improvement\": inputs.loc[negative_models, \"relative_improvement\"].groupby([\"subject\", \"output_dim\"]).max(),\n",
    "            \"negative_r2_absolute_improvement\": inputs.loc[negative_models, \"absolute_improvement\"].groupby([\"subject\", \"output_dim\"]).max(),\n",
    "            \"negative_r2_absolute\": inputs.loc[negative_models, \"full_model\"].groupby([\"subject\", \"output_dim\"]).max(),\n",
    "        })\n",
    "\n",
    "        if self.mode == \"relative\":\n",
    "            outcomes[\"contrast_value\"] = outcomes[\"positive_r2_relative_improvement\"] - outcomes[\"negative_r2_relative_improvement\"].combine(0, max)\n",
    "        elif self.mode == \"absolute\":\n",
    "            outcomes[\"contrast_value\"] = outcomes[\"positive_r2_absolute\"] - outcomes[\"negative_r2_absolute\"].combine(0, max)\n",
    "        # exclude overfit models\n",
    "        outcomes.loc[outcomes[\"positive_r2_absolute\"] < 0, \"contrast_value\"] = 0\n",
    "\n",
    "        outcomes = outcomes.sort_values(\"contrast_value\", ascending=False)\n",
    "\n",
    "        # add qualitative label\n",
    "        outcomes[\"outcome\"] = None\n",
    "\n",
    "        if self.mode == \"relative\":\n",
    "            outcomes.loc[(outcomes[\"positive_r2_relative_improvement\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"] > -self.r2_contrast_threshold), \"outcome\"] = \"positive\"\n",
    "            outcomes.loc[(outcomes[\"positive_r2_relative_improvement\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"] <= self.r2_contrast_threshold), \"outcome\"] = \"negative\"\n",
    "            outcomes.loc[(outcomes[\"positive_r2_relative_improvement\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"].abs() <= self.r2_contrast_threshold), \"outcome\"] = \"balanced\"\n",
    "        elif self.mode == \"absolute\":\n",
    "            outcomes.loc[(outcomes[\"positive_r2_absolute\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"] > -self.r2_contrast_threshold), \"outcome\"] = \"positive\"\n",
    "            outcomes.loc[(outcomes[\"positive_r2_absolute\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"] <= self.r2_contrast_threshold), \"outcome\"] = \"negative\"\n",
    "            outcomes.loc[(outcomes[\"positive_r2_absolute\"] > self.r2_threshold)\n",
    "                        & (outcomes[\"contrast_value\"].abs() <= self.r2_contrast_threshold), \"outcome\"] = \"balanced\"\n",
    "        # exclude overfit models\n",
    "        outcomes.loc[outcomes[\"positive_r2_absolute\"] < 0, \"outcome\"] = None\n",
    "\n",
    "        return outcomes\n",
    "\n",
    "\n",
    "CONTRAST_METHODS = {\n",
    "    \"pval\": PValueContrast,\n",
    "    # \"relative_r2_10\": partial(RelativeR2Contrast, relative_r2_threshold=0.1, relative_r2_contrast_threshold=0.1),\n",
    "    \"absolute_r2_1e-3\": partial(R2Contrast, r2_threshold=1e-3, r2_contrast_threshold=1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_outcomes = {}\n",
    "for contrast_method, contraster in CONTRAST_METHODS.items():\n",
    "    for contrast_name, (positive_models, negative_models) in contrasts.items():\n",
    "        contraster = CONTRAST_METHODS[contrast_method](ttest_df, scores_df, electrode_df,\n",
    "                                                    study_model_codes=study_model_codes)\n",
    "        contrast_inputs = contraster.get_contrast_inputs()\n",
    "        assert contrast_inputs.index.names == [\"model2\", \"subject\", \"output_dim\"], \\\n",
    "            f\"Unexpected index names: {contrast_inputs.index.names}\"\n",
    "\n",
    "        positive_models_ = set(positive_models) & set(study_model_codes)\n",
    "        negative_models_ = set(negative_models) & set(study_model_codes)\n",
    "        if not positive_models or not negative_models_:\n",
    "            raise ValueError(\"Missing all negative models or all positive models\")\n",
    "        if positive_models_ != set(positive_models):\n",
    "            L.warning(\"Missing some positive models: %s\", set(positive_models) - positive_models_)\n",
    "        if negative_models_ != set(negative_models):\n",
    "            L.warning(\"Missing some negative models: %s\", set(negative_models) - negative_models_)\n",
    "        positive_models_ = list(positive_models_)\n",
    "        negative_models_ = list(negative_models_)\n",
    "\n",
    "        contrast_outcomes[contrast_method, contrast_name] = contraster.get_contrast_outcome(\n",
    "            contrast_inputs, positive_models_, negative_models_)\n",
    "    \n",
    "contrast_outcomes_df = pd.concat(contrast_outcomes, names=[\"contrast_method\", \"contrast\"])\n",
    "contrast_outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(contrast_outcomes_df.index.get_level_values(\"contrast_method\").unique()) == 2:\n",
    "    cm1, cm2 = contrast_outcomes_df.index.get_level_values(\"contrast_method\").unique()\n",
    "    contrast_confusion = contrast_outcomes_df.reset_index().pivot(index=[\"contrast\", \"subject\", \"output_dim\"], columns=[\"contrast_method\"], values=\"outcome\") \\\n",
    "        .groupby([\"contrast\"]).apply(lambda xs: pd.crosstab(xs[cm1], xs[cm2]))\n",
    "    print(contrast_confusion)\n",
    "\n",
    "    sns.heatmap(contrast_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = contrast_outcomes_df.reset_index().pivot(index=[\"contrast\", \"subject\", \"output_dim\"], columns=[\"contrast_method\"], values=\"outcome\")\n",
    "hm = hm.applymap({\"negative\": -1, \"balanced\": 1, \"positive\": 2}.get).fillna(0.)\n",
    "sns.heatmap(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_outcomes_pivot = contrast_outcomes_df.reset_index().pivot(\n",
    "    index=[\"subject\", \"output_dim\"], columns=[\"contrast_method\", \"contrast\"],\n",
    "    values=[\"outcome\", \"contrast_value\"])\n",
    "contrast_outcomes_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_to_plot = contrast_outcomes_pivot[\"contrast_value\"].dropna().astype(float)\n",
    "outcomes_to_plot = outcomes_to_plot.loc[~(outcomes_to_plot == 0).all(axis=1)]\n",
    "# normalize within measure\n",
    "outcomes_to_plot = outcomes_to_plot.stack().apply(lambda xs: (xs - xs.mean()) / xs.std()).unstack()\n",
    "sns.clustermap(outcomes_to_plot, col_cluster=False, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_outcomes_df.to_csv(Path(output_dir) / \"contrasts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
