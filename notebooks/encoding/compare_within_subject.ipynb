{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_1samp\n",
    "import torch\n",
    "\n",
    "from src.encoding.ecog.timit import trf_grid_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "subject = \"EC260\"\n",
    "dataset = \"timit-no_repeats\"\n",
    "\n",
    "model1 = \"baseline\"\n",
    "model2 = \"ph-ls-word_broad-hinge-w2v2_8-l2norm\"\n",
    "\n",
    "model1_scores_path = f\"outputs/encoders/{dataset}/{model1}/{subject}/scores.csv\"\n",
    "model2_scores_path = f\"outputs/encoders/{dataset}/{model2}/{subject}/scores.csv\"\n",
    "model1_coefs_path = f\"outputs/encoders/{dataset}/{model1}/{subject}/coefs.pkl\"\n",
    "model2_coefs_path = f\"outputs/encoders/{dataset}/{model2}/{subject}/coefs.pkl\"\n",
    "model1_model_path = f\"outputs/encoders/{dataset}/{model1}/{subject}/model.pkl\"\n",
    "model2_model_path = f\"outputs/encoders/{dataset}/{model2}/{subject}/model.pkl\"\n",
    "\n",
    "model2_permutation_score_paths = {\n",
    "    \"units\": [\n",
    "        f\"outputs/encoders-permute_units/0/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_units/1/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_units/2/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_units/3/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_units/4/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "    ],\n",
    "    \"shift\": [\n",
    "        f\"outputs/encoders-permute_shift/0/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_shift/1/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_shift/2/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_shift/3/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "        f\"outputs/encoders-permute_shift/4/{dataset}/{model2}/{subject}/scores.csv\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_scores = pd.read_csv(model1_scores_path)\n",
    "model2_scores = pd.read_csv(model2_scores_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(model1_scores.output_dim) == set(model2_scores.output_dim)\n",
    "\n",
    "# output dim -- output name mapping should be consistent between evaluations\n",
    "assert set(model1_scores[[\"output_dim\", \"output_name\"]].to_records(index=False).tolist()) == \\\n",
    "         set(model2_scores[[\"output_dim\", \"output_name\"]].to_records(index=False).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_permutation_scores = {\n",
    "    permutation_name: pd.concat([\n",
    "            pd.read_csv(permutation_scores_path)\n",
    "            for permutation_scores_path in permutation_scores_paths\n",
    "        ], names=[\"permutation_idx\"], keys=range(len(permutation_scores_paths)))\n",
    "    for permutation_name, permutation_scores_paths in model2_permutation_score_paths.items()\n",
    "}\n",
    "model2_permutation_scores = pd.concat(model2_permutation_scores, names=[\"permutation\"]) \\\n",
    "    .droplevel(-1).set_index([\"output_dim\", \"fold\"], append=True)\n",
    "model2_permutation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(model2_permutation_scores.index.get_level_values(\"output_dim\")) == set(model1_scores.output_dim)\n",
    "\n",
    "# output dim -- output name mapping should be consistent between evaluations\n",
    "assert set(model1_scores[[\"output_dim\", \"output_name\"]].to_records(index=False).tolist()) == \\\n",
    "         set(model2_permutation_scores.reset_index()[[\"output_dim\", \"output_name\"]].to_records(index=False).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any fit dimensions which are missing an output name. These are likely clinical channels\n",
    "# which weren't given an annotation in the research montage.\n",
    "# But make sure that, if we have these extra channels, they exist for all model fits. Otherwise\n",
    "# we might be looking at a different issue.\n",
    "missing_output_names = [tuple(sorted(set(scores_df.output_dim[scores_df.output_name.isna()])))\n",
    "                        for scores_df in [model1_scores, model2_scores, model2_permutation_scores.reset_index()]]\n",
    "assert len(set(missing_output_names)) == 1, \"missing output channel names should be consistent across model fits. inconsistent annotations used?\"\n",
    "\n",
    "# drop these channel labels now\n",
    "missing_output_names = missing_output_names[0]\n",
    "model1_scores = model1_scores[~model1_scores.output_dim.isin(missing_output_names)]\n",
    "model2_scores = model2_scores[~model2_scores.output_dim.isin(missing_output_names)]\n",
    "model2_permutation_scores = model2_permutation_scores[~model2_permutation_scores.index.get_level_values(\"output_dim\").isin(missing_output_names)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save merged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.concat([model1_scores, model2_scores], names=[\"model\"], keys=[model1, model2]) \\\n",
    "    .droplevel(-1) \\\n",
    "    .set_index([\"output_dim\", \"fold\"], append=True)\n",
    "all_scores.to_csv(Path(output_dir) / \"scores.csv\")\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_improvements = all_scores.loc[model2].score - all_scores.loc[model1].score.combine(0, max)\n",
    "all_improvements.loc[all_scores.loc[model2].score < 0] = np.nan\n",
    "all_improvements.to_csv(Path(output_dir) / \"improvements.csv\")\n",
    "all_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_improvements = pd.merge(model2_permutation_scores, all_scores.loc[model1], left_index=True, right_index=True, how=\"inner\",\n",
    "                                    suffixes=(\"_perm\", \"_model1\"))\n",
    "permutation_improvements[\"score_model1\"] = permutation_improvements.score_model1.combine(0, max)\n",
    "permutation_improvements.loc[permutation_improvements.score_perm < 0, \"score_perm\"] = np.nan\n",
    "permutation_improvements = (permutation_improvements.score_perm - permutation_improvements.score_model1).rename(\"score\")\n",
    "permutation_improvements.to_csv(Path(output_dir) / \"permutation_improvements.csv\")\n",
    "permutation_improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize electrode performance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_electrode_performance_distribution(score_data: pd.DataFrame, ax=None):\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots()\n",
    "\n",
    "    if \"electrode_group\" in score_data.columns:\n",
    "        sns.violinplot(x=\"electrode_group\", y=\"score\", data=score_data, ax=ax)\n",
    "        sns.swarmplot(x=\"electrode_group\", y=\"score\", color=\"black\",\n",
    "                      alpha=0.5, data=score_data, ax=ax)\n",
    "    else:\n",
    "        sns.violinplot(data=score_data, y=\"score\", ax=ax)\n",
    "        \n",
    "        sns.swarmplot(data=score_data, y=\"score\", color=\"black\",\n",
    "                      alpha=0.5, ax=ax)\n",
    "\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean_scores = all_scores.loc[model1].groupby(\"output_dim\").score.mean()\n",
    "plot_electrode_performance_distribution(baseline_mean_scores.to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_responsive_electrodes = baseline_mean_scores[baseline_mean_scores > 0.025].index\n",
    "speech_responsive_electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_data = all_improvements.groupby([\"output_dim\"]).mean().to_frame()\n",
    "swarm_data[\"electrode_group\"] = \"na\"\n",
    "swarm_data.loc[speech_responsive_electrodes, \"electrode_group\"] = \"speech responsive\"\n",
    "\n",
    "plot_electrode_performance_distribution(swarm_data.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_data = permutation_improvements.groupby([\"permutation\", \"output_dim\"]).mean().to_frame()\n",
    "swarm_data[\"electrode_group\"] = \"na\"\n",
    "swarm_data.loc[(slice(None), speech_responsive_electrodes), \"electrode_group\"] = \"speech responsive\"\n",
    "swarm_data = swarm_data.reset_index()\n",
    "\n",
    "f, axs = plt.subplots(swarm_data.permutation.nunique(), 1, figsize=(6, 5 * swarm_data.permutation.nunique()))\n",
    "for permutation, ax in zip(swarm_data.permutation.unique(), axs):\n",
    "    plot_electrode_performance_distribution(swarm_data[swarm_data.permutation == permutation], ax=ax)\n",
    "    ax.set_title(permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize contrast as heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as grid\n",
    "num_output_dims = len(model1_scores.output_dim.unique())\n",
    "grid_num_rows = np.ceil(np.sqrt(num_output_dims)).astype(int)\n",
    "grid_num_cols = np.ceil(num_output_dims / grid_num_rows).astype(int)\n",
    "\n",
    "def output_dim_to_grid_coords(output_dim):\n",
    "    return grid_num_cols - output_dim // grid_num_cols - 1, grid_num_rows - output_dim % grid_num_rows - 1\n",
    "\n",
    "electrode_grid = np.zeros((grid_num_rows, grid_num_cols)) * np.nan\n",
    "for i in range(num_output_dims):\n",
    "    x, y = output_dim_to_grid_coords(i)\n",
    "    electrode_grid[y, x] = i\n",
    "\n",
    "scores_grid = np.zeros((2, grid_num_rows, grid_num_cols)) * np.nan\n",
    "for i, scores in enumerate([model1_scores, model2_scores]):\n",
    "    for output_dim, scores_rows in scores.groupby(\"output_dim\"):\n",
    "        x, y = output_dim_to_grid_coords(output_dim)\n",
    "        mean_score = max(0, scores_rows.score.mean())\n",
    "        scores_grid[i, y, x] = mean_score\n",
    "\n",
    "scores_diff_grid = np.zeros((grid_num_rows, grid_num_cols)) * np.nan\n",
    "for output_dim, scores_rows in all_improvements.groupby(\"output_dim\"):\n",
    "    x, y = output_dim_to_grid_coords(output_dim)\n",
    "    scores_diff_grid[y, x] = scores_rows.mean()\n",
    "\n",
    "permutation_scores_diff_grid = np.zeros((grid_num_rows, grid_num_cols)) * np.nan\n",
    "for output_dim, scores_rows in permutation_improvements.groupby(\"output_dim\"):\n",
    "    x, y = output_dim_to_grid_coords(output_dim)\n",
    "    permutation_scores_diff_grid[y, x] = scores_rows.mean()\n",
    "\n",
    "vmin_abs = scores_grid.min()\n",
    "vmax_abs = scores_grid.max()\n",
    "\n",
    "vmin_diff = min(np.nanmin(scores_diff_grid), np.nanmin(permutation_scores_diff_grid))\n",
    "vmax_diff = max(np.nanmax(scores_diff_grid), np.nanmax(permutation_scores_diff_grid))\n",
    "\n",
    "f, axs = plt.subplots(5, 1, figsize=(6, 6 * 5))\n",
    "\n",
    "# Sanity check: plot electrode IDs in grid form. Cross-check this with recon\n",
    "sns.heatmap(electrode_grid, annot=True, fmt=\".0f\", ax=axs[0])\n",
    "axs[0].set_title(\"Electrode IDs\")\n",
    "\n",
    "sns.heatmap(scores_grid[0], vmin=vmin_abs, vmax=vmax_abs, ax=axs[1])\n",
    "axs[1].set_title(model1)\n",
    "\n",
    "sns.heatmap(scores_grid[1], vmin=vmin_abs, vmax=vmax_abs, ax=axs[2])\n",
    "axs[2].set_title(model2)\n",
    "\n",
    "sns.heatmap(scores_diff_grid, ax=axs[3], vmin=vmin_diff, vmax=vmax_diff,\n",
    "            center=0.0, cmap=\"RdBu\")\n",
    "axs[3].set_title(f\"{model2} - {model1}\")\n",
    "\n",
    "sns.heatmap(permutation_scores_diff_grid, ax=axs[4], vmin=vmin_diff, vmax=vmax_diff,\n",
    "            center=0.0, cmap=\"RdBu\")\n",
    "axs[4].set_title(f\"Permutation {model2} - {model1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_improvement = all_improvements.groupby(\"output_dim\").mean() > 0\n",
    "print(f\"Electrodes showing numerical improvement over baseline: \"\n",
    "      f\"{positive_improvement.sum()} ({positive_improvement.mean() * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_improvements = all_improvements.loc[positive_improvement[positive_improvement].index]\n",
    "study_permutation_improvements = pd.merge(permutation_improvements, positive_improvement[positive_improvement].rename(\"positive_improvement\"),\n",
    "         left_index=True, right_index=True, how=\"inner\").score \\\n",
    "    .groupby([\"output_dim\", \"fold\", \"permutation\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most stringent picture: take the minimum difference between full model and ANY permuted model,\n",
    "# marginalizing over permutation type\n",
    "improvements_over_permutation = (study_improvements - study_permutation_improvements) \\\n",
    "    .groupby([\"permutation\", \"output_dim\", \"fold\"]).min()\n",
    "improvements_over_permutation.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(improvements_over_permutation.groupby([\"permutation\", \"output_dim\"]).mean().reset_index(),\n",
    "                 x=\"permutation\", y=\"score\", kind=\"box\")\n",
    "g.axes.flat[0].axhline(0, color=\"k\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ttest_improvements) == 0:\n",
    "    print(\"No electrodes showing improvement. Stop.\")\n",
    "    pd.DataFrame().to_csv(Path(output_dir) / \"ttest_results.csv\")\n",
    "else:\n",
    "    ttest_results = improvements_over_permutation \\\n",
    "        .groupby([\"output_dim\", \"permutation\"]).apply(lambda xs: pd.Series(ttest_1samp(xs, 0), index=[\"tval\", \"pval\"])) \\\n",
    "        .unstack() \\\n",
    "        .sort_values(\"pval\")\n",
    "    ttest_results.to_csv(Path(output_dir) / \"ttest_results.csv\")\n",
    "\n",
    "    ttest_grid = np.zeros((grid_num_rows, grid_num_cols)) * np.nan\n",
    "    for output_dim, ttest_rows in ttest_results.groupby(\"output_dim\"):\n",
    "        x, y = output_dim_to_grid_coords(output_dim)\n",
    "        ttest_grid[y, x] = np.nanmin(ttest_rows.tval)\n",
    "\n",
    "    ax = sns.heatmap(ttest_grid)\n",
    "    ax.set_title(\"ttest t-values\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
