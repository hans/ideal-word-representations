{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "from matplotlib import transforms\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.analysis.trf import coefs_to_df\n",
    "from src.encoding.ecog import timit as timit_encoding, \\\n",
    "     AlignedECoGDataset, ContrastiveModelSnapshot, epoch_by_state_space\n",
    "from src.utils.timit import get_word_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/userdata/jgauthier/projects/neural-foundation-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"timit-no_repeats\"\n",
    "subject = \"EC212\"\n",
    "ttest_results_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/ttest.csv\"\n",
    "scores_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/scores.csv\"\n",
    "unique_variance_path = f\"outputs/encoder_unique_variance/{dataset}/baseline/{subject}/unique_variance.csv\"\n",
    "contrasts_path = f\"outputs/electrode_contrast/{dataset}/contrasts.csv\"\n",
    "\n",
    "baseline_model = \"baseline\"\n",
    "encoder_dirs = list(Path(\"outputs/encoders\").glob(f\"{dataset}/*/{subject}\"))\n",
    "\n",
    "output_dir = \".\"\n",
    "\n",
    "pval_threshold = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts_df = pd.read_csv(contrasts_path, index_col=[\"contrast_method\", \"contrast\", \"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(scores_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\"]).loc[dataset, subject]\n",
    "study_models = sorted(scores_df.index.get_level_values(\"model2\").unique())\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df = pd.read_csv(ttest_results_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\", \"output_dim\"]) \\\n",
    "    .loc[dataset].loc[subject].loc[study_models]\n",
    "ttest_df[\"log_pval\"] = np.log10(ttest_df[\"pval\"])\n",
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_filtered_df = ttest_df.dropna().sort_values(\"pval\", ascending=False) \\\n",
    "    .groupby([\"model2\", \"output_dim\"]).first()\n",
    "ttest_filtered_df = ttest_filtered_df.loc[ttest_filtered_df[\"pval\"] < pval_threshold]\n",
    "ttest_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variance_df = pd.read_csv(unique_variance_path, index_col=[\"dropped_feature\", \"fold\", \"output_dim\"])\n",
    "# ^ this is actually not unique variance, but the inputs to the calculation. let's do it:\n",
    "unique_variance = unique_variance_df.loc[np.nan].unique_variance_score - unique_variance_df[~unique_variance_df.index.get_level_values(\"dropped_feature\").isna()].unique_variance_score\n",
    "unique_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dirs = [Path(p) for p in encoder_dirs]\n",
    "encoder_dirs = {encoder_dir.parent.name: encoder_dir for encoder_dir in encoder_dirs\n",
    "                if encoder_dir.parent.name in [baseline_model] + study_models}\n",
    "encoders = {model_name: torch.load(encoder_dir / \"model.pkl\")\n",
    "            for model_name, encoder_dir in encoder_dirs.items()}\n",
    "encoder_names = sorted(encoders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.read_csv(encoder_dirs[baseline_model] / \"scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just need a random config in order to extract relevant paths and get outfile\n",
    "sample_model_path = encoder_dirs[next(iter(study_models))]\n",
    "with (sample_model_path / \".hydra\" / \"config.yaml\").open() as f:\n",
    "    model_config = OmegaConf.create(yaml.safe_load(f))\n",
    "out = timit_encoding.prepare_out_file(model_config, next(iter(model_config.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = ContrastiveModelSnapshot.from_config(model_config, next(iter(model_config.feature_sets.model_features.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = AlignedECoGDataset(snapshot, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrode_df = pd.read_csv(next(iter(encoder_dirs.values())) / \"electrodes.csv\")\n",
    "electrode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fit_electrodes = next(iter(encoders.values())).coef_.shape[0]\n",
    "electrode_names = electrode_df.head(num_fit_electrodes).electrode_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dfs = {model_name: coefs_to_df(torch.load(encoder_dir / \"coefs.pkl\"),\n",
    "                                    encoders[model_name].feature_names,\n",
    "                                    electrode_names,\n",
    "                                    encoders[model_name].sfreq)\n",
    "            for model_name, encoder_dir in tqdm(encoder_dirs.items())}\n",
    "coef_df = pd.concat(coef_dfs, names=[\"model\"]).droplevel(1)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts_df = contrasts_df.loc[contrasts_df.output_dim <= num_fit_electrodes]\n",
    "contrasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trf_features = coef_df.feature.unique()\n",
    "all_trf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute epoched HGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack together a new state space spec for sentence onset\n",
    "# nb state space bounds are inclusive, so we need to subtract 1 from the end of each bound\n",
    "trial_spec = StateSpaceAnalysisSpec(\n",
    "    aligned.total_num_frames,\n",
    "    [\"trial\"],\n",
    "    [sorted([(start, end - 1) for start, end in aligned.name_to_frame_bounds.values()])],\n",
    ")\n",
    "aligned._snapshot.all_state_spaces[\"trial\"] = trial_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_epochs = epoch_by_state_space(\n",
    "    aligned, \"trial\",\n",
    "    epoch_window=(-0.1, 1.),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)\n",
    "trial_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trial_epochs.groupby([\"epoch_idx\", \"electrode_idx\", \"epoch_sample\"]).value.count().max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_epochs = epoch_by_state_space(\n",
    "    aligned, \"word\",\n",
    "    epoch_window=(-0.1, 0.6),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_metadata = get_word_metadata(snapshot.all_state_spaces[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in word metadata\n",
    "word_epochs = pd.merge(\n",
    "    word_epochs, word_metadata,\n",
    "    left_on=[\"epoch_label\", \"instance_idx\"],\n",
    "    right_on=[\"label\", \"instance_idx\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_offset_epochs = epoch_by_state_space(\n",
    "    aligned, \"word\",\n",
    "    align_to=\"offset\",\n",
    "    epoch_window=(-0.6, 0.1),\n",
    "    baseline_window=(0., 0.1),\n",
    "    return_df=True)\n",
    "# Merge in word metadata\n",
    "word_offset_epochs = pd.merge(\n",
    "    word_offset_epochs, word_metadata,\n",
    "    left_on=[\"epoch_label\", \"instance_idx\"],\n",
    "    right_on=[\"label\", \"instance_idx\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\")\n",
    "\n",
    "word_offset_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_epochs = epoch_by_state_space(\n",
    "    aligned, \"syllable\",\n",
    "    epoch_window=(-0.1, 0.3),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_color_norm = plt.Normalize(0, len(encoder_names))\n",
    "model_color_mapper = plt.colormaps[\"tab10\"]\n",
    "get_model_color = lambda model_name: model_color_mapper(model_color_norm(encoder_names.index(model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correspondences between electrodes significant under different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pvals = ttest_filtered_df.pivot_table(values=\"log_pval\", index=\"model2\", columns=\"output_dim\").fillna(0)\n",
    "log_pvals.index.name = \"model_name\"\n",
    "log_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(log_pvals, vmax=0, xticklabels=1, figsize=(20, 16))\n",
    "g.ax_heatmap.set_xlabel(\"Electrode\")\n",
    "g.ax_heatmap.set_ylabel(\"Model name\")\n",
    "g.ax_heatmap.set_xticks([])\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pvals_flat = log_pvals.copy()\n",
    "log_pvals_flat = log_pvals_flat.reset_index().melt(id_vars=\"model_name\", value_name=\"log_pval\")\n",
    "log_pvals_flat = log_pvals_flat[log_pvals_flat.log_pval < 0]\n",
    "log_pvals_flat = pd.merge(log_pvals_flat, electrode_df, left_on=[\"output_dim\"], right_on=[\"electrode_idx\"])\n",
    "log_pvals_flat = log_pvals_flat.set_index(\"model_name\")\n",
    "log_pvals_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "venn_labels = {\n",
    "    \"Word\": \"ph-ls-word_broad-hinge-w2v2_8-l2norm\",\n",
    "    \"Word discrim2\": \"ph-ls-word_broad-hinge-w2v2_8-discrim2-l2norm\",\n",
    "    \"Phoneme\": \"phoneme-w2v2_8-l2norm\"\n",
    "}\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 15))\n",
    "venn3(\n",
    "    [set(log_pvals_flat.loc[model_name].electrode_idx)\n",
    "     if model_name in log_pvals_flat.index\n",
    "     else set()\n",
    "     for model_name in venn_labels.values()],\n",
    "    set_labels=[label if model_name in log_pvals_flat.index else \"\"\n",
    "                for label, model_name in venn_labels.items()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = sorted(log_pvals_flat.roi.unique())\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(rois) / n_cols))\n",
    "\n",
    "f, axes = plt.subplots(n_rows, n_cols, figsize=(10 * n_cols, 10 * n_rows))\n",
    "for roi, ax in zip(rois, axes.flat):\n",
    "    plot_data = [set(log_pvals_flat.loc[model_name].query(f\"roi == '{roi}'\").electrode_idx)\n",
    "                 if model_name in log_pvals_flat.index\n",
    "                 else set()\n",
    "                 for model_name in venn_labels.values()]\n",
    "    # skip labels for empty sets\n",
    "    plot_labels = [label if len(data) > 0 else \"\"\n",
    "                   for label, data in zip(venn_labels.keys(), plot_data)]\n",
    "\n",
    "    venn3(plot_data, set_labels=plot_labels, ax=ax)\n",
    "    ax.set_title(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation of baseline predictiveness and model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_relationship = scores_df.assign(model=scores_df.model.replace({model_name: \"full_model\" for model_name in set(scores_df.model) - {baseline_model}})) \\\n",
    "    .xs(baseline_model, level=\"model1\") \\\n",
    "    .reset_index().pivot(index=[\"model2\", \"output_dim\", \"fold\"], columns=\"model\", values=\"score\")\n",
    "score_relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=score_relationship.reset_index(), x=\"baseline\", y=\"full_model\", col=\"model2\", col_wrap=3,\n",
    "               facet_kws=dict(sharex=False, sharey=False))\n",
    "\n",
    "ax_min = 0.\n",
    "ax_max = score_relationship.max().max()\n",
    "for ax in g.axes.ravel():\n",
    "    ax.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\", alpha=0.4)\n",
    "    ax.set_xlim(ax_min, ax_max)\n",
    "    ax.set_ylim(ax_min, ax_max)\n",
    "    ax.set_xlabel(\"Baseline encoder $r^2$\")\n",
    "    ax.set_ylabel(\"Full model $r^2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation study by $p$-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get least-significant p-value result per model -- electrode\n",
    "electrode_pvals = ttest_df.loc[(slice(None), baseline_model), \"log_pval\"].groupby([\"model2\", \"output_dim\"]).max()\n",
    "# insert zero pvals for missing model--electrode combinations\n",
    "electrode_pvals = electrode_pvals.reindex(pd.MultiIndex.from_product([study_models, electrode_names.index], names=[\"model2\", \"output_dim\"])) \\\n",
    "    .fillna(0.)\n",
    "electrode_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_comparison = scores_df.xs(baseline_model, level=\"model1\")\n",
    "r2_comparison.loc[r2_comparison.model != baseline_model, \"model\"] = \"full_model\"\n",
    "r2_comparison = r2_comparison.reset_index().pivot_table(index=[\"model2\", \"output_dim\", \"fold\"], columns=\"model\", values=\"score\")\n",
    "r2_comparison[\"absolute_improvement\"] = r2_comparison[\"full_model\"] - r2_comparison[baseline_model].combine(0, max)\n",
    "r2_comparison.loc[r2_comparison[\"absolute_improvement\"] < 0, \"absolute_improvement\"] = 0\n",
    "r2_comparison = r2_comparison.sort_values(\"absolute_improvement\", ascending=False)\n",
    "r2_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.loc[study_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef_df = coef_df.loc[study_models].reset_index()\n",
    "# filter to electrodes of interest\n",
    "plot_coef_df = plot_coef_df[plot_coef_df.output_dim.isin([152])]\n",
    "# name model embedding coefficients according to model\n",
    "model_coefs = plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\")]\n",
    "plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\"), \"feature\"] = \\\n",
    "    model_coefs.model.str.cat(model_coefs.feature, sep=\"_\")\n",
    "\n",
    "# filter to features of interest\n",
    "features = sorted([f for f in coef_df.feature.unique() if not f.startswith(\"model_embedding\")])\n",
    "plot_coef_df_features = plot_coef_df[plot_coef_df.feature.str.startswith(tuple(features))]\n",
    "plot_coef_df_features = plot_coef_df_features[[\"fold\", \"feature\", \"output_dim\", \"time\", \"coef\"]]\n",
    "plot_coef_df_features[\"type\"] = \"basic_feature\"\n",
    "\n",
    "# add computed feature norms for embeddings\n",
    "plot_coef_df_embeddings = plot_coef_df[plot_coef_df.feature.str.contains(\"model_embedding\")]\n",
    "plot_coef_df_embeddings = plot_coef_df_embeddings.groupby([\"fold\", \"model\", \"output_dim\", \"time\"]) \\\n",
    "    .coef.apply(lambda xs: xs.abs().sum()).reset_index() \\\n",
    "    .rename(columns={\"model\": \"feature\"}).assign(type=\"model_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_contrast = \"word_dominant\"\n",
    "panel_contrast_df = contrasts_df.xs(panel_contrast, level=\"contrast\").xs(subject, level=\"subject\") \\\n",
    "    .reset_index().set_index([\"outcome\", \"output_dim\"])\n",
    "\n",
    "panel_contrast_electrodes = {\n",
    "    outcome: panel_contrast_df.loc[outcome].index.unique()\n",
    "    for outcome in panel_contrast_df.index.get_level_values(\"outcome\").unique()\n",
    "    if outcome not in [np.nan, None]\n",
    "}\n",
    "\n",
    "panel_contrast_electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_electrode_panel(\n",
    "        electrode, model_embeddings=None, features=None,\n",
    "        trial_epoch_kwargs=None,\n",
    "        word_epoch_kwargs=None,\n",
    "        word_epoch2_kwargs=None,\n",
    "        smoke_test=False):\n",
    "    figure = plt.figure(figsize=(48, 24) if not smoke_test else (10, 8))\n",
    "    gs = gridspec.GridSpec(3, 5, figure=figure,\n",
    "                           width_ratios=[3, 3, 2, 2, 1.5], hspace=0.25, wspace=0.25)\n",
    "    electrodes = [electrode]\n",
    "\n",
    "    errorbar = \"se\" if not smoke_test else None\n",
    "\n",
    "    if model_embeddings is None:\n",
    "        model_embeddings = sorted([m for m in electrode_pvals.index.get_level_values(\"model2\").unique() if m != \"baseline\"])\n",
    "    if features is None:\n",
    "        features = sorted([f for f in coef_df.feature.unique() if not f.startswith(\"model_embedding\")])\n",
    "\n",
    "    ##### plot electrode t-values and feature norms\n",
    "    print(\"performance plots\")\n",
    "\n",
    "    tval_ax = figure.add_subplot(gs[0, 0])\n",
    "    tval_ax.set_title(\"Improvement log $p$-values\\nby model embedding\")\n",
    "    tval_ax.axvline(np.log10(pval_threshold), color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "    r2_ax = figure.add_subplot(gs[0, 1])\n",
    "    r2_ax.set_title(\"Improvement $r^2$\\nby model embedding\")\n",
    "    feature_norm_ax = figure.add_subplot(gs[0, 2])\n",
    "    feature_norm_ax.set_title(\"Unique variance\")\n",
    "\n",
    "    tval_df = electrode_pvals.loc[model_embeddings].loc[(slice(None), electrodes)]\n",
    "    tval_df_order = tval_df.sort_values(ascending=True).index.get_level_values(\"model2\")\n",
    "    sns.barplot(data=tval_df.reset_index(), x=\"log_pval\", y=\"model2\",\n",
    "                ax=tval_ax, order=tval_df_order)\n",
    "    for ticklabel in tval_ax.get_yticklabels():\n",
    "        if ticklabel.get_text() in model_embeddings:\n",
    "            ticklabel.set_fontweight(\"bold\")\n",
    "\n",
    "    r2_df = r2_comparison.loc[study_models]\n",
    "    # r2_order = pdf[\"absolute_improvement\"].groupby(\"model2\").mean().sort_values(ascending=False).index\n",
    "    # share order + ticks with p-value plot\n",
    "    r2_order = tval_df_order\n",
    "    sns.barplot(data=r2_df.reset_index(), x=\"absolute_improvement\", y=\"model2\", order=r2_order, ax=r2_ax)\n",
    "    r2_ax.set_yticklabels([])\n",
    "    r2_ax.set_ylabel(None)\n",
    "\n",
    "    unique_variance_df = unique_variance.loc[(slice(None), electrodes)].reset_index().rename(columns={\"dropped_feature\": \"feature\"})\n",
    "    unique_variance_means = unique_variance_df.groupby(\"feature\").unique_variance_score.mean()\n",
    "    unique_variance_df_order = unique_variance_means[unique_variance_means >= 0].sort_values(ascending=False).index\n",
    "    sns.barplot(data=unique_variance_df,\n",
    "                x=\"unique_variance_score\", y=\"feature\",\n",
    "                ax=feature_norm_ax, order=unique_variance_df_order)\n",
    "    feature_norm_ax.set_xlim((0, feature_norm_ax.get_xlim()[1]))\n",
    "    for ticklabel in feature_norm_ax.get_yticklabels():\n",
    "        if ticklabel.get_text().startswith(tuple(features)):\n",
    "            ticklabel.set_fontweight(\"bold\")\n",
    "\n",
    "    #####\n",
    "    print(\"coef prep\")\n",
    "\n",
    "    # prepare single coefficient df\n",
    "    plot_coef_df = coef_df.loc[model_embeddings].reset_index()\n",
    "    # filter to electrodes of interest\n",
    "    plot_coef_df = plot_coef_df[plot_coef_df.output_dim.isin(electrodes)]\n",
    "    # name model embedding coefficients according to model\n",
    "    model_coefs = plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\")]\n",
    "    plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\"), \"feature\"] = \\\n",
    "        model_coefs.model.str.cat(model_coefs.feature, sep=\"_\")\n",
    "    \n",
    "    # filter to features of interest\n",
    "    plot_coef_df_features = plot_coef_df[plot_coef_df.feature.str.startswith(tuple(features))]\n",
    "    plot_coef_df_features = plot_coef_df_features[[\"fold\", \"feature\", \"output_dim\", \"time\", \"coef\"]]\n",
    "    plot_coef_df_features[\"type\"] = \"basic_feature\"\n",
    "    # add computed feature norms for embeddings\n",
    "    plot_coef_df_embeddings = plot_coef_df[plot_coef_df.feature.str.contains(\"model_embedding\")]\n",
    "    plot_coef_df_embeddings = plot_coef_df_embeddings.groupby([\"fold\", \"model\", \"output_dim\", \"time\"]) \\\n",
    "        .coef.apply(lambda xs: xs.abs().sum()).reset_index() \\\n",
    "        .rename(columns={\"model\": \"feature\"}).assign(type=\"model_embedding\")\n",
    "    \n",
    "    #####\n",
    "    # coef_line_ax = figure.add_subplot(gs[1, :])\n",
    "    # sns.lineplot(data=plot_coef_subset_df, x=\"time\", y=\"coef\", hue=\"feature\", style=\"type\", ax=coef_line_ax)\n",
    "\n",
    "    #####\n",
    "\n",
    "    print(\"coef plots\")\n",
    "\n",
    "    feature_coef_heatmap_ax = figure.add_subplot(gs[1, :2])\n",
    "    plot_coef_heatmap_df = plot_coef_df_features.pivot_table(\n",
    "        index=\"feature\", columns=\"time\", values=\"coef\", aggfunc=\"mean\")\n",
    "    plot_coef_heatmap_df = plot_coef_heatmap_df.loc[sorted(plot_coef_df_features.feature.unique())]\n",
    "    sns.heatmap(plot_coef_heatmap_df, ax=feature_coef_heatmap_ax, cmap=\"RdBu\", center=0, yticklabels=True)\n",
    "\n",
    "    model_coef_heatmap_ax = figure.add_subplot(gs[2, :2])\n",
    "    plot_coef_heatmap_df = plot_coef_df_embeddings.pivot_table(\n",
    "        index=\"feature\", columns=\"time\", values=\"coef\", aggfunc=\"mean\")\n",
    "    # # order by decreasing t-value\n",
    "    # plot_coef_heatmap_df = plot_coef_heatmap_df.loc[[model for model in tval_df_order if model in plot_coef_heatmap_df.index]]\n",
    "    # order by name\n",
    "    plot_coef_heatmap_df = plot_coef_heatmap_df.loc[sorted(plot_coef_df_embeddings.feature.unique())]\n",
    "    sns.heatmap(plot_coef_heatmap_df, ax=model_coef_heatmap_ax, cmap=\"RdBu\", center=0, yticklabels=True)\n",
    "\n",
    "    ##### contrast outcomes\n",
    "\n",
    "    outcome_int_map = {\"positive\": 2, \"balanced\": 1, \"none\": 7, \"negative\": 0}\n",
    "    outcome_heatmap_data = panel_contrast_df.xs(electrode, level=\"output_dim\").reset_index() \\\n",
    "        .set_index(\"contrast_method\").outcome.sort_index().fillna(\"none\")\n",
    "    outcome_ax = figure.add_subplot(gs[0, 4])\n",
    "    sns.heatmap(outcome_heatmap_data.map(outcome_int_map).to_frame(),\n",
    "        cmap=\"Set1\", vmin=min(outcome_int_map.values()), vmax=max(outcome_int_map.values()),\n",
    "        annot=outcome_heatmap_data.to_frame(), fmt=\"\", cbar=False, ax=outcome_ax)\n",
    "\n",
    "    #####\n",
    "\n",
    "    print(\"epochs plots\")\n",
    "\n",
    "    trial_epochs_ax = figure.add_subplot(gs[0, 3])\n",
    "    trial_epochs_ax.set_title(\"Trial ERP\")\n",
    "    trial_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_trial_epochs = trial_epochs[(trial_epochs.electrode_idx == electrode)]\n",
    "    sns.lineplot(data=plot_trial_epochs, x=\"epoch_time\", y=\"value\", ax=trial_epochs_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 **(trial_epoch_kwargs or {}))\n",
    "\n",
    "    word_epochs_ax = figure.add_subplot(gs[1, 2])\n",
    "    word_epochs_ax.set_title(\"Word ERP\")\n",
    "    word_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_epochs = word_epochs[word_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_epochs, x=\"epoch_time\", y=\"value\", ax=word_epochs_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 legend=False,  # legend is shared with offset plot\n",
    "                 **(word_epoch_kwargs or {}))\n",
    "    \n",
    "    word_epochs2_ax = figure.add_subplot(gs[2, 2])\n",
    "    word_epochs2_ax.set_title(\"Word ERP\")\n",
    "    word_epochs2_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_epochs2 = word_epochs[word_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_epochs2, x=\"epoch_time\", y=\"value\", ax=word_epochs2_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 legend=False,  # legend is shared with offset plot\n",
    "                 **(word_epoch2_kwargs or {}))\n",
    "\n",
    "    # syllable_epochs_ax = figure.add_subplot(gs[0, 3])\n",
    "    # syllable_epochs_ax.set_title(\"Syllable ERP\")\n",
    "    # syllable_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    # plot_syllable_epochs = syllable_epochs[syllable_epochs.electrode_idx == electrode]\n",
    "    # sns.lineplot(data=plot_syllable_epochs, x=\"epoch_time\", y=\"value\", ax=syllable_epochs_ax,\n",
    "    #              errorbar=errorbar)\n",
    "\n",
    "    word_offset_epochs_ax = figure.add_subplot(gs[1, 3])\n",
    "    word_offset_epochs_ax.set_title(\"Word offset ERP\")\n",
    "    word_offset_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_offset_epochs = word_offset_epochs[word_offset_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_offset_epochs, x=\"epoch_time\", y=\"value\", ax=word_offset_epochs_ax,\n",
    "                 errorbar=errorbar, **(word_epoch_kwargs or {}))\n",
    "    epochs1_ax = figure.add_subplot(gs[1, 4])\n",
    "    epochs1_ax.axis(\"off\")\n",
    "    epochs1_ax.legend(*word_offset_epochs_ax.get_legend_handles_labels(), loc=\"upper left\") \\\n",
    "                      .set_title(word_offset_epochs_ax.get_legend().get_title().get_text())\n",
    "    word_offset_epochs_ax.get_legend().remove()\n",
    "\n",
    "    word_offset_epoch2_ax = figure.add_subplot(gs[2, 3])\n",
    "    word_offset_epoch2_ax.set_title(\"Word offset ERP\")\n",
    "    word_offset_epoch2_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_offset_epoch2 = word_offset_epochs[word_offset_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_offset_epoch2, x=\"epoch_time\", y=\"value\", ax=word_offset_epoch2_ax,\n",
    "                 errorbar=errorbar, **(word_epoch2_kwargs or {}))\n",
    "    epochs2_ax = figure.add_subplot(gs[2, 4])\n",
    "    epochs2_ax.axis(\"off\")\n",
    "    epochs2_ax.legend(*word_offset_epoch2_ax.get_legend_handles_labels(), loc=\"upper left\") \\\n",
    "                      .set_title(word_offset_epoch2_ax.get_legend().get_title().get_text())\n",
    "    word_offset_epoch2_ax.get_legend().remove()\n",
    "\n",
    "    plt.suptitle(f\"Electrode {electrode} study\")\n",
    "    \n",
    "    return plot_coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = False\n",
    "for electrode in tqdm(panel_contrast_electrodes[\"positive\"]):\n",
    "    print(electrode)\n",
    "    with plt.rc_context(rc={\"font.size\": 24 if not smoke_test else 12}):\n",
    "        render_electrode_panel(\n",
    "            electrode, smoke_test=smoke_test,\n",
    "            model_embeddings=study_models,\n",
    "            word_epoch_kwargs=dict(hue=\"monosyllabic\"),\n",
    "            word_epoch2_kwargs=dict(hue=\"word_frequency_quantile\"))\n",
    "        f = plt.gcf()\n",
    "        f.savefig(f\"{output_dir}/electrode_panel-{subject}-{electrode}.png\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
