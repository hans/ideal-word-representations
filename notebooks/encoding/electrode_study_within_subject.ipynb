{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "from matplotlib import transforms\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.analysis.trf import coefs_to_df\n",
    "from src.encoding.ecog.timit import OutFile\n",
    "from src.encoding.ecog import timit as timit_encoding, \\\n",
    "     AlignedECoGDataset, ContrastiveModelSnapshot, epoch_by_state_space\n",
    "from src.utils.timit import get_word_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"timit\"\n",
    "subject = \"EC196\"\n",
    "study_models = [\n",
    "    \"random8\",\n",
    "    \"phoneme\",\n",
    "    \"biphone_pred\",\n",
    "    \"biphone_recon\",\n",
    "    \"next_phoneme\",\n",
    "    \"syllable\",\n",
    "    \"word_broad-aniso2-w2v2_8\",\n",
    "]\n",
    "ttest_results_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/ttest.csv\"\n",
    "scores_path = f\"outputs/encoder_comparison_across_subjects/{dataset}/scores.csv\"\n",
    "unique_variance_path = f\"outputs/encoder_unique_variance/{dataset}/baseline/{subject}/unique_variance.csv\"\n",
    "\n",
    "encoder_dirs = list(Path(\"outputs/encoders\").glob(f\"{dataset}/*/{subject}\"))\n",
    "\n",
    "output_dir = \".\"\n",
    "\n",
    "pval_threshold = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(scores_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\"]).loc[dataset, subject]\n",
    "if study_models is None:\n",
    "    study_models = sorted(scores_df.index.get_level_values(\"model2\").unique())\n",
    "else:\n",
    "    scores_df = scores_df.loc[scores_df.index.get_level_values(\"model2\").isin(study_models)]\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df = pd.read_csv(ttest_results_path, index_col=[\"dataset\", \"subject\", \"model2\", \"model1\", \"output_dim\"]) \\\n",
    "    .loc[dataset].loc[subject].loc[study_models]\n",
    "ttest_df[\"log_pval\"] = np.log10(ttest_df[\"pval\"])\n",
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_filtered_df = ttest_df.dropna().sort_values(\"pval\", ascending=False) \\\n",
    "    .groupby([\"model2\", \"output_dim\"]).first()\n",
    "ttest_filtered_df = ttest_filtered_df.loc[ttest_filtered_df[\"pval\"] < pval_threshold]\n",
    "ttest_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variance_df = pd.read_csv(unique_variance_path, index_col=[\"dropped_feature\", \"fold\", \"output_dim\"])\n",
    "# ^ this is actually not unique variance, but the inputs to the calculation. let's do it:\n",
    "unique_variance = unique_variance_df.loc[np.nan].unique_variance_score - unique_variance_df[~unique_variance_df.index.get_level_values(\"dropped_feature\").isna()].unique_variance_score\n",
    "unique_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dirs = [Path(p) for p in encoder_dirs]\n",
    "encoder_dirs = {encoder_dir.parent.name: encoder_dir for encoder_dir in encoder_dirs\n",
    "                if encoder_dir.parent.name in [\"baseline\"] + study_models}\n",
    "encoders = {model_name: torch.load(encoder_dir / \"model.pkl\")\n",
    "            for model_name, encoder_dir in encoder_dirs.items()}\n",
    "encoder_names = sorted(encoders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = pd.read_csv(encoder_dirs[\"baseline\"] / \"scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just need a random config in order to extract relevant paths and get outfile\n",
    "sample_model_path = encoder_dirs[\"phoneme\"]\n",
    "with (sample_model_path / \".hydra\" / \"config.yaml\").open() as f:\n",
    "    model_config = OmegaConf.create(yaml.safe_load(f))\n",
    "out = timit_encoding.prepare_out_file(model_config, next(iter(model_config.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = ContrastiveModelSnapshot.from_config(model_config, next(iter(model_config.feature_sets.model_features.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = AlignedECoGDataset(snapshot, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrode_df = pd.read_csv(next(iter(encoder_dirs.values())) / \"electrodes.csv\")\n",
    "electrode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fit_electrodes = next(iter(encoders.values())).coef_.shape[0]\n",
    "electrode_names = electrode_df.head(num_fit_electrodes).electrode_name\n",
    "coef_dfs = {model_name: coefs_to_df(torch.load(encoder_dir / \"coefs.pkl\"),\n",
    "                                    encoders[model_name].feature_names,\n",
    "                                    electrode_names,\n",
    "                                    encoders[model_name].sfreq)\n",
    "            for model_name, encoder_dir in tqdm(encoder_dirs.items())}\n",
    "coef_df = pd.concat(coef_dfs, names=[\"model\"]).droplevel(1)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trf_features = coef_df.feature.unique()\n",
    "all_trf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute epoched HGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack together a new state space spec for sentence onset\n",
    "# nb state space bounds are inclusive, so we need to subtract 1 from the end of each bound\n",
    "trial_spec = StateSpaceAnalysisSpec(\n",
    "    aligned.total_num_frames,\n",
    "    [\"trial\"],\n",
    "    [sorted([(start, end - 1) for start, end in aligned.name_to_frame_bounds.values()])],\n",
    ")\n",
    "aligned._snapshot.all_state_spaces[\"trial\"] = trial_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_epochs = epoch_by_state_space(\n",
    "    aligned, \"trial\",\n",
    "    epoch_window=(-0.1, 1.),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)\n",
    "trial_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trial_epochs.groupby([\"epoch_idx\", \"electrode_idx\", \"epoch_sample\"]).value.count().max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_epochs = epoch_by_state_space(\n",
    "    aligned, \"word\",\n",
    "    epoch_window=(-0.1, 0.6),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_metadata = get_word_metadata(snapshot.all_state_spaces[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in word metadata\n",
    "word_epochs = pd.merge(\n",
    "    word_epochs, word_metadata,\n",
    "    left_on=[\"epoch_label\", \"instance_idx\"],\n",
    "    right_on=[\"label\", \"instance_idx\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_offset_epochs = epoch_by_state_space(\n",
    "    aligned, \"word\",\n",
    "    align_to=\"offset\",\n",
    "    epoch_window=(-0.6, 0.1),\n",
    "    baseline_window=(0., 0.1),\n",
    "    return_df=True)\n",
    "# Merge in word metadata\n",
    "word_offset_epochs = pd.merge(\n",
    "    word_offset_epochs, word_metadata,\n",
    "    left_on=[\"epoch_label\", \"instance_idx\"],\n",
    "    right_on=[\"label\", \"instance_idx\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\")\n",
    "\n",
    "word_offset_epochs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_epochs = epoch_by_state_space(\n",
    "    aligned, \"syllable\",\n",
    "    epoch_window=(-0.1, 0.3),\n",
    "    baseline_window=(-0.1, 0.),\n",
    "    return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_color_norm = plt.Normalize(0, len(encoder_names))\n",
    "model_color_mapper = plt.colormaps[\"tab10\"]\n",
    "get_model_color = lambda model_name: model_color_mapper(model_color_norm(encoder_names.index(model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correspondences between electrodes significant under different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pvals = ttest_filtered_df.pivot_table(values=\"log_pval\", index=\"model2\", columns=\"output_dim\").fillna(0)\n",
    "log_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(log_pvals, vmax=0, xticklabels=1, figsize=(14, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation of baseline predictiveness and model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_relationship = scores_df.assign(model=scores_df.model.replace({model_name: \"full_model\" for model_name in set(scores_df.model) - {\"baseline\"}})) \\\n",
    "    .reset_index().pivot(index=[\"model2\", \"output_dim\", \"fold\"], columns=\"model\", values=\"score\")\n",
    "score_relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=score_relationship.reset_index(), x=\"baseline\", y=\"full_model\", col=\"model2\", col_wrap=3,\n",
    "               facet_kws=dict(sharex=False, sharey=False))\n",
    "\n",
    "ax_min = 0.\n",
    "ax_max = score_relationship.max().max()\n",
    "for ax in g.axes.ravel():\n",
    "    ax.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\", alpha=0.4)\n",
    "    ax.set_xlim(ax_min, ax_max)\n",
    "    ax.set_ylim(ax_min, ax_max)\n",
    "    ax.set_xlabel(\"Baseline encoder $r^2$\")\n",
    "    ax.set_ylabel(\"Full model $r^2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation of model embedding and baseline predictivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_responsive_threshold = 0.01\n",
    "speech_responsive_electrodes = baseline_scores.groupby(\"output_dim\").score.mean()\n",
    "speech_responsive_electrodes = speech_responsive_electrodes[speech_responsive_electrodes >= speech_responsive_threshold].index\n",
    "speech_responsive_electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding_improvements = (score_relationship.full_model - score_relationship.baseline).unstack(\"model2\").groupby(\"output_dim\").mean()\n",
    "# not interested in overfit electrodes\n",
    "model_embedding_improvements[model_embedding_improvements < 0] = np.nan\n",
    "model_embedding_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_feature_improvements = unique_variance.unstack(\"dropped_feature\").groupby(\"output_dim\").mean()\n",
    "# not interested in overfit electrodes\n",
    "baseline_feature_improvements[baseline_feature_improvements < 0] = np.nan\n",
    "baseline_feature_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(pd.concat([model_embedding_improvements, baseline_feature_improvements], axis=1)\n",
    "                 .corr().loc[baseline_feature_improvements.columns, model_embedding_improvements.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_improvements = pd.merge(baseline_feature_improvements, model_embedding_improvements,\n",
    "                            left_index=True, right_index=True, how=\"left\", validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent improvement within each model as % of maximum unique variance\n",
    "all_improvements_relative = all_improvements.apply(lambda xs: xs / (xs.max() - xs.min()), axis=0)\n",
    "all_improvements_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_improvements_relative.sort_values(\"word_broad-aniso2-w2v2_8\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_improvements[\"word_broad-aniso2-w2v2_8\"].fillna(0) - all_improvements[[\"biphone_pred\", \"biphone_recon\", \"phoneme\"]].max(axis=1).fillna(0)).dropna().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocation study by $p$-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get least-significant p-value result per model -- electrode\n",
    "electrode_pvals = ttest_df.loc[(slice(None), \"baseline\"), \"log_pval\"].groupby([\"model2\", \"output_dim\"]).max()\n",
    "# insert zero pvals for missing model--electrode combinations\n",
    "electrode_pvals = electrode_pvals.reindex(pd.MultiIndex.from_product([study_models, electrode_names.index], names=[\"model2\", \"output_dim\"])) \\\n",
    "    .fillna(0.)\n",
    "electrode_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_focus = \"word_broad-aniso2-w2v2_8\"\n",
    "contrast_negatives = [\"phoneme\", \"biphone_pred\", \"biphone_recon\"]\n",
    "word_contrasts = electrode_pvals.groupby(\"output_dim\").apply(\n",
    "    lambda xs: xs.loc[contrast_focus] - xs.loc[contrast_negatives].min()).sort_values(ascending=True)\n",
    "word_contrasts = word_contrasts.rename(\"word_contrast\").to_frame().droplevel(-1)\n",
    "word_contrasts = pd.merge(word_contrasts, electrode_pvals.loc[contrast_focus], left_index=True, right_index=True)\n",
    "word_contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_greatest_contrast = word_contrasts.head(6).index.get_level_values(0)\n",
    "plot_greatest_contrast_df = coef_df.loc[contrast_focus]\n",
    "plot_greatest_contrast_df = plot_greatest_contrast_df[plot_greatest_contrast_df.output_dim.isin(plot_greatest_contrast)]\n",
    "plot_greatest_contrast_df = plot_greatest_contrast_df[plot_greatest_contrast_df.feature.str.startswith(\"model_embedding\")]\n",
    "\n",
    "g = sns.relplot(data=plot_greatest_contrast_df,\n",
    "                col=\"output_dim\", col_wrap=2, col_order=plot_greatest_contrast,\n",
    "                x=\"time\", y=\"coef\", hue=\"feature\", kind=\"line\", errorbar=\"se\",\n",
    "                facet_kws=dict(sharex=False))\n",
    "for ax in g.axes.ravel():\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_electrode_panel(\n",
    "        electrode, model_embeddings=None, features=None,\n",
    "        trial_epoch_kwargs=None,\n",
    "        word_epoch_kwargs=None,\n",
    "        word_epoch2_kwargs=None,\n",
    "        smoke_test=False):\n",
    "    figure = plt.figure(figsize=(32, 24) if not smoke_test else (10, 8))\n",
    "    gs = gridspec.GridSpec(3, 4, figure=figure,\n",
    "                           width_ratios=[3, 3, 2, 2], hspace=0.25, wspace=0.25)\n",
    "    electrodes = [electrode]\n",
    "\n",
    "    errorbar = \"se\" if not smoke_test else None\n",
    "\n",
    "    if model_embeddings is None:\n",
    "        model_embeddings = sorted([m for m in electrode_pvals.index.get_level_values(\"model2\").unique() if m != \"baseline\"])\n",
    "    if features is None:\n",
    "        features = sorted([f for f in coef_df.feature.unique() if not f.startswith(\"model_embedding\")])\n",
    "\n",
    "    ##### plot electrode t-values and feature norms\n",
    "\n",
    "    tval_ax = figure.add_subplot(gs[0, 0])\n",
    "    tval_ax.set_title(\"Improvement log $p$-values by model embedding\")\n",
    "    tval_ax.axvline(np.log10(pval_threshold), color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "    feature_norm_ax = figure.add_subplot(gs[0, 1])\n",
    "    feature_norm_ax.set_title(\"Unique variance\")\n",
    "\n",
    "    tval_df = electrode_pvals.loc[model_embeddings].loc[(slice(None), electrodes)]\n",
    "    tval_df_order = tval_df.sort_values(ascending=True).index.get_level_values(\"model2\")\n",
    "    sns.barplot(data=tval_df.reset_index(), x=\"log_pval\", y=\"model2\",\n",
    "                ax=tval_ax, order=tval_df_order)\n",
    "    for ticklabel in tval_ax.get_yticklabels():\n",
    "        if ticklabel.get_text() in model_embeddings:\n",
    "            ticklabel.set_fontweight(\"bold\")\n",
    "\n",
    "    unique_variance_df = unique_variance.loc[(slice(None), electrodes)].reset_index().rename(columns={\"dropped_feature\": \"feature\"})\n",
    "    unique_variance_means = unique_variance_df.groupby(\"feature\").unique_variance_score.mean()\n",
    "    unique_variance_df_order = unique_variance_means[unique_variance_means >= 0].sort_values(ascending=False).index\n",
    "    sns.barplot(data=unique_variance_df,\n",
    "                x=\"unique_variance_score\", y=\"feature\",\n",
    "                ax=feature_norm_ax, order=unique_variance_df_order)\n",
    "    feature_norm_ax.set_xlim((0, feature_norm_ax.get_xlim()[1]))\n",
    "    for ticklabel in feature_norm_ax.get_yticklabels():\n",
    "        if ticklabel.get_text().startswith(tuple(features)):\n",
    "            ticklabel.set_fontweight(\"bold\")\n",
    "\n",
    "    #####\n",
    "\n",
    "    # prepare single coefficient df\n",
    "    plot_coef_df = coef_df.loc[model_embeddings].reset_index()\n",
    "    # name model embedding coefficients according to model\n",
    "    model_coefs = plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\")]\n",
    "    plot_coef_df.loc[plot_coef_df.feature.str.startswith(\"model_embedding\"), \"feature\"] = \\\n",
    "        model_coefs.model.str.cat(model_coefs.feature, sep=\"_\")\n",
    "\n",
    "    # filter to electrodes of interest\n",
    "    plot_coef_df = plot_coef_df[plot_coef_df.output_dim.isin(electrodes)]\n",
    "    # filter to features of interest\n",
    "    plot_coef_df_features = plot_coef_df[plot_coef_df.feature.str.startswith(tuple(features))]\n",
    "    plot_coef_df_features = plot_coef_df_features[[\"fold\", \"feature\", \"output_dim\", \"time\", \"coef\"]]\n",
    "    plot_coef_df_features[\"type\"] = \"basic_feature\"\n",
    "    # add computed feature norms for embeddings\n",
    "    plot_coef_df_embeddings = plot_coef_df[plot_coef_df.feature.str.contains(\"model_embedding\")]\n",
    "    plot_coef_df_embeddings = plot_coef_df_embeddings.groupby([\"fold\", \"model\", \"output_dim\", \"time\"]) \\\n",
    "        .coef.apply(lambda xs: xs.abs().sum()).reset_index() \\\n",
    "        .rename(columns={\"model\": \"feature\"}).assign(type=\"model_embedding\")\n",
    "    \n",
    "    #####\n",
    "    # coef_line_ax = figure.add_subplot(gs[1, :])\n",
    "    # sns.lineplot(data=plot_coef_subset_df, x=\"time\", y=\"coef\", hue=\"feature\", style=\"type\", ax=coef_line_ax)\n",
    "\n",
    "    #####\n",
    "\n",
    "    feature_coef_heatmap_ax = figure.add_subplot(gs[1, :2])\n",
    "    plot_coef_heatmap_df = plot_coef_df_features.pivot_table(\n",
    "        index=\"feature\", columns=\"time\", values=\"coef\", aggfunc=\"mean\")\n",
    "    plot_coef_heatmap_df = plot_coef_heatmap_df.loc[sorted(plot_coef_df_features.feature.unique())]\n",
    "    sns.heatmap(plot_coef_heatmap_df, ax=feature_coef_heatmap_ax, cmap=\"RdBu\", center=0, yticklabels=True)\n",
    "\n",
    "    model_coef_heatmap_ax = figure.add_subplot(gs[2, :2])\n",
    "    plot_coef_heatmap_df = plot_coef_df_embeddings.pivot_table(\n",
    "        index=\"feature\", columns=\"time\", values=\"coef\", aggfunc=\"mean\")\n",
    "    # # order by decreasing t-value\n",
    "    # plot_coef_heatmap_df = plot_coef_heatmap_df.loc[[model for model in tval_df_order if model in plot_coef_heatmap_df.index]]\n",
    "    # order by name\n",
    "    plot_coef_heatmap_df = plot_coef_heatmap_df.loc[sorted(plot_coef_df_embeddings.feature.unique())]\n",
    "    sns.heatmap(plot_coef_heatmap_df, ax=model_coef_heatmap_ax, cmap=\"RdBu\", center=0, yticklabels=True)\n",
    "\n",
    "    #####\n",
    "\n",
    "    trial_epochs_ax = figure.add_subplot(gs[0, 2])\n",
    "    trial_epochs_ax.set_title(\"Trial ERP\")\n",
    "    trial_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_trial_epochs = trial_epochs[(trial_epochs.electrode_idx == electrode)]\n",
    "    sns.lineplot(data=plot_trial_epochs, x=\"epoch_time\", y=\"value\", ax=trial_epochs_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 **(trial_epoch_kwargs or {}))\n",
    "\n",
    "    word_epochs_ax = figure.add_subplot(gs[1, 2])\n",
    "    word_epochs_ax.set_title(\"Word ERP\")\n",
    "    word_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_epochs = word_epochs[word_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_epochs, x=\"epoch_time\", y=\"value\", ax=word_epochs_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 **(word_epoch_kwargs or {}))\n",
    "    \n",
    "    word_epochs2_ax = figure.add_subplot(gs[2, 2])\n",
    "    word_epochs2_ax.set_title(\"Word ERP\")\n",
    "    word_epochs2_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_epochs2 = word_epochs[word_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_epochs2, x=\"epoch_time\", y=\"value\", ax=word_epochs2_ax,\n",
    "                 errorbar=errorbar,\n",
    "                 **(word_epoch2_kwargs or {}))\n",
    "\n",
    "    syllable_epochs_ax = figure.add_subplot(gs[0, 3])\n",
    "    syllable_epochs_ax.set_title(\"Syllable ERP\")\n",
    "    syllable_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_syllable_epochs = syllable_epochs[syllable_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_syllable_epochs, x=\"epoch_time\", y=\"value\", ax=syllable_epochs_ax,\n",
    "                 errorbar=errorbar)\n",
    "\n",
    "    word_offset_epochs_ax = figure.add_subplot(gs[1, 3])\n",
    "    word_offset_epochs_ax.set_title(\"Word offset ERP\")\n",
    "    word_offset_epochs_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_offset_epochs = word_offset_epochs[word_offset_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_offset_epochs, x=\"epoch_time\", y=\"value\", ax=word_offset_epochs_ax,\n",
    "                 errorbar=errorbar, **(word_epoch_kwargs or {}))\n",
    "\n",
    "    word_offset_epoch2_ax = figure.add_subplot(gs[2, 3])\n",
    "    word_offset_epoch2_ax.set_title(\"Word offset ERP\")\n",
    "    word_offset_epoch2_ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    plot_word_offset_epoch2 = word_offset_epochs[word_offset_epochs.electrode_idx == electrode]\n",
    "    sns.lineplot(data=plot_word_offset_epoch2, x=\"epoch_time\", y=\"value\", ax=word_offset_epoch2_ax,\n",
    "                 errorbar=errorbar, **(word_epoch2_kwargs or {}))\n",
    "\n",
    "    plt.suptitle(f\"Electrode {electrode} study\")\n",
    "    \n",
    "    return plot_coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrodes showing greater response to word_broad than to biphone and phoneme features\n",
    "panel_electrodes = {\n",
    "    \"word_dominant\": word_contrasts[(word_contrasts.word_contrast <= -1) & (word_contrasts.log_pval <= -2)].index.tolist(),\n",
    "    \"phone_dominant\": word_contrasts[(word_contrasts.word_contrast >= 1) & (word_contrasts.log_pval <= -2)].index.tolist(),\n",
    "    \"balanced\": word_contrasts[(word_contrasts.word_contrast.between(-0.5, 0.5)) & (word_contrasts.log_pval <= -2)].index.tolist(),\n",
    "}\n",
    "\n",
    "# # electrodes showing balanced response between word_broad and biphone/phoneme features\n",
    "# panel_electrodes += [200, 204, 221, 173, 199, 123, 257, 314, 172, 234, 331, 211]\n",
    "\n",
    "# electrodes tuned to matched features and different models\n",
    "# panel_electrodes = [231, 205, 373, 173, 214, 123]\n",
    "# electrodes tuned to F0 but not to models\n",
    "# panel_electrodes += [212, 33, 219, 193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, electrode in tqdm([(group, electrode) for group, electrodes in panel_electrodes.items() for electrode in electrodes]):\n",
    "    with plt.rc_context(rc={\"font.size\": 24}):\n",
    "        render_electrode_panel(\n",
    "            electrode,\n",
    "            model_embeddings=study_models,\n",
    "            word_epoch_kwargs=dict(hue=\"monosyllabic\"),\n",
    "            word_epoch2_kwargs=dict(hue=\"word_frequency_quantile\"))\n",
    "        f = plt.gcf()\n",
    "        f.savefig(f\"{output_dir}/electrode_panel-{subject}-{group}-{electrode}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean_scores = baseline_scores.groupby(\"output_dim\").score.mean()\n",
    "ax = sns.swarmplot(baseline_mean_scores, color=\"gray\")\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "all_panel_electrodes = list(itertools.chain.from_iterable(panel_electrodes.values()))\n",
    "for elec, score in baseline_mean_scores.loc[all_panel_electrodes].items():\n",
    "    ax.text(0.2 + np.random.normal(0, 0.1), score, elec, ha=\"center\", va=\"bottom\",\n",
    "            transform=transforms.blended_transform_factory(ax.transAxes, ax.transData))\n",
    "    ax.axhline(score, color=\"blue\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "ax.set_title(\"Electrode baseline performance\")\n",
    "ax.set_ylabel(\"Baseline $r^2$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
