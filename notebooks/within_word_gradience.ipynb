{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "state_space_specs_path = \"outputs/state_space_specs/librispeech-train-clean-100/w2v2_8/state_space_specs.pkl\"\n",
    "embeddings_path = \"outputs/model_embeddings/librispeech-train-clean-100/w2v2_8/rnn_8-weightdecay0.01/phoneme_10frames/librispeech-train-clean-100.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(\"outputs/preprocessed_data/librispeech-train-clean-100\") \\\n",
    "    .remove_columns([\"audio\", \"file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pronunciations = defaultdict(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(item):\n",
    "    for word, phones in zip(item[\"word_detail\"][\"utterance\"], item[\"word_phonemic_detail\"]):\n",
    "        phones = tuple(phone[\"phone\"] for phone in phones)\n",
    "        observed_pronunciations[word][phones] += 1\n",
    "ds.map(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(observed_pronunciations, key=lambda x: len(observed_pronunciations[x].values()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronunciation_stats = []\n",
    "for word, pronunciations in observed_pronunciations.items():\n",
    "    if not word: continue\n",
    "\n",
    "    total = sum(pronunciations.values())\n",
    "    proportions = np.array([count / total for count in pronunciations.values()])\n",
    "    entropy = -np.sum(proportions * np.log(proportions))\n",
    "    pronunciation_stats.append((word, total, entropy))\n",
    "\n",
    "pronunciation_stats = pd.DataFrame(pronunciation_stats, columns=[\"word\", \"total\", \"entropy\"])\n",
    "pronunciation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pronunciations_pca(study_word, ax=None):\n",
    "    study_label_idx = state_space_spec.labels.index(study_word)\n",
    "    study_instances = defaultdict(list)\n",
    "    study_classes = {}\n",
    "    study_X = []\n",
    "    study_Y = []\n",
    "\n",
    "    for instance_idx, rows in state_space_spec.cuts.loc[study_word].xs(\"phoneme\", level=\"level\").groupby(\"instance_idx\"):\n",
    "        phons = tuple(rows.description)\n",
    "        if phons in study_classes:\n",
    "            cls = study_classes[phons]\n",
    "        else:\n",
    "            cls = len(study_classes)\n",
    "            study_classes[phons] = cls\n",
    "\n",
    "        study_instances[cls].append(instance_idx)\n",
    "        frame_start, frame_end = state_space_spec.target_frame_spans[study_label_idx][instance_idx]\n",
    "        study_X.append(model_representations[frame_start:frame_end])\n",
    "        study_Y.append(cls)\n",
    "\n",
    "    study_X = np.array([np.mean(Xi, axis=0) for Xi in study_X])\n",
    "    study_X = (study_X - study_X.mean(axis=0)) / study_X.std(axis=0)\n",
    "\n",
    "    study_Y = np.array(study_Y)\n",
    "\n",
    "    pca = PCA(2).fit(study_X)\n",
    "    study_X_pca = pca.transform(study_X)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_title(study_word)\n",
    "    for phons, idx in study_classes.items():\n",
    "        ax.scatter(study_X_pca[study_Y == idx, 0], study_X_pca[study_Y == idx, 1], label=\" \".join(phons), alpha=0.3)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n = 20\n",
    "n_cols = int(np.floor(np.sqrt(plot_n)))\n",
    "n_rows = int(np.ceil(plot_n / n_cols))\n",
    "f, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "for ax, (_, row) in zip(tqdm(axs.flat), pronunciation_stats.sort_values(\"entropy\", ascending=False).head(plot_n).iterrows()):\n",
    "    plot_pronunciations_pca(row[\"word\"], ax=ax)\n",
    "\n",
    "f.suptitle(\"Words with maximal entropy over pronunciations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n = 20\n",
    "n_cols = int(np.floor(np.sqrt(plot_n)))\n",
    "n_rows = int(np.ceil(plot_n / n_cols))\n",
    "f, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "for ax, (_, row) in zip(tqdm(axs.flat), pronunciation_stats[pronunciation_stats.entropy != 0].sort_values(\"total\", ascending=False).head(plot_n).iterrows()):\n",
    "    plot_pronunciations_pca(row[\"word\"], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_syllable_counts = state_space_spec.cuts.xs(\"syllable\", level=\"level\").groupby([\"label\", \"instance_idx\"]).size()\n",
    "has_syllable_variation = instance_syllable_counts.groupby(\"label\").nunique() > 1\n",
    "has_syllable_variation = has_syllable_variation[has_syllable_variation].index\n",
    "has_syllable_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_count_entropy = instance_syllable_counts.loc[has_syllable_variation].groupby(\"label\").apply(lambda x: -np.sum(x / x.sum() * np.log(x / x.sum())))\n",
    "syllable_count_entropy.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_syllables_pca(study_word, ax=None):\n",
    "    study_label_idx = state_space_spec.labels.index(study_word)\n",
    "    study_instances = defaultdict(list)\n",
    "    study_classes = {}\n",
    "    study_X = []\n",
    "    study_Y = []\n",
    "\n",
    "    for instance_idx, rows in state_space_spec.cuts.loc[study_word].xs(\"syllable\", level=\"level\").groupby(\"instance_idx\"):\n",
    "        phons = rows.description.str.join(\" \").str.cat(sep=\"-\")\n",
    "        if phons in study_classes:\n",
    "            cls = study_classes[phons]\n",
    "        else:\n",
    "            cls = len(study_classes)\n",
    "            study_classes[phons] = cls\n",
    "\n",
    "        study_instances[cls].append(instance_idx)\n",
    "        frame_start, frame_end = state_space_spec.target_frame_spans[study_label_idx][instance_idx]\n",
    "        study_X.append(model_representations[frame_start:frame_end])\n",
    "        study_Y.append(cls)\n",
    "\n",
    "    study_X = np.array([np.mean(Xi, axis=0) for Xi in study_X])\n",
    "    study_X = (study_X - study_X.mean(axis=0)) / study_X.std(axis=0)\n",
    "\n",
    "    study_Y = np.array(study_Y)\n",
    "\n",
    "    pca = PCA(2).fit(study_X)\n",
    "    study_X_pca = pca.transform(study_X)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_title(study_word)\n",
    "    for phons, idx in study_classes.items():\n",
    "        ax.scatter(study_X_pca[study_Y == idx, 0], study_X_pca[study_Y == idx, 1], label=phons, alpha=0.3)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n = 20\n",
    "n_cols = int(np.floor(np.sqrt(plot_n)))\n",
    "n_rows = int(np.ceil(plot_n / n_cols))\n",
    "f, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "for ax, word in zip(tqdm(axs.flat), syllable_count_entropy.sort_values(ascending=False).head(plot_n).index):\n",
    "    plot_syllables_pca(word, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation in onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_phoneme_words = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").groupby([\"label\", \"instance_idx\"]).size() > 1\n",
    "multi_phoneme_words = multi_phoneme_words[multi_phoneme_words].index\n",
    "multi_onset_counts = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").loc[multi_phoneme_words].groupby([\"label\", \"instance_idx\"]).head(1).groupby(\"label\").description.value_counts()\n",
    "multi_onset_counts = multi_onset_counts.groupby(\"label\").filter(lambda xs: len(xs) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_onset_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n = 20\n",
    "n_cols = int(np.floor(np.sqrt(plot_n)))\n",
    "n_rows = int(np.ceil(plot_n / n_cols))\n",
    "f, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "for ax, (word, _) in zip(tqdm(axs.flat), multi_onset_counts.groupby(\"label\").filter(lambda xs: xs.sum() > 100).groupby(\"label\").apply(lambda xs: -np.sum(xs / xs.sum() * np.log(xs / xs.sum()))).sort_values(ascending=False).head(20).items()):\n",
    "    plot_pronunciations_pca(word, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
