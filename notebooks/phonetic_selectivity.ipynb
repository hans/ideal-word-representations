{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare state space trajectories for a lexical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from src.models import get_best_checkpoint\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.models.integrator import ContrastiveEmbeddingModel, compute_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# model_dir = \"out/ce_model_phoneme_within_word_prefix_6_32\"\n",
    "model_dir = \"out/ce_model_random_32\"\n",
    "\n",
    "# use a word-level equivalence dataset regardless of model, so that we can look up cohort facts\n",
    "equiv_dataset_path = \"data/timit_equiv_phoneme_6_1.pkl\"\n",
    "timit_corpus_path = \"data/timit_phonemes\"\n",
    "\n",
    "phoneme_response_window = (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveEmbeddingModel.from_pretrained(get_best_checkpoint(model_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(equiv_dataset_path, \"rb\") as f:\n",
    "    equiv_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_corpus = datasets.load_from_disk(timit_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemes = set([phone[\"phone\"] for words in timit_corpus[\"train\"][\"word_phonemic_detail\"]\n",
    " for word in words\n",
    " for phone in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_features = {\n",
    "    \"AA\": \"low back unrounded\",\n",
    "    \"AE\": \"low front unrounded\",\n",
    "    \"AH\": \"low central unrounded\",\n",
    "    \"AO\": \"low back rounded\",\n",
    "    \"AW\": \"mid back rounded\",\n",
    "    \"AY\": \"high front unrounded\",\n",
    "    \"B\": \"voiced bilabial plosive\",\n",
    "    \"CH\": \"voiceless palato-alveolar affricate\",\n",
    "    \"D\": \"voiced alveolar plosive\",\n",
    "    \"DH\": \"voiced dental fricative\",\n",
    "    \"EH\": \"mid front unrounded\",\n",
    "    \"ER\": \"mid central unrounded\",\n",
    "    \"EY\": \"mid front rounded\",\n",
    "    \"F\": \"voiceless labiodental fricative\",\n",
    "    \"G\": \"voiced velar plosive\",\n",
    "    \"HH\": \"voiceless glottal fricative\",\n",
    "    \"IH\": \"high front unrounded\",\n",
    "    \"IY\": \"high front rounded\",\n",
    "    \"JH\": \"voiced palato-alveolar affricate\",\n",
    "    \"K\": \"voiceless velar plosive\",\n",
    "    \"L\": \"voiced alveolar lateral approximant\",\n",
    "    \"M\": \"voiced bilabial nasal\",\n",
    "    \"N\": \"voiced alveolar nasal\",\n",
    "    \"NG\": \"voiced velar nasal\",\n",
    "    \"OW\": \"mid back rounded\",\n",
    "    \"OY\": \"mid back rounded\",\n",
    "    \"P\": \"voiceless bilabial plosive\",\n",
    "    \"R\": \"voiced alveolar approximant\",\n",
    "    \"S\": \"voiceless alveolar fricative\",\n",
    "    \"SH\": \"voiceless palato-alveolar fricative\",\n",
    "    \"T\": \"voiceless alveolar plosive\",\n",
    "    \"TH\": \"voiceless dental fricative\",\n",
    "    \"UH\": \"high back rounded\",\n",
    "    \"UW\": \"high back rounded\",\n",
    "    \"V\": \"voiced labiodental fricative\",\n",
    "    \"W\": \"voiced labio-velar approximant\",\n",
    "    \"Y\": \"voiced palatal approximant\",\n",
    "    \"Z\": \"voiced alveolar fricative\",\n",
    "    \"ZH\": \"voiced palato-alveolar fricative\",\n",
    "}\n",
    "cmudict_features = {k: v.split() for k, v in cmudict_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(sorted(set(itertools.chain(*cmudict_features.values()))))\n",
    "feature2idx = {f: i for i, f in enumerate(all_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_feature_idxs = {k: [feature2idx[f] for f in v] for k, v in cmudict_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_phonemes = {f: [k for k, v in cmudict_features.items() if f in v] for f in all_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(type(label) == str for label in equiv_dataset.class_labels), \"Assumes dataset with phoneme labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_representations = load_or_compute_embeddings(model, equiv_dataset, model_dir, equiv_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_frames_by_item = equiv_dataset.hidden_state_dataset.frames_by_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rep = np.mean(model_representations, axis=0, keepdims=True)\n",
    "std_rep = np.std(model_representations, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_responses = defaultdict(list)\n",
    "phoneme_agg_fn = np.mean\n",
    "zscore = True\n",
    "\n",
    "def get_phoneme_responses(item, idx):\n",
    "    start_frame, end_frame = equiv_frames_by_item[idx]\n",
    "    compression_ratio = (end_frame - start_frame) / len(item[\"input_values\"])\n",
    "\n",
    "    window_left, window_right = phoneme_response_window\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        for phone in word:\n",
    "            phone_start = start_frame + int(phone[\"start\"] * compression_ratio)\n",
    "            phone_end = start_frame + int(phone[\"stop\"] * compression_ratio)\n",
    "\n",
    "            response = model_representations[phone_end + window_left:phone_end + window_right]\n",
    "\n",
    "            if zscore:\n",
    "                response = (response - mean_rep) / std_rep\n",
    "\n",
    "            phoneme_responses[phone[\"phone\"]].append(phoneme_agg_fn(response, axis=0))\n",
    "\n",
    "timit_corpus.map(get_phoneme_responses, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_responses = defaultdict(list)\n",
    "for feature, phonemes in feature_to_phonemes.items():\n",
    "    for phoneme in phonemes:\n",
    "        feature_responses[feature].extend(phoneme_responses[phoneme])\n",
    "\n",
    "feature_responses = {k: np.stack(v) for k, v in feature_responses.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voiced = pca.transform(feature_responses[\"voiced\"])\n",
    "plot_voiceless = pca.transform(feature_responses[\"voiceless\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(plot_voiced[:, 0], plot_voiced[:, 1], label=\"voiced\", alpha=0.3)\n",
    "plt.scatter(plot_voiceless[:, 0], plot_voiceless[:, 1], label=\"voiceless\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features * num_dimensions\n",
    "feature_responses_mat = np.array([feature_responses_i.mean(axis=0)\n",
    "                                  for feature_responses_i in feature_responses.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_responses_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature--hidden unit pair, calculate feature selectivity index:\n",
    "# FSI_ij receives 1 point for every feature to which hidden unit i responds\n",
    "# more weakly than it did to feature j by a threshold (0.15).\n",
    "feature_selectivity_threshold = 0.4\n",
    "feature_selectivity = np.zeros_like(feature_responses_mat)\n",
    "\n",
    "for hidden_idx in range(feature_selectivity.shape[1]):\n",
    "    for feature_idx in range(feature_selectivity.shape[0]):\n",
    "        feature_response = feature_responses_mat[feature_idx, hidden_idx]\n",
    "\n",
    "        other_feature_responses = np.concatenate([\n",
    "            feature_responses_mat[:feature_idx, hidden_idx],\n",
    "            feature_responses_mat[feature_idx + 1:, hidden_idx],\n",
    "        ])\n",
    "        feature_selectivity[feature_idx, hidden_idx] = (np.abs(feature_response - other_feature_responses) > feature_selectivity_threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(feature_selectivity, yticklabels=feature_to_phonemes.keys(), xticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
