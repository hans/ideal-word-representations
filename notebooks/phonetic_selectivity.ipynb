{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare state space trajectories for a lexical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from src.models import get_best_checkpoint\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec\n",
    "from src.models.integrator import ContrastiveEmbeddingModel, load_or_compute_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# model_dir = \"out/ce_model_phoneme_within_word_prefix_6_32\"\n",
    "model_dir = \"out/ce_model_random_32\"\n",
    "\n",
    "# use a word-level equivalence dataset regardless of model, so that we can look up cohort facts\n",
    "equiv_dataset_path = \"data/timit_equiv_phoneme_6_1.pkl\"\n",
    "timit_corpus_path = \"data/timit_phonemes\"\n",
    "\n",
    "phoneme_response_window = (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveEmbeddingModel.from_pretrained(get_best_checkpoint(model_dir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(equiv_dataset_path, \"rb\") as f:\n",
    "    equiv_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_corpus = datasets.load_from_disk(timit_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemes = set([phone[\"phone\"] for words in timit_corpus[\"train\"][\"word_phonemic_detail\"]\n",
    " for word in words\n",
    " for phone in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_features = {\n",
    "    \"AA\": \"low back syllabic sonorant\",\n",
    "    \"AE\": \"low front syllabic sonorant\",\n",
    "    \"AH\": \"syllabic sonorant\",\n",
    "    \"AO\": \"low back syllabic sonorant\",\n",
    "    \"AW\": \"syllabic sonorant\",\n",
    "    \"AY\": \"syllabic sonorant\",\n",
    "    \"B\": \"anterior bilabial obstruent consonantal plosive voiced\",\n",
    "    \"CH\": \"velar unvoiced\",\n",
    "    \"D\": \"anterior coronal alveolar obstruent consonantal plosive voiced\",\n",
    "    \"DH\": \"anterior alveolar obstruent consonantal fricative voiced\",\n",
    "    \"EH\": \"low front syllabic sonorant\",\n",
    "    \"ER\": \"sonorant\",\n",
    "    \"EY\": \"syllabic sonorant\",\n",
    "    \"F\": \"anterior labiodental obstruent consonantal fricative unvoiced\",\n",
    "    \"G\": \"dorsal velar obstruent consonantal plosive voiced\",\n",
    "    \"HH\": \"consonantal fricative unvoiced\",\n",
    "    \"IH\": \"high front syllabic sonorant\",\n",
    "    \"IY\": \"high front syllabic sonorant\",\n",
    "    \"JH\": \"velar voiced\",\n",
    "    \"K\": \"dorsal velar obstruent consonantal plosive unvoiced\",\n",
    "    \"L\": \"anterior coronal alveolar approximant consonantal sonorant\",\n",
    "    \"M\": \"anterior bilabial consonantal sonorant nasal\",\n",
    "    \"N\": \"anterior coronal alveolar consonantal sonorant nasal\",\n",
    "    \"NG\": \"dorsal consonantal sonorant nasal\",\n",
    "    \"OW\": \"syllabic sonorant\",\n",
    "    \"OY\": \"syllabic sonorant\",\n",
    "    \"P\": \"anterior bilabial obstruent consonantal plosive unvoiced\",\n",
    "    \"R\": \"anterior coronal alveolar approximant consonantal sonorant\",\n",
    "    \"S\": \"anterior coronal alveolar obstruent consonantal sibilant fricative unvoiced\",\n",
    "    \"SH\": \"coronal velar obstruent consonantal sibilant fricative unvoiced\",\n",
    "    \"T\": \"anterior coronal alveolar obstruent consonantal plosive unvoiced\",\n",
    "    \"TH\": \"anterior alveolar obstruent consonantal fricative unvoiced\",\n",
    "    \"UH\": \"high back syllabic sonorant\",\n",
    "    \"UW\": \"high back syllabic sonorant\",\n",
    "    \"V\": \"anterior labiodental obstruent consonantal fricative voiced\",\n",
    "    \"W\": \"approximant sonorant\",  # y no place?\n",
    "    \"Y\": \"velar approximant sonorant\",\n",
    "    \"Z\": \"anterior coronal alveolar obstruent consonantal sibilant fricative voiced\",\n",
    "    \"ZH\": \"coronal alveolar obstruent consonantal sibilant fricative voiced\",\n",
    "}\n",
    "cmudict_features = {k: v.split() for k, v in cmudict_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(sorted(set(itertools.chain(*cmudict_features.values()))))\n",
    "feature2idx = {f: i for i, f in enumerate(all_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_feature_idxs = {k: [feature2idx[f] for f in v] for k, v in cmudict_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_phonemes = {f: [k for k, v in cmudict_features.items() if f in v] for f in all_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(type(label) == str for label in equiv_dataset.class_labels), \"Assumes dataset with phoneme labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_representations = load_or_compute_embeddings(model, equiv_dataset, model_dir, equiv_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme2idx = {p: i for i, p in enumerate(cmudict_features.keys())}\n",
    "feature_matrix = np.zeros((len(feature2idx), len(phoneme2idx)), dtype=int)\n",
    "for feature, phonemes in feature_to_phonemes.items():\n",
    "    for phoneme in phonemes:\n",
    "        feature_matrix[feature2idx[feature], phoneme2idx[phoneme]] = 1\n",
    "feature_df = pd.DataFrame(feature_matrix, index=all_features, columns=list(cmudict_features.keys()))\n",
    "sns.heatmap(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_frames_by_item = equiv_dataset.hidden_state_dataset.frames_by_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rep = np.mean(model_representations, axis=0, keepdims=True)\n",
    "std_rep = np.std(model_representations, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_responses = defaultdict(list)\n",
    "phoneme_agg_fn = np.mean\n",
    "zscore = True\n",
    "\n",
    "def get_phoneme_responses(item, idx):\n",
    "    start_frame, end_frame = equiv_frames_by_item[idx]\n",
    "    compression_ratio = (end_frame - start_frame) / len(item[\"input_values\"])\n",
    "\n",
    "    window_left, window_right = phoneme_response_window\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        for phone in word:\n",
    "            phone_start = start_frame + int(phone[\"start\"] * compression_ratio)\n",
    "            phone_end = start_frame + int(phone[\"stop\"] * compression_ratio)\n",
    "\n",
    "            response = model_representations[phone_end + window_left:phone_end + window_right]\n",
    "\n",
    "            if zscore:\n",
    "                response = (response - mean_rep) / std_rep\n",
    "\n",
    "            phoneme_responses[phone[\"phone\"]].append(phoneme_agg_fn(response, axis=0))\n",
    "\n",
    "timit_corpus.map(get_phoneme_responses, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_responses = defaultdict(list)\n",
    "for feature, phonemes in feature_to_phonemes.items():\n",
    "    for phoneme in phonemes:\n",
    "        feature_responses[feature].extend(phoneme_responses[phoneme])\n",
    "\n",
    "feature_responses = {k: np.stack(v) for k, v in feature_responses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(output_dir) / \"phoneme_responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(phoneme_responses, f)\n",
    "with open(Path(output_dir) / \"feature_responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features * num_dimensions\n",
    "feature_responses_mat = np.array([feature_responses_i.mean(axis=0)\n",
    "                                  for feature_responses_i in feature_responses.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature--hidden unit pair, calculate feature selectivity index:\n",
    "# FSI_ij receives 1 point for every feature to which hidden unit i responds\n",
    "# more weakly than it did to feature j by a threshold (0.15).\n",
    "feature_selectivity_threshold = 0.4\n",
    "feature_selectivity = np.zeros_like(feature_responses_mat)\n",
    "\n",
    "for hidden_idx in range(feature_selectivity.shape[1]):\n",
    "    for feature_idx in range(feature_selectivity.shape[0]):\n",
    "        feature_response = feature_responses_mat[feature_idx, hidden_idx]\n",
    "\n",
    "        other_feature_responses = np.concatenate([\n",
    "            feature_responses_mat[:feature_idx, hidden_idx],\n",
    "            feature_responses_mat[feature_idx + 1:, hidden_idx],\n",
    "        ])\n",
    "        feature_selectivity[feature_idx, hidden_idx] = (np.abs(feature_response - other_feature_responses) > feature_selectivity_threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(feature_selectivity, yticklabels=feature_to_phonemes.keys(), xticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
