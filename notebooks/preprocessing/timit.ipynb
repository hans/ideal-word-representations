{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook converts the raw TIMIT corpus (as represented in Huggingface `timit_asr`) into a format friendly for our modeling and analysis.\n",
    "\n",
    "Our specific modifications to the annotation include\n",
    "- mapping from TIMIT phonetic annotation into a CMUDICT phonemic annotation (available in item key `phonemic_detail`; `word_phonemic_detail` grouped by words)\n",
    "- syllable annotation (available in item key `word_syllable_detail`)\n",
    "\n",
    "Processes in this notebook\n",
    "- run the conversion\n",
    "- cross-check resulting lexical mappings with cmudict representation\n",
    "- make sure we don't have overlapping phonemes\n",
    "- visually check some waveforms and make sure they make sense\n",
    "- check syllable annotations, pay attention to how syllabic consonants represented especially. there are a lot of these in the timit annotations and we are removing the evidence before syllabification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_dir = \"/userdata/jgauthier/projects/ideal-word-representations\"\n",
    "\n",
    "# input Huggingface-format `timit_asr` dataset, with train/test split\n",
    "dataset_path = \"data/timit_raw\"\n",
    "\n",
    "out_path = \"data/timit_syllables\"\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "drop_timit_phones = [\"h#\", \"pau\", \"epi\"]\n",
    "drop_cmudict_phonemes = [\"[SIL]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd {base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import timit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = datasets.load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for introspecting on raw/resulting annotations, if necessary\n",
    "\n",
    "log_item = 25\n",
    "\n",
    "# list(zip(raw_corpus[\"train\"][log_item][\"phonetic_detail\"][\"utterance\"],\n",
    "#          raw_corpus[\"train\"][log_item][\"phonetic_detail\"][\"start\"],\n",
    "#             raw_corpus[\"train\"][log_item][\"phonetic_detail\"][\"stop\"]))\n",
    "\n",
    "# list(zip(raw_corpus[\"train\"][log_item][\"word_detail\"][\"utterance\"],\n",
    "#             raw_corpus[\"train\"][log_item][\"word_detail\"][\"start\"],\n",
    "#                 raw_corpus[\"train\"][log_item][\"word_detail\"][\"stop\"]))\n",
    "\n",
    "# list(zip(corpus[log_item][\"phonemic_detail\"][\"utterance\"],\n",
    "#          corpus[log_item][\"phonemic_detail\"][\"start\"],\n",
    "#             corpus[log_item][\"phonemic_detail\"][\"stop\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark original indices in each item, then concatenate into a single dataset\n",
    "def mark_item_indices(item, idx, split=None):\n",
    "    item[\"original_idx\"] = idx\n",
    "    item[\"original_split\"] = split\n",
    "raw_corpus[\"train\"] = raw_corpus[\"train\"].map(mark_item_indices, with_indices=True, fn_kwargs=dict(split=\"train\"))\n",
    "raw_corpus[\"test\"] = raw_corpus[\"test\"].map(mark_item_indices, with_indices=True, fn_kwargs=dict(split=\"test\"))\n",
    "corpus = datasets.concatenate_datasets([raw_corpus[\"train\"], raw_corpus[\"test\"]])\n",
    "\n",
    "def mark_item_indices_2(item, idx):\n",
    "    item[\"idx\"] = idx\n",
    "corpus = corpus.map(mark_item_indices_2, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = timit.prepare_corpus(corpus, processor,\n",
    "                              drop_phones=drop_timit_phones + drop_cmudict_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item(item_idx, ax, plot_units=\"phoneme\", viz_rate=1000):\n",
    "    item = corpus[item_idx]\n",
    "\n",
    "    times = np.linspace(0, len(item[\"input_values\"]) / 16000, int(len(item[\"input_values\"]) / 16000 * viz_rate))\n",
    "    # normalize to [-1, 1]\n",
    "    values = np.array(item[\"input_values\"])\n",
    "    values = (values - values.min()) / (values.max() - values.min()) * 2 - 1\n",
    "    # resample to viz frame rate\n",
    "    values = np.interp(times, np.arange(len(values)) / 16000, values)\n",
    "    ax.plot(times, values, alpha=0.2)\n",
    "\n",
    "    # plot word and phoneme boundaries\n",
    "    for i, word in enumerate(item[\"word_phonemic_detail\"]):\n",
    "        word_str = item[\"word_detail\"][\"utterance\"][i]\n",
    "\n",
    "        word_start, word_stop = word[0][\"start\"] / 16000, word[-1][\"stop\"] / 16000\n",
    "        ax.axvline(word_start, color=\"black\", linestyle=\"--\")\n",
    "        ax.text(word_start, 0.8, word_str, rotation=90, verticalalignment=\"bottom\", alpha=0.7)\n",
    "\n",
    "        if plot_units == \"phoneme\":\n",
    "            for j, phoneme in enumerate(word):\n",
    "                phoneme_str = phoneme[\"phone\"]\n",
    "                phoneme_start, phoneme_stop = phoneme[\"start\"] / 16000, phoneme[\"stop\"] / 16000\n",
    "\n",
    "                if j > 0:\n",
    "                    color = \"black\" if phoneme[\"idx_in_syllable\"] == 0 else \"gray\"\n",
    "                    ax.axvline(phoneme_start, color=color, linestyle=\":\", alpha=0.5)\n",
    "                ax.text(phoneme_start + 0.01, -6, phoneme_str, rotation=90, verticalalignment=\"bottom\",\n",
    "                        fontdict={\"size\": 15})\n",
    "        elif plot_units == \"syllable\":\n",
    "            for j, syllable in enumerate(item[\"word_syllable_detail\"][i]):\n",
    "                syllable_str = \" \".join(syllable[\"phones\"])\n",
    "                syllable_start, syllable_stop = syllable[\"start\"] / 16000, syllable[\"stop\"] / 16000\n",
    "\n",
    "                if j > 0:\n",
    "                    ax.axvline(syllable_start, color=\"black\", linestyle=\":\", alpha=0.5)\n",
    "                ax.text(syllable_start + 0.01, -6, syllable_str, rotation=90, verticalalignment=\"bottom\",\n",
    "                        fontdict={\"size\": 15})\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown plot_units: {plot_units}\")\n",
    "\n",
    "    # align at origin\n",
    "    ax.set_ylim((-8, 8))\n",
    "\n",
    "    ax.set_title(f\"{item['speaker_id']}_{item['id']}: {item['text']}\")\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(False)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 1, figsize=(25, 2 * 8))\n",
    "idx = np.random.choice(len(corpus))\n",
    "print(idx)\n",
    "plot_item(idx, axs[0], plot_units=\"phoneme\")\n",
    "plot_item(idx, axs[1], plot_units=\"syllable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check word-level correspondence with CMUdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from urllib.request import urlretrieve\n",
    "import re\n",
    "\n",
    "# Download and parse cmudict\n",
    "cmudict_entries = defaultdict(list)\n",
    "with NamedTemporaryFile() as f:\n",
    "    urlretrieve(\"https://github.com/cmusphinx/cmudict/raw/master/cmudict.dict\", f.name)\n",
    "\n",
    "    with open(f.name, \"r\") as f:\n",
    "        for line in f:\n",
    "            # remove comments\n",
    "            line = re.sub(r'(\\s)*#.*', '', line)\n",
    "\n",
    "            fields = line.strip().split(\" \")\n",
    "            word = fields[0]\n",
    "\n",
    "            # remove word idx number, indicating secondary pronunciation\n",
    "            word = re.sub(r\"\\(\\d\\)$\", \"\", word)\n",
    "\n",
    "            phones = tuple(fields[1:])\n",
    "            # remove stress markers\n",
    "            phones = tuple(re.sub(r\"\\d\", \"\", p) for p in phones)\n",
    "\n",
    "            cmudict_entries[word].append(phones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track attested pronunciations of each word in TIMIT\n",
    "corpus_cmudict_mapping = defaultdict(Counter)\n",
    "def process_item(item):\n",
    "    for word, word_phonemes in zip(item[\"word_detail\"][\"utterance\"], item[\"word_phonemic_detail\"]):\n",
    "        corpus_cmudict_mapping[word][tuple(p[\"phone\"] for p in word_phonemes)] += 1\n",
    "corpus.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words have multiple pronunciations?\n",
    "multiple_pronunciations = {k: v for k, v in corpus_cmudict_mapping.items() if len(v) > 1}\n",
    "print(f\"{len(multiple_pronunciations)} words ({len(multiple_pronunciations) / len(corpus_cmudict_mapping) * 100}%) have multiple pronunciations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words have CMUDICT pronunciations?\n",
    "has_cmudict = {k: v for k, v in corpus_cmudict_mapping.items() if k in cmudict_entries}\n",
    "print(f\"{len(has_cmudict)} words ({len(has_cmudict) / len(corpus_cmudict_mapping) * 100}%) have CMUDICT pronunciations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For how many words does the majority pronunciation align with the CMUDICT pronunciation?\n",
    "majority_aligned = {k: v for k, v in corpus_cmudict_mapping.items()\n",
    "                    if len(cmudict_entries[k]) > 0 and v.most_common(1)[0][0] == cmudict_entries[k][0]}\n",
    "majority_misaligned = {k: v for k, v in corpus_cmudict_mapping.items()\n",
    "                       if len(cmudict_entries[k]) > 0 and v.most_common(1)[0][0] != cmudict_entries[k][0]}\n",
    "print(f\"{len(majority_aligned)} words ({len(majority_aligned) / len(corpus_cmudict_mapping) * 100}%) have majority-aligned CMUDICT pronunciations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For misaligned majorities, compare with CMUDICT\n",
    "for word, counts in majority_misaligned.items():\n",
    "    print(f\"{word}: {' '.join(counts.most_common(1)[0][0])} (TIMIT) vs {' '.join(cmudict_entries[word][0])} (CMUDICT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_misaligned.get(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_misaligned.get(\"provoked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches = [idx for idx in trange(len(corpus)) if \"success\" in corpus[idx][\"word_detail\"][\"utterance\"]]\n",
    "# matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syllable_counts = Counter()\n",
    "word_syllable_counts = defaultdict(Counter)\n",
    "\n",
    "def process_item(item):\n",
    "    for i, (word, syllables) in enumerate(zip(item[\"word_detail\"][\"utterance\"], item[\"word_syllable_detail\"])):\n",
    "        syll_string = tuple(tuple(syllable[\"phones\"]) for syllable in syllables)\n",
    "        word_syllable_counts[word][syll_string] += 1\n",
    "        for syllable in syll_string:\n",
    "            all_syllable_counts[syllable] += 1\n",
    "corpus.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syllable_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_vowels = {\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\", \"IY\", \"OW\", \"OY\", \"UH\", \"UW\"}\n",
    "\n",
    "print(\"Syllabic consonant frequencies:\")\n",
    "syllabic_frequencies = Counter({k: v for k, v in all_syllable_counts.items() if len(k) == 1 and k[0] not in cmudict_vowels})\n",
    "pprint(syllabic_frequencies)\n",
    "\n",
    "print(\"Proportion of total syllable tokens: \", sum(syllabic_frequencies.values()) / sum(all_syllable_counts.values()) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_syllabification_words = Counter({k: v for k, v in word_syllable_counts.items() if len(v) > 1})\n",
    "print(f\"{len(multiple_syllabification_words)} words ({len(multiple_syllabification_words) / len(word_syllable_counts) * 100}%) have multiple syllabifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log top token frequency syllables\n",
    "sorted(multiple_syllabification_words.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllables without any content\n",
    "# This can emerge when, according to TIMIT annotation, a syllable is completely coarticulated\n",
    "# with its preceding syllable. We arbitrarily assign phoneme annotations to the preceding syllable,\n",
    "# leaving the latter syllable empty.\n",
    "empty_syllables = {word: counts[()] for word, counts in word_syllable_counts.items()\n",
    "                   if () in counts and counts[()] > 0}\n",
    "print(f\"{len(empty_syllables)} syllables ({sum(empty_syllables.values())} tokens, {sum(empty_syllables.values()) / sum(all_syllable_counts.values()) * 100}%) are empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.save_to_disk(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
