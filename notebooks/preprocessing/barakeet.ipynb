{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import itertools\n",
    "import logging\n",
    "import re\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "from src.datasets.barakeet import BarakeetDataset\n",
    "from src.utils import syllabifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(\"barakeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/barakeet\"\n",
    "out_path = \"outputs/preprocessed_data/barakeet\"\n",
    "left_pad = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tempdir:\n",
    "    ds = datasets.load_dataset(\"src/datasets/barakeet.py\", data_dir=data_dir, cache_dir=tempdir)\n",
    "ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make audio statistically comparable to librispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_ds = datasets.load_from_disk(\"outputs/preprocessed_data/librispeech-train-clean-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_item_stats(item):\n",
    "    arr = item[\"audio\"][\"array\"]\n",
    "\n",
    "    freqs, psd = welch(arr, fs=item[\"audio\"][\"sampling_rate\"], nperseg=1024)\n",
    "\n",
    "    signal_power = np.mean(arr ** 2)\n",
    "    noise_power = np.var(arr)\n",
    "    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))  # Avoid div by zero\n",
    "\n",
    "    return {\n",
    "        \"mean\": np.mean(arr),\n",
    "        \"median\": np.median(arr),\n",
    "        \"var\": np.var(arr),\n",
    "        \"min\": np.min(arr),\n",
    "        \"max\": np.max(arr),\n",
    "        \"snr\": snr,\n",
    "    }\n",
    "\n",
    "def compute_dataset_stats(ds):\n",
    "    # random sample 10% of dataset\n",
    "    ds = ds.select(np.random.choice(len(ds), size=min(300, len(ds)), replace=False))\n",
    "    # compute stats for each item\n",
    "    stats = ds.map(compute_item_stats, keep_in_memory=True)\n",
    "    # aggregate stats into a dataframe\n",
    "    stats_df = pd.DataFrame([{k: v for k, v in item.items() if k not in ds.features.keys()} for item in stats])\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_stats = compute_dataset_stats(librispeech_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_mean_stats = librispeech_stats.mean()\n",
    "librispeech_mean_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_target = max(abs(librispeech_mean_stats[\"min\"]), abs(librispeech_mean_stats[\"max\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(min(len(ds), len(librispeech_ds)))\n",
    "fixed = ds[i][\"audio\"][\"array\"].copy()\n",
    "# remove DC distortion\n",
    "fixed -= fixed.mean()\n",
    "# match value of silence in librispeech\n",
    "# fixed += librispeech_ds[0][\"audio\"][\"array\"][:10].mean()\n",
    "# pad\n",
    "fixed = np.pad(fixed, (int(16000 * left_pad), 0), mode=\"constant\", constant_values=0)\n",
    "fixed += librispeech_mean_stats[\"mean\"]\n",
    "fixed /= np.max(np.abs(fixed))\n",
    "fixed *= rescale_target\n",
    "plt.plot(librispeech_ds[i][\"audio\"][\"array\"], alpha=0.5)\n",
    "plt.plot(fixed, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(fixed, rate=ds[i][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_item(item):\n",
    "    arr = item[\"audio\"][\"array\"].copy()\n",
    "    # remove DC distortion\n",
    "    arr -= arr.mean()\n",
    "\n",
    "    # pad\n",
    "    arr = np.pad(arr, (int(16000 * left_pad), 0), mode=\"constant\", constant_values=0)\n",
    "\n",
    "    arr += librispeech_mean_stats[\"mean\"]\n",
    "    arr /= np.max(np.abs(arr))\n",
    "    arr *= rescale_target\n",
    "    item[\"audio\"][\"array\"] = arr\n",
    "    return item\n",
    "ds = ds.map(rescale_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add offsets to all annotations\n",
    "def fix_offsets(item):\n",
    "    for annot in [\"word_detail\", \"word_raw_detail\", \"phonetic_detail\"]:\n",
    "        item[annot][\"start\"] = [start + int(left_pad * item[\"audio\"][\"sampling_rate\"]) for start in item[annot][\"start\"]]\n",
    "        item[annot][\"stop\"] = [end + int(left_pad * item[\"audio\"][\"sampling_rate\"]) for end in item[annot][\"stop\"]]\n",
    "\n",
    "    return item\n",
    "ds = ds.map(fix_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"charsiu/tokenizer_en_cmu\")\n",
    "feature_extractor = transformers.Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = transformers.Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_phonemic_detail(item):\n",
    "    starts = copy(item[\"phonetic_detail\"][\"start\"])\n",
    "    stops = copy(item[\"phonetic_detail\"][\"stop\"])\n",
    "    utterances = copy(item[\"phonetic_detail\"][\"utterance\"])\n",
    "\n",
    "    # remove stress annotations\n",
    "    utterances = [re.sub(r\"\\d\", \"\", u) for u in utterances]\n",
    "\n",
    "    item[\"phonemic_detail\"] = {\n",
    "        \"start\": starts,\n",
    "        \"stop\": stops,\n",
    "        \"utterance\": utterances\n",
    "    }\n",
    "\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_phonetic_detail(item, idx, drop_phones=None, key=\"phonetic_detail\"):\n",
    "    \"\"\"\n",
    "    Group phonetic_detail entries according to the containing word.\n",
    "    \"\"\"\n",
    "    phonetic_detail = item[key]\n",
    "    word_detail = item[\"word_detail\"]\n",
    "\n",
    "    # Assure that each phone gets mapped to exactly one word. We'll arbitrarily map to the\n",
    "    # first word that contains the phone; this seems to most frequently match TIMIT annotation standards\n",
    "    phone_mask = np.zeros(len(phonetic_detail[\"start\"]), dtype=bool)\n",
    "    # Note that we also assign phonemes which span words to the leftmost word, consistent\n",
    "    # with this strategy\n",
    "\n",
    "    word_phonetic_detail = []\n",
    "    for start, stop, word in zip(word_detail[\"start\"], word_detail[\"stop\"], word_detail[\"utterance\"]):\n",
    "        word_phonetic_detail.append([])\n",
    "        for j, (phon_start, phon_stop, phon) in enumerate(zip(phonetic_detail[\"start\"], phonetic_detail[\"stop\"], phonetic_detail[\"utterance\"])):\n",
    "            if phone_mask[j]:\n",
    "                continue\n",
    "            elif drop_phones is not None and phon in drop_phones:\n",
    "                phone_mask[j] = True\n",
    "                continue\n",
    "            \n",
    "            # if the phoneme has start in this word, assign it to this word\n",
    "            if phon_start >= start and phon_start < stop:\n",
    "                phone_mask[j] = True\n",
    "                word_phonetic_detail[-1].append({\"phone\": phon, \"start\": phon_start, \"stop\": phon_stop})\n",
    "\n",
    "        if len(word_phonetic_detail[-1]) == 0:\n",
    "            if word == \"\":\n",
    "                # expected for these empty-word cases in librispeech annotations\n",
    "                continue\n",
    "            preceding_word_phones = \" \".join(phone[\"phone\"] for phone in word_phonetic_detail[-2]) if len(word_phonetic_detail) > 1 else \"\"\n",
    "            L.warning(f\"No phones found for word {word} in item {idx} ({item['text']}) (preceding word: {preceding_word_phones})\")\n",
    "\n",
    "    for unused_phone in np.flatnonzero(~phone_mask):\n",
    "        preceding_phones = \" \".join(phonetic_detail[\"utterance\"][max(0, unused_phone - 3):unused_phone])\n",
    "        following_phones = \" \".join(phonetic_detail[\"utterance\"][unused_phone + 1:min(len(phonetic_detail[\"utterance\"]), unused_phone + 4)])\n",
    "        unused_phone_str = phonetic_detail[\"utterance\"][unused_phone]\n",
    "        L.warning(f\"Unused phone {unused_phone_str} in item {idx} ({item['text']}) (preceding: {preceding_phones}, following: {following_phones})\")\n",
    "\n",
    "    # from pprint import pprint\n",
    "    # pprint(list(zip(word_detail[\"start\"], word_detail[\"stop\"], word_detail[\"utterance\"])))\n",
    "    # pprint(list(zip(phonetic_detail[\"start\"], phonetic_detail[\"stop\"], phonetic_detail[\"utterance\"])))\n",
    "    # pprint(word_phonetic_detail)\n",
    "\n",
    "    item[f\"word_{key}\"] = word_phonetic_detail\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_syllabic_detail(item):\n",
    "    word_syllables = []\n",
    "\n",
    "    # syllabifier doesn't use stress information so we can just use\n",
    "    # phonemic detail here\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        phones = [ph[\"phone\"] for ph in word if ph[\"phone\"] not in [\"[SIL]\", \"\"]]\n",
    "        if len(phones) > 0:\n",
    "            syllables = syllabifier.syllabify(syllabifier.EnglishIPA, phones)\n",
    "\n",
    "            assert phones == list(itertools.chain.from_iterable(\n",
    "                [tuple(onset) + tuple(nucleus) + tuple(coda) for stress, onset, nucleus, coda in syllables]))\n",
    "            # print(syllables)\n",
    "            # word[\"syllables\"] = syllables\n",
    "\n",
    "            phoneme_idx, syllable_idx = 0, 0\n",
    "            syllable_dicts = []\n",
    "            for stress, onset, nucleus, coda in syllables:\n",
    "                syllable_phones = tuple(onset + nucleus + coda)\n",
    "                syllable_dict = {\n",
    "                    \"phones\": syllable_phones,\n",
    "                    \"idx\": syllable_idx,\n",
    "                    \"phoneme_start_idx\": phoneme_idx,\n",
    "                    \"phoneme_end_idx\": phoneme_idx + len(syllable_phones), # exclusive\n",
    "                    \"stress\": stress,\n",
    "\n",
    "                    \"start\": word[phoneme_idx][\"start\"],\n",
    "                    \"stop\": word[phoneme_idx + len(syllable_phones) - 1][\"stop\"],\n",
    "                }\n",
    "\n",
    "                # Add cross-reference data in word_phonemic_detail\n",
    "                for j, ph in enumerate(syllable_phones):\n",
    "                    word[phoneme_idx + j][\"syllable_idx\"] = syllable_idx\n",
    "                    word[phoneme_idx + j][\"idx_in_syllable\"] = j\n",
    "                    word[phoneme_idx + j][\"syllable_phones\"] = tuple(syllable_phones)\n",
    "                    word[phoneme_idx + j][\"stress\"] = stress\n",
    "                    word[phoneme_idx + j][\"syllable_start\"] = syllable_dict[\"start\"]\n",
    "                    word[phoneme_idx + j][\"syllable_stop\"] = syllable_dict[\"stop\"]\n",
    "\n",
    "                syllable_dicts.append(syllable_dict)\n",
    "                phoneme_idx += len(syllable_phones)\n",
    "                syllable_idx += 1\n",
    "        else:\n",
    "            syllable_dicts = []\n",
    "\n",
    "        word_syllables.append(syllable_dicts)\n",
    "    \n",
    "    item[\"word_syllable_detail\"] = word_syllables\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_item(item, idx, drop_phones=None):\n",
    "    try:\n",
    "        grouped_phonemic_detail = item[\"word_phonemic_detail\"]\n",
    "        grouped_syllable_detail = item[\"word_syllable_detail\"]\n",
    "        assert len(grouped_phonemic_detail) == len(item[\"word_detail\"][\"utterance\"])\n",
    "        assert len(grouped_syllable_detail) == len(item[\"word_detail\"][\"utterance\"])\n",
    "\n",
    "        all_phonemes = [phon[\"phone\"] for word in grouped_phonemic_detail for phon in word]\n",
    "        all_phonemes_syll = [phone for word in item[\"word_syllable_detail\"] for syllable in word for phone in syllable[\"phones\"]]\n",
    "        assert len(all_phonemes) == len(all_phonemes_syll)\n",
    "        assert all_phonemes == all_phonemes_syll, \"phonemic detail does not match phonemes within syllable detail\"\n",
    "\n",
    "        # NB we do expect a mismatch here since some phonemes in the flat representation\n",
    "        # won't appear in the word grouped representation, if they are outside the span of a word\n",
    "        # all_phonemes_flat = [ph for ph in item[\"phonemic_detail\"][\"utterance\"] if ph not in (drop_phones or [])]\n",
    "        # assert all_phonemes == all_phonemes_flat, \\\n",
    "        #     f\"grouped phonemic detail does not match non-grouped phonemic detail in item {idx}:\" \\\n",
    "        #     f\"\\n{item['text']}\\n{all_phonemes}\\n{all_phonemes_flat}\"\n",
    "    except Exception as e:\n",
    "        L.error(f\"Error in item {idx} ({item['text']})\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_audio(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_idx(item, idx):\n",
    "    item[\"idx\"] = idx\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "\n",
    "dev_dataset = ds.map(add_phonemic_detail)\n",
    "dev_dataset = dev_dataset.map(group_phonetic_detail, with_indices=True,\n",
    "                              fn_kwargs=dict(drop_phones=drop_phones))\n",
    "dev_dataset = dev_dataset.map(group_phonetic_detail, with_indices=True,\n",
    "                              fn_kwargs=dict(key=\"phonemic_detail\", drop_phones=drop_phones))\n",
    "\n",
    "dev_dataset = dev_dataset.map(add_syllabic_detail)\n",
    "\n",
    "dev_dataset.map(check_item, with_indices=True)\n",
    "\n",
    "dev_dataset = dev_dataset.map(prepare_audio)\n",
    "dev_dataset = dev_dataset.map(add_idx, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item(item_idx, ax, plot_units=\"phoneme\", viz_rate=1000):\n",
    "    item = dev_dataset[item_idx]\n",
    "\n",
    "    times = np.linspace(0, len(item[\"input_values\"]) / 16000, int(len(item[\"input_values\"]) / 16000 * viz_rate))\n",
    "    # normalize to [-1, 1]\n",
    "    values = np.array(item[\"input_values\"])\n",
    "    values = (values - values.min()) / (values.max() - values.min()) * 2 - 1\n",
    "    # resample to viz frame rate\n",
    "    values = np.interp(times, np.arange(len(values)) / 16000, values)\n",
    "    ax.plot(times, values, alpha=0.2)\n",
    "\n",
    "    # plot word and phoneme boundaries\n",
    "    for i, word in enumerate(item[\"word_phonemic_detail\"]):\n",
    "        if not word:\n",
    "            continue\n",
    "        word_str = item[\"word_detail\"][\"utterance\"][i]\n",
    "\n",
    "        word_start, word_stop = word[0][\"start\"] / 16000, word[-1][\"stop\"] / 16000\n",
    "        ax.axvline(word_start, color=\"black\", linestyle=\"--\")\n",
    "        ax.text(word_start, 0.8, word_str, rotation=90, verticalalignment=\"bottom\", alpha=0.7)\n",
    "\n",
    "        if plot_units == \"phoneme\":\n",
    "            for j, phoneme in enumerate(word):\n",
    "                phoneme_str = phoneme[\"phone\"]\n",
    "                phoneme_start, phoneme_stop = phoneme[\"start\"] / 16000, phoneme[\"stop\"] / 16000\n",
    "\n",
    "                if j > 0:\n",
    "                    color = \"black\" if phoneme[\"idx_in_syllable\"] == 0 else \"gray\"\n",
    "                    ax.axvline(phoneme_start, color=color, linestyle=\":\", alpha=0.5)\n",
    "                ax.text(phoneme_start + 0.01, -6, phoneme_str, rotation=90, verticalalignment=\"bottom\",\n",
    "                        fontdict={\"size\": 15})\n",
    "        elif plot_units == \"syllable\":\n",
    "            for j, syllable in enumerate(item[\"word_syllable_detail\"][i]):\n",
    "                syllable_str = \" \".join(syllable[\"phones\"])\n",
    "                syllable_start, syllable_stop = syllable[\"start\"] / 16000, syllable[\"stop\"] / 16000\n",
    "\n",
    "                if j > 0:\n",
    "                    ax.axvline(syllable_start, color=\"black\", linestyle=\":\", alpha=0.5)\n",
    "                ax.text(syllable_start + 0.01, -6, syllable_str, rotation=90, verticalalignment=\"bottom\",\n",
    "                        fontdict={\"size\": 15})\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown plot_units: {plot_units}\")\n",
    "\n",
    "    # align at origin\n",
    "    ax.set_ylim((-8, 8))\n",
    "\n",
    "    ax.set_title(f\"{item['id']}: {item['text']}\")\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(False)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 1, figsize=(25, 2 * 8))\n",
    "idx = np.random.choice(len(dev_dataset))\n",
    "print(idx)\n",
    "plot_item(idx, axs[0], plot_units=\"phoneme\")\n",
    "plot_item(idx, axs[1], plot_units=\"syllable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset.save_to_disk(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
