{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a trial has four defining factors. NB that a trial is more than an item now; itâ€™s the conjunction of two items and an actual target we are trying to reach:\n",
    "\n",
    "- source inflected phones (e.g. P ER P)\n",
    "- target base phones (e.g. D AW)\n",
    "- target GT next phone (e.g. D)\n",
    "- target actual desired next phone (e.g. B)\n",
    "\n",
    "from boolean statements relating these values we can derive critical conditions:\n",
    "\n",
    "- **Control**: source next phone != target next phone. Tests how reachable the target\n",
    "- **Weak experiment**: Can we reach the GT next phone? True when target GT == target actual\n",
    "- **Strong experiment**: Can we reach non-GT next phones which are attested in the lexicon? True when target GT != target actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import analogy, analogy_pseudocausal\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"ffff_32-pc-mAP1\"#discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "dataset = train_dataset\n",
    "experiment = \"syllable_at_0\"\n",
    "\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{dataset}.npy\"\n",
    "\n",
    "inputs_dir = f\"outputs/analogy_pseudocausal_broad/inputs/{dataset}/w2v2_pc/{experiment}\"\n",
    "instances_path = f\"{inputs_dir}/instances.csv\"\n",
    "state_space_specs_path = f\"{inputs_dir}/state_space_spec.h5\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OmegaConf from yaml with `experiment`\n",
    "config = OmegaConf.load(f\"conf/experiments/analogy_pseudocausal/{experiment}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fns = [\n",
    "    (\"mean_within_cut\", config.unit_level)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea3c8eff406466f9fff317470ab94db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f860101ae86c4ad98522d8585a4eca43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating:   0%|          | 0/32046 [00:00<?, ?label/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, agg_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(config.unit_level, level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "agg_flat_idxs = pd.Series(list(range(len(agg_src))),\n",
    "                          index=pd.MultiIndex.from_tuples([tuple(xs) for xs in agg_src],\n",
    "                                                          names=[\"label_idx\", \"instance_idx\", \"frame_idx\"]))\n",
    "cuts_df = pd.merge(cuts_df, agg_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(cuts_df.description.iloc[0]) == tuple:\n",
    "    cuts_df[\"description\"] = cuts_df.description.apply(''.join)\n",
    "cut_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "word_freq_df = word_freq_df.loc[~word_freq_df.index.duplicated()]\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df = pd.read_csv(instances_path)\n",
    "all_instances_df[\"base_phones\"] = all_instances_df[\"base_phones\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_unit_set = set(all_instances_df.post_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_cohort = defaultdict(set)\n",
    "for _, row in all_instances_df.iterrows():\n",
    "    expt_cohort[row[\"base_phones\"]].add(row[\"post_divergence\"])\n",
    "expt_cohort = dict(expt_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b40856a1c1f44cdbbda2c476bf59d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare prediction equivalences: effectively a set of evaluations which \n",
    "# can be run on any individual prediction trial, establishing which outputs\n",
    "# are \"correct\" or incorrect\n",
    "all_prediction_equivalences = {}\n",
    "\n",
    "for (base_phones, inflected_phones, next_unit), _ in tqdm(all_instances_df.groupby([\"base_phones\", \"inflected_phones\", \"inflection\"])):\n",
    "    equiv_key = (inflected_phones,)\n",
    "    all_prediction_equivalences[equiv_key] = \\\n",
    "        analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_forms, base_phones, next_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_inflections = [\n",
    "    {\"base_phones\": cohort,\n",
    "     \"inflected_phones\": f\"{cohort} {unit}\".strip(),\n",
    "     \"counterfactual_inflection\": unit,\n",
    "     \"post_divergence\": unit}\n",
    "    for cohort, next_units in expt_cohort.items()\n",
    "    for unit in next_units\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctf_trials = pd.merge(\n",
    "    all_instances_df[[\"base_phones\", \"inflection\", \"cohort_length\", \"next_phoneme_idx\", \"inflected\", \"inflected_idx\", \"inflected_instance_idx\"]],\n",
    "    pd.DataFrame(counterfactual_inflections))\n",
    "ctf_trials = ctf_trials[ctf_trials.inflection != ctf_trials.counterfactual_inflection]\n",
    "ctf_trials[\"inflection\"] = \"ctf-\" + ctf_trials.counterfactual_inflection\n",
    "ctf_trials = ctf_trials.drop(columns=[\"counterfactual_inflection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials = pd.concat([ctf_trials, all_instances_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth experiments:\n",
    "# these use arbitrary sources to try to predict the ground-truth next phoneme observed in word tokens\n",
    "gt_experiments = {\n",
    "    f\"gt-{source_inflection}_{prefix}_{target_inflection}\": {\n",
    "        \"base_query\": f\"inflection == '{source_inflection}'\",\n",
    "        \"inflected_query\": f\"base_phones == '{prefix}' and inflection == '{target_inflection}'\",\n",
    "        \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "        \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "    }\n",
    "    for source_inflection in next_unit_set\n",
    "    for prefix, valid_next_phones in expt_cohort.items()\n",
    "    for target_inflection in valid_next_phones\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counterfactual experiments:\n",
    "# these use arbitrary sources to try to generate other phoneme completions which are not the\n",
    "# ground-truth next phoneme observed in word tokens, but which are consistent with an attested\n",
    "# word prefix in the lexicon\n",
    "ctf_experiments = {\n",
    "    f\"ctf-{source_inflection}_{prefix}_{target_inflection}\": {\n",
    "        \"base_query\": f\"inflection == '{source_inflection}'\",\n",
    "        \"inflected_query\": f\"base_phones == '{prefix}' and inflection == 'ctf-{target_inflection}'\",\n",
    "        \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "        \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "    }\n",
    "    for source_inflection in next_unit_set\n",
    "    for prefix, valid_next_phones in expt_cohort.items()\n",
    "    for target_inflection in valid_next_phones\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    # **gt_experiments,\n",
    "    **ctf_experiments,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reinstate this\n",
    "# small_targets = all_instances_df[all_instances_df.inflection.str.startswith(\"small-\")].inflection.str.split(\"small-\").str[1].unique()\n",
    "# for phone in small_targets:\n",
    "#     for source_phone in next_phon_set:\n",
    "#         experiments[f\"{source_phone}-to-small-{phone}\"] = {\n",
    "#             \"base_query\": f\"inflection == '{source_phone}'\",\n",
    "#             \"inflected_query\": f\"inflection == 'small-{phone}'\",\n",
    "#             \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "#             \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctf-AH__SEH\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b559c0d2eb4819b12630330a3b463a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484113b5ae042f78686b9426c732048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ('AH', \"other's\") -> ('SEH', 'involved') invited IHN VAY TIHD 0.3593381481586116\n",
      " ('AH', \"other's\") -> ('SEH', 'involved') involuntarily IHN VAA LAHN TER AH LIY 0.3602719551589702\n",
      " ('AH', \"other's\") -> ('SEH', 'involved') unavailing AH NAH VEY LIHNG 0.36736555376849994\n",
      " ('AH', \"other's\") -> ('SEH', 'involved') understood AHN DER STUHD 0.37438648830618393\n",
      " ('AH', \"other's\") -> ('SEH', 'involved') involve IHN VAALV 0.37769021418356064\n",
      " ('AH', \"other's\") -> ('SEH', 'lincoln') england IHNG GLAHND 0.3784921365883768\n",
      " ('AH', \"other's\") -> ('SEH', 'lincoln') olenin AA LIH NIHN 0.3803331381943457\n",
      " ('AH', \"other's\") -> ('SEH', 'lincoln') olenin AA LIH NIHN 0.38131541685301873\n",
      " ('AH', \"other's\") -> ('SEH', 'lincoln') english IHNG GLIHSH 0.38221096715512803\n",
      " ('AH', \"other's\") -> ('SEH', 'lincoln') enquired IHN KWAYRD 0.38841173498850334\n",
      " ('AH', \"another's\") -> ('SEH', 'solution') conversation KAAN VER SEY SHAHN 0.4585924603634903\n",
      " ('AH', \"another's\") -> ('SEH', 'solution') affection AH FEHK SHAHN 0.4611212727576959\n",
      " ('AH', \"another's\") -> ('SEH', 'solution') invitation IHN VIH TEY SHAHN 0.47170558472484225\n",
      " ('AH', \"another's\") -> ('SEH', 'solution') creation KRIY EY SHAHN 0.471722208495964\n",
      " ('AH', \"another's\") -> ('SEH', 'solution') connection KAH NEHK SHAHN 0.471771486622757\n",
      " ('AH', 'across') -> ('SEH', 'attitude') edition IH DIH SHAHN 0.3404340508013437\n",
      " ('AH', 'across') -> ('SEH', 'attitude') attitude AE TAH TUWD 0.35182202841693366\n",
      " ('AH', 'across') -> ('SEH', 'attitude') decision DIH SIH ZHAHN 0.36033804449542806\n",
      " ('AH', 'across') -> ('SEH', 'attitude') attitude AE TAH TUWD 0.3644820978329447\n",
      " ('AH', 'across') -> ('SEH', 'attitude') addition AH DIH SHAHN 0.3649622905842943\n",
      " ('AH', 'a') -> ('SEH', 'circumstances') circumstances SER KAHM STAEN SIHZ 0.40571215727074356\n",
      " ('AH', 'a') -> ('SEH', 'circumstances') circumstances SER KAHM STAEN SIHZ 0.4105816202628674\n",
      " ('AH', 'a') -> ('SEH', 'circumstances') circumstances SER KAHM STAEN SIHZ 0.41106375195281275\n",
      " ('AH', 'a') -> ('SEH', 'circumstances') circumstances SER KAHM STAEN SAHZ 0.4120259915668594\n",
      " ('AH', 'a') -> ('SEH', 'circumstances') circumstances SER KAHM STAEN SIHZ 0.4132550156741275\n"
     ]
    }
   ],
   "source": [
    "# name = \"ctf-AH__SEH\"\n",
    "# ret = analogy_pseudocausal.run_experiment_equiv_level(\n",
    "#     name,\n",
    "#     ctf_experiments[name],\n",
    "#     state_space_spec, all_trials,\n",
    "#     agg, agg_src,\n",
    "#     cut_phonemic_forms=cut_forms,\n",
    "#     prediction_equivalences=all_prediction_equivalences,\n",
    "#     verbose=True,\n",
    "#     num_samples=5,\n",
    "#     max_num_vector_samples=100,\n",
    "#     seed=seed,\n",
    "#     device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t_agg = torch.tensor(agg, device=device)\n",
    "t_agg_src = torch.tensor(agg_src, device=device)\n",
    "\n",
    "# pre-compute flat idx lookup\n",
    "flat_idx_lookup = {(label_idx, instance_idx, phoneme_idx): flat_idx\n",
    "                    for flat_idx, (label_idx, instance_idx, phoneme_idx) in enumerate(agg_src)}\n",
    "\n",
    "experiment_results = pd.concat({\n",
    "    experiment: analogy_pseudocausal.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_trials,\n",
    "        t_agg, t_agg_src,\n",
    "        flat_idx_lookup=flat_idx_lookup,\n",
    "        cut_phonemic_forms=cut_forms,\n",
    "        prediction_equivalences=all_prediction_equivalences,\n",
    "        num_samples=250,\n",
    "        max_num_vector_samples=100,\n",
    "        seed=seed,\n",
    "        device=device)\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[\"control\"] = experiment_results.inflection_to.str.split(\"-\").str[-1] != experiment_results.inflection_from\n",
    "experiment_results[\"ctf\"] = experiment_results.inflection_to.str.startswith(\"ctf-\")\n",
    "experiment_results[\"inflection_to_clean\"] = experiment_results.inflection_to.str.replace(\"^[a-z]+-\", \"\", regex=True)\n",
    "experiment_results.to_csv(f\"{output_dir}/experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"matches_next_phoneme_weak_target_rank\"\n",
    "experiment_results.query(\"not control and ctf\").groupby([\"ctf\", \"to_base_phones\", \"inflection_to_clean\"])[metric].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"matches_next_phoneme_weak_target_rank\"\n",
    "advantage_df = \\\n",
    "    experiment_results.query(\"not control and ctf\").groupby([\"ctf\", \"to_base_phones\", \"inflection_to_clean\"])[metric].mean() - \\\n",
    "    experiment_results.query(\"control and ctf\").groupby([\"ctf\", \"to_base_phones\", \"inflection_to_clean\"])[metric].mean()\n",
    "advantage_df.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.groupby([\"ctf\", \"control\", \"inflection_to_clean\"])[metric].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.query(\"ctf\")[[\"inflection_from\", \"inflection_to\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_mat = experiment_results.query(\"ctf\").pivot_table(\n",
    "    index=[\"inflection_from\"],\n",
    "    columns=[\"inflection_to\"],\n",
    "    values=\"matches_next_phoneme_weak_target_rank\",\n",
    ")\n",
    "sns.heatmap(advantage_mat / np.diag(advantage_mat.values), cmap=\"coolwarm\", center=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
