{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"ff_32\"#discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/analogy/inputs/{train_dataset}/w2v2_pc/state_space_spec.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    (\"mean_within_cut\", \"phoneme\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jgauthier/transformers/lib/python3.10/site-packages/tables/attributeset.py:295: DataTypeWarning: Unsupported type for attribute 'labels_are_repr' in node '/'. Offending HDF5 class: 8\n",
      "  value = self._g_getattr(self._v_node, name)\n"
     ]
    }
   ],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed034c49a5d647cab0121273495e3e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f9008b2aac4ecf825cc18b2b3bd114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating:   0%|          | 0/32046 [00:00<?, ?label/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, agg_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "agg_flat_idxs = pd.Series(list(range(len(agg_src))),\n",
    "                          index=pd.MultiIndex.from_tuples([tuple(xs) for xs in agg_src],\n",
    "                                                          names=[\"label_idx\", \"instance_idx\", \"frame_idx\"]))\n",
    "cuts_df = pd.merge(cuts_df, agg_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "word_freq_df = word_freq_df.loc[~word_freq_df.index.duplicated()]\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_phon_set = set(\"AH ER IH L S Z T D M N\".split())\n",
    "target_cohort_length = 2\n",
    "# defines an alternative \"small\" cohort: prefixes which have only N of the above phones\n",
    "target_small_cohort_size = 3\n",
    "assert target_small_cohort_size < len(next_phon_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c950f2c17f0244ccb689a0141a0a942d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohorts = defaultdict(set)\n",
    "for phones in tqdm(cut_phonemic_forms.unique()):\n",
    "    phones = tuple(phones.split())\n",
    "    for i in range(len(phones)):\n",
    "        cohorts[phones[:i + 1]].add(phones)\n",
    "\n",
    "csz_next = pd.DataFrame([(\" \".join(coh), \" \".join(item), item[len(coh)]) for coh, items in cohorts.items()\n",
    "                            for item in items if len(item) > len(coh)],\n",
    "                            columns=[\"cohort\", \"item\", \"next_phoneme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1704205/910310388.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cohort\n",
       "AH N    [AA, AE, AH, AO, AW, AY, B, CH, D, EH, ER, EY,...\n",
       "K ER    [AA, AE, AH, AO, AW, B, CH, D, EH, ER, EY, F, ...\n",
       "K OW    [AH, B, CH, D, ER, HH, IH, IY, JH, K, L, M, N,...\n",
       "L AY    [AE, AH, B, D, DH, ER, F, IH, K, L, M, N, P, R...\n",
       "P ER    [AA, AE, AH, AY, B, CH, D, EH, ER, EY, F, G, H...\n",
       "P EY    [AH, D, ER, G, IH, JH, L, M, N, P, S, SH, T, T...\n",
       "R OW    [AH, B, D, ER, G, HH, IH, IY, K, L, M, N, P, S...\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt_cohort = csz_next[csz_next.cohort.str.count(\" \") == target_cohort_length - 1] \\\n",
    "    .groupby(\"cohort\").filter(lambda xs: set(xs.next_phoneme) >= next_phon_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n",
    "expt_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1704205/2887618418.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cohort\n",
       "AA F      [AH, L, T]\n",
       "AO G     [AH, ER, M]\n",
       "AY OW     [AH, L, T]\n",
       "EH TH    [AH, IH, N]\n",
       "ER EY      [D, N, Z]\n",
       "ER JH    [AH, D, IH]\n",
       "ER OW      [M, N, Z]\n",
       "F UW       [D, L, Z]\n",
       "K OY       [L, N, T]\n",
       "TH AH      [D, M, N]\n",
       "W AW       [N, T, Z]\n",
       "Z AY      [AH, D, S]\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now search for type-small cohorts -- cohorts which only have N of the phone set\n",
    "expt_cohort_small = csz_next[csz_next.cohort.str.count(\" \") == target_cohort_length - 1].groupby(\"cohort\").filter(lambda xs: len(set(xs.next_phoneme)) == target_small_cohort_size and set(xs.next_phoneme) <= next_phon_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n",
    "expt_cohort_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare instance-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_equivalences(cohort, next_phoneme):\n",
    "    \"\"\"\n",
    "    Equivalence-class vocabulary for evaluating predictions on the given\n",
    "    cohort + next phoneme.\n",
    "    \"\"\"\n",
    "\n",
    "    cohort_length = cohort.count(\" \") + 1\n",
    "\n",
    "    # next-phoneme strict: match next phoneme\n",
    "    matches_next_phoneme = cuts_df.xs(cohort_length, level=\"frame_idx\") \\\n",
    "        .query(\"description == @next_phoneme\")\n",
    "\n",
    "    # next-phoneme weak: match next phoneme, but allow predicting that phoneme frame\n",
    "    # or any future frame of the word\n",
    "    matches_next_phoneme_weak = cuts_df.query(\"frame_idx >= @cohort_length\").merge(\n",
    "        matches_next_phoneme.traj_flat_idx.rename(\"next_phoneme_flat_idx\"),\n",
    "        how=\"inner\", left_index=True, right_index=True\n",
    "    )\n",
    "\n",
    "    # matches cohort\n",
    "    matches_cohort = cut_phonemic_forms[cut_phonemic_forms.str.match(f\"^{cohort}\")].index\n",
    "    matches_cohort = pd.merge(cuts_df.reset_index(),\n",
    "                              matches_cohort.to_frame(index=False),\n",
    "                              how=\"inner\",\n",
    "                              on=[\"label\", \"instance_idx\"]) \\\n",
    "        .query(\"frame_idx >= @cohort_length\")\n",
    "    \n",
    "    matches_cohort_and_next_phoneme = matches_next_phoneme[\n",
    "        matches_next_phoneme.traj_flat_idx.isin(matches_cohort.traj_flat_idx)]\n",
    "    \n",
    "    matches_cohort_and_next_phoneme_weak = matches_next_phoneme_weak[\n",
    "        matches_next_phoneme_weak.traj_flat_idx.isin(matches_cohort.traj_flat_idx)]\n",
    "\n",
    "    ret = {\n",
    "        \"matches_next_phoneme\": matches_next_phoneme.traj_flat_idx.values,\n",
    "        \"matches_next_phoneme_weak\": matches_next_phoneme_weak.traj_flat_idx.values,\n",
    "        \"matches_cohort\": matches_cohort.traj_flat_idx.values,\n",
    "        \"matches_cohort_and_next_phoneme\": matches_cohort_and_next_phoneme.traj_flat_idx.values,\n",
    "        \"matches_cohort_and_next_phoneme_weak\": matches_cohort_and_next_phoneme_weak.traj_flat_idx.values,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70f61a9c72d4aeca62320acd54ed455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AH N AH 155\n",
      "AH N D 456\n",
      "AH N IH 89\n",
      "AH N L 163\n",
      "AH N M 32\n",
      "AH N N 117\n",
      "AH N S 67\n",
      "AH N T 190\n",
      "K ER AH 87\n",
      "K ER IH 94\n",
      "K ER L 43\n",
      "K ER N 75\n",
      "K ER S 26\n",
      "K ER T 73\n",
      "K OW D 23\n",
      "K OW L 187\n",
      "K OW M 21\n",
      "K OW S 86\n",
      "K OW T 103\n",
      "L AY AH 76\n",
      "L AY IH 98\n",
      "L AY N 190\n",
      "L AY S 23\n",
      "L AY T 331\n",
      "L AY Z 54\n",
      "P ER D 16\n",
      "P ER L 72\n",
      "P ER M 137\n",
      "P ER S 521\n",
      "P ER T 225\n",
      "P ER Z 39\n",
      "P EY D 97\n",
      "P EY IH 27\n",
      "P EY L 120\n",
      "P EY N 284\n",
      "P EY S 72\n",
      "P EY T 76\n",
      "R OW D 215\n",
      "R OW L 131\n",
      "R OW M 223\n",
      "R OW S 16\n",
      "R OW T 87\n",
      "R OW Z 195\n"
     ]
    }
   ],
   "source": [
    "all_instances = []\n",
    "all_prediction_equivalences = {}\n",
    "\n",
    "# Sample at most this many combinations of cohort + next phone\n",
    "max_items_per_cohort_and_next_phone = 15\n",
    "\n",
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}\n",
    "for cohort, next_phons in tqdm(expt_cohort.items(), total=len(expt_cohort)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_phon_set:\n",
    "            continue\n",
    "\n",
    "        inflected_phones = f\"{cohort} {phon}\"\n",
    "        instances = cut_phonemic_forms[cut_phonemic_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_phone:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_phone).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "            print(cohort, phon, len(instances))\n",
    "        \n",
    "        equiv_key = (inflected_phones,)\n",
    "        if equiv_key not in all_prediction_equivalences:\n",
    "            all_prediction_equivalences[equiv_key] = prepare_equivalences(cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": phon,\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_phon_set,\n",
    "\n",
    "                \"cohort_length\": target_cohort_length,\n",
    "                \"next_phoneme_idx\": target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be9afc9d6e8476096741e09557a814d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cohort, next_phons in tqdm(expt_cohort_small.items(), total=len(expt_cohort_small)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_phon_set:\n",
    "            continue\n",
    "        inflected_phones = f\"{cohort} {phon}\"\n",
    "        instances = cut_phonemic_forms[cut_phonemic_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_phone:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_phone).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "\n",
    "        equiv_key = (inflected_phones,)\n",
    "        if equiv_key not in all_prediction_equivalences:\n",
    "            all_prediction_equivalences[equiv_key] = prepare_equivalences(cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": f\"small-{phon}\",\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_phon_set,\n",
    "\n",
    "                \"cohort_length\": target_cohort_length,\n",
    "                \"next_phoneme_idx\": target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_phones</th>\n",
       "      <th>inflected_phones</th>\n",
       "      <th>post_divergence</th>\n",
       "      <th>inflection</th>\n",
       "      <th>next_phoneme_in_restricted_set</th>\n",
       "      <th>cohort_length</th>\n",
       "      <th>next_phoneme_idx</th>\n",
       "      <th>inflected</th>\n",
       "      <th>inflected_idx</th>\n",
       "      <th>inflected_instance_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AH N</td>\n",
       "      <td>AH N AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>another</td>\n",
       "      <td>2725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AH N</td>\n",
       "      <td>AH N AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>another</td>\n",
       "      <td>2725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AH N</td>\n",
       "      <td>AH N AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>another</td>\n",
       "      <td>2725</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AH N</td>\n",
       "      <td>AH N AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>another</td>\n",
       "      <td>2725</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AH N</td>\n",
       "      <td>AH N AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>another</td>\n",
       "      <td>2725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>W AW</td>\n",
       "      <td>W AW Z</td>\n",
       "      <td>Z</td>\n",
       "      <td>small-Z</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>wowzer</td>\n",
       "      <td>28693</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>Z AY</td>\n",
       "      <td>Z AY AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>zion</td>\n",
       "      <td>25333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>Z AY</td>\n",
       "      <td>Z AY AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>zion</td>\n",
       "      <td>25333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>Z AY</td>\n",
       "      <td>Z AY D</td>\n",
       "      <td>D</td>\n",
       "      <td>small-D</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>zuyder</td>\n",
       "      <td>20487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>Z AY</td>\n",
       "      <td>Z AY S</td>\n",
       "      <td>S</td>\n",
       "      <td>small-S</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>zeiss</td>\n",
       "      <td>28782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6310 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     base_phones inflected_phones post_divergence inflection  \\\n",
       "0           AH N          AH N AH              AH         AH   \n",
       "1           AH N          AH N AH              AH         AH   \n",
       "2           AH N          AH N AH              AH         AH   \n",
       "3           AH N          AH N AH              AH         AH   \n",
       "4           AH N          AH N AH              AH         AH   \n",
       "...          ...              ...             ...        ...   \n",
       "6305        W AW           W AW Z               Z    small-Z   \n",
       "6306        Z AY          Z AY AH              AH   small-AH   \n",
       "6307        Z AY          Z AY AH              AH   small-AH   \n",
       "6308        Z AY           Z AY D               D    small-D   \n",
       "6309        Z AY           Z AY S               S    small-S   \n",
       "\n",
       "      next_phoneme_in_restricted_set  cohort_length  next_phoneme_idx  \\\n",
       "0                               True              2                 2   \n",
       "1                               True              2                 2   \n",
       "2                               True              2                 2   \n",
       "3                               True              2                 2   \n",
       "4                               True              2                 2   \n",
       "...                              ...            ...               ...   \n",
       "6305                            True              2                 2   \n",
       "6306                            True              2                 2   \n",
       "6307                            True              2                 2   \n",
       "6308                            True              2                 2   \n",
       "6309                            True              2                 2   \n",
       "\n",
       "     inflected  inflected_idx  inflected_instance_idx  \n",
       "0      another           2725                       0  \n",
       "1      another           2725                       1  \n",
       "2      another           2725                       2  \n",
       "3      another           2725                       3  \n",
       "4      another           2725                       4  \n",
       "...        ...            ...                     ...  \n",
       "6305    wowzer          28693                       2  \n",
       "6306      zion          25333                       0  \n",
       "6307      zion          25333                       1  \n",
       "6308    zuyder          20487                       0  \n",
       "6309     zeiss          28782                       0  \n",
       "\n",
       "[6310 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances_df = pd.DataFrame(all_instances)\n",
    "all_instances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_phones</th>\n",
       "      <th>inflected_phones</th>\n",
       "      <th>post_divergence</th>\n",
       "      <th>inflection</th>\n",
       "      <th>next_phoneme_in_restricted_set</th>\n",
       "      <th>cohort_length</th>\n",
       "      <th>next_phoneme_idx</th>\n",
       "      <th>inflected</th>\n",
       "      <th>inflected_idx</th>\n",
       "      <th>inflected_instance_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>AA F</td>\n",
       "      <td>AA F AH</td>\n",
       "      <td>AH</td>\n",
       "      <td>small-AH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>awful</td>\n",
       "      <td>3571</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     base_phones inflected_phones post_divergence inflection  \\\n",
       "5548        AA F          AA F AH              AH   small-AH   \n",
       "5549        AA F          AA F AH              AH   small-AH   \n",
       "5550        AA F          AA F AH              AH   small-AH   \n",
       "5551        AA F          AA F AH              AH   small-AH   \n",
       "5552        AA F          AA F AH              AH   small-AH   \n",
       "5553        AA F          AA F AH              AH   small-AH   \n",
       "5554        AA F          AA F AH              AH   small-AH   \n",
       "5555        AA F          AA F AH              AH   small-AH   \n",
       "5556        AA F          AA F AH              AH   small-AH   \n",
       "5557        AA F          AA F AH              AH   small-AH   \n",
       "5558        AA F          AA F AH              AH   small-AH   \n",
       "5559        AA F          AA F AH              AH   small-AH   \n",
       "5560        AA F          AA F AH              AH   small-AH   \n",
       "5561        AA F          AA F AH              AH   small-AH   \n",
       "5562        AA F          AA F AH              AH   small-AH   \n",
       "5563        AA F          AA F AH              AH   small-AH   \n",
       "\n",
       "      next_phoneme_in_restricted_set  cohort_length  next_phoneme_idx  \\\n",
       "5548                            True              2                 2   \n",
       "5549                            True              2                 2   \n",
       "5550                            True              2                 2   \n",
       "5551                            True              2                 2   \n",
       "5552                            True              2                 2   \n",
       "5553                            True              2                 2   \n",
       "5554                            True              2                 2   \n",
       "5555                            True              2                 2   \n",
       "5556                            True              2                 2   \n",
       "5557                            True              2                 2   \n",
       "5558                            True              2                 2   \n",
       "5559                            True              2                 2   \n",
       "5560                            True              2                 2   \n",
       "5561                            True              2                 2   \n",
       "5562                            True              2                 2   \n",
       "5563                            True              2                 2   \n",
       "\n",
       "     inflected  inflected_idx  inflected_instance_idx  \n",
       "5548     awful           3571                       4  \n",
       "5549     awful           3571                       5  \n",
       "5550     awful           3571                      16  \n",
       "5551     awful           3571                      24  \n",
       "5552     awful           3571                      33  \n",
       "5553     awful           3571                      34  \n",
       "5554     awful           3571                      35  \n",
       "5555     awful           3571                      36  \n",
       "5556     awful           3571                      39  \n",
       "5557     awful           3571                      42  \n",
       "5558     awful           3571                      44  \n",
       "5559     awful           3571                      55  \n",
       "5560     awful           3571                      56  \n",
       "5561     awful           3571                      57  \n",
       "5562     awful           3571                      58  \n",
       "5563     awful           3571                      59  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances_df.query(\"inflected == 'awful'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1704205/1620844978.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_instances_df.groupby([\"base_phones\", \"post_divergence\"]).apply(lambda xs: len(xs.inflected.unique())).sort_values()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "base_phones  post_divergence\n",
       "AA F         AH                  1\n",
       "             L                   1\n",
       "AH N         Z                   1\n",
       "AY OW        AH                  1\n",
       "R OW         IH                  1\n",
       "                                ..\n",
       "L AY         T                  16\n",
       "AH N         AH                 16\n",
       "R OW         Z                  16\n",
       "P EY         T                  16\n",
       "K OW         L                  17\n",
       "Length: 106, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances_df.groupby([\"base_phones\", \"post_divergence\"]).apply(lambda xs: len(xs.inflected.unique())).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, TypeAlias\n",
    "import torch\n",
    "from src.analysis.analogy import nxn_cos_sim\n",
    "import logging\n",
    "\n",
    "L = logging.getLogger(__name__)\n",
    "\n",
    "PredictionEquivalenceKey: TypeAlias = tuple\n",
    "PredictionEquivalenceCollection: TypeAlias = dict[PredictionEquivalenceKey, dict[str, set[int]]]\n",
    "\n",
    "\n",
    "def iter_equivalences(\n",
    "        config, all_cross_instances, agg_src: np.ndarray,\n",
    "        num_samples=100, max_num_vector_samples=250,\n",
    "        seed=None,):\n",
    "    \n",
    "    # Pre-compute lookup from label idx, instance idx to flat idx\n",
    "    if isinstance(agg_src, torch.Tensor):\n",
    "        agg_src = agg_src.cpu().numpy()\n",
    "    flat_idx_lookup = {(label_idx, instance_idx, phoneme_idx): flat_idx\n",
    "                       for flat_idx, (label_idx, instance_idx, phoneme_idx) in enumerate(agg_src)}\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if \"group_by\" in config:\n",
    "        grouper = all_cross_instances.groupby(config[\"group_by\"])\n",
    "    else:\n",
    "        grouper = [(\"\", all_cross_instances)]\n",
    "\n",
    "    for group, rows in tqdm(grouper, leave=False):\n",
    "        try:\n",
    "            if \"base_query\" in config:\n",
    "                rows_from = rows.query(config[\"base_query\"])\n",
    "            else:\n",
    "                rows_from = rows\n",
    "\n",
    "            if \"inflected_query\" in config:\n",
    "                rows_to = rows.query(config[\"inflected_query\"])\n",
    "            else:\n",
    "                rows_to = rows\n",
    "\n",
    "            if \"all_query\" in config:\n",
    "                rows_from = rows_from.query(config[\"all_query\"])\n",
    "                rows_to = rows_to.query(config[\"all_query\"])\n",
    "\n",
    "            inflection_from = rows_from.inflection.iloc[0]\n",
    "            inflection_to = rows_to.inflection.iloc[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        if len(rows_from) == 0 or len(rows_to) == 0:\n",
    "            continue\n",
    "\n",
    "        # prepare equivalences for 'from' and 'to' groups.\n",
    "        # equivalences define the set of instances over which we can average representations\n",
    "        # before computing the analogy.\n",
    "        if \"equivalence_keys\" in config:\n",
    "            from_equivalence_keys = config[\"equivalence_keys\"]\n",
    "            to_equivalence_keys = config[\"equivalence_keys\"]\n",
    "        else:\n",
    "            from_equivalence_keys = [\"inflected_phones\"]\n",
    "            to_equivalence_keys = [\"inflected_phones\"]\n",
    "\n",
    "        # we must group on at least the forms themselves\n",
    "        assert set([\"inflected_phones\"]) <= set(from_equivalence_keys)\n",
    "        assert set([\"inflected_phones\"]) <= set(to_equivalence_keys)\n",
    "\n",
    "        from_equiv = rows_from.groupby(from_equivalence_keys)\n",
    "        to_equiv = rows_to.groupby(to_equivalence_keys)\n",
    "        from_equiv_labels = [k for k, count in from_equiv.size().items() if count >= 1]\n",
    "        to_equiv_labels = [k for k, count in to_equiv.size().items() if count >= 1]\n",
    "\n",
    "        if len(set(from_equiv_labels) | set(to_equiv_labels)) <= 1:\n",
    "            # not enough labels to support transfer.\n",
    "            L.error(f\"Skipping {group} due to insufficient labels\")\n",
    "            continue\n",
    "\n",
    "        # Make sure labels are tuples\n",
    "        if not isinstance(from_equiv_labels[0], tuple):\n",
    "            from_equiv_labels = [(label,) for label in from_equiv_labels]\n",
    "        if not isinstance(to_equiv_labels[0], tuple):\n",
    "            to_equiv_labels = [(label,) for label in to_equiv_labels]\n",
    "\n",
    "        # sample pairs of base forms\n",
    "        candidate_pairs = [(x, y) for x, y in itertools.product(from_equiv_labels, to_equiv_labels) if x != y]\n",
    "        num_samples_i = min(num_samples, len(candidate_pairs))\n",
    "        samples = np.random.choice(len(candidate_pairs), num_samples_i, replace=False)\n",
    "\n",
    "        for idx in tqdm(samples, leave=False):\n",
    "            from_equiv_label_i, to_equiv_label_i = candidate_pairs[idx]\n",
    "            rows_from_i = from_equiv.get_group(tuple(from_equiv_label_i))\n",
    "            rows_to_i = to_equiv.get_group(tuple(to_equiv_label_i))\n",
    "\n",
    "            # sample pairs for comparison across the two forms\n",
    "            n = min(max_num_vector_samples, max(len(rows_from_i), len(rows_to_i)))\n",
    "            if len(rows_from_i) < n:\n",
    "                rows_from_i = rows_from_i.sample(n, replace=True)\n",
    "            elif len(rows_from_i) > n:\n",
    "                rows_from_i = rows_from_i.sample(n, replace=False)\n",
    "\n",
    "            if len(rows_to_i) < n:\n",
    "                rows_to_i = rows_to_i.sample(n, replace=True)\n",
    "            elif len(rows_to_i) > n:\n",
    "                rows_to_i = rows_to_i.sample(n, replace=False)\n",
    "\n",
    "            from_label = rows_from_i.inflected.iloc[0]\n",
    "            from_idx = rows_from_i.inflected_idx.iloc[0]\n",
    "            to_label = rows_to_i.inflected.iloc[0]\n",
    "            to_idx = rows_to_i.inflected_idx.iloc[0]\n",
    "\n",
    "            # what are the \"base\" and \"inflected\" forms?\n",
    "            from_base_phones = rows_from_i.base_phones.iloc[0].split()\n",
    "            from_post_divergence = rows_from_i.post_divergence.iloc[0].split()\n",
    "            to_base_phones = rows_to_i.base_phones.iloc[0].split()\n",
    "            to_post_divergence = rows_to_i.post_divergence.iloc[0].split()\n",
    "\n",
    "            # compute individual representation indices\n",
    "            from_inflected_flat_idx = torch.tensor(\n",
    "                [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.next_phoneme_idx)]\n",
    "                 for _, row in rows_from_i.iterrows()])\n",
    "            # TODO design choice: do we take repr from previous phoneme or averaged over all previous\n",
    "            # phonemes?\n",
    "            from_base_flat_idx = torch.tensor(\n",
    "                [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.next_phoneme_idx - 1)]\n",
    "                 for _, row in rows_from_i.iterrows()])\n",
    "            to_base_flat_idx = torch.tensor(\n",
    "                [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.next_phoneme_idx - 1)]\n",
    "                 for _, row in rows_to_i.iterrows()])\n",
    "            \n",
    "            yield {\n",
    "                \"group\": group,\n",
    "\n",
    "                \"from_label\": from_label,\n",
    "                \"from_idx\": from_idx,\n",
    "                \"to_label\": to_label,\n",
    "                \"to_idx\": to_idx,\n",
    "\n",
    "                \"from_inflected_phones\": rows_from_i.inflected_phones.iloc[0],\n",
    "                \"from_base_phones\": \" \".join(from_base_phones),\n",
    "                \"from_post_divergence\": \" \".join(from_post_divergence),\n",
    "\n",
    "                \"to_inflected_phones\": rows_to_i.inflected_phones.iloc[0],\n",
    "                \"to_base_phones\": \" \".join(to_base_phones),\n",
    "                \"to_post_divergence\": \" \".join(to_post_divergence),\n",
    "\n",
    "                \"inflection_from\": inflection_from,\n",
    "                \"inflection_to\": inflection_to,\n",
    "                \"from_equiv_label_i\": from_equiv_label_i,\n",
    "                \"to_equiv_label_i\": to_equiv_label_i,\n",
    "                \n",
    "                \"from_inflected_flat_idx\": from_inflected_flat_idx,\n",
    "                \"from_base_flat_idx\": from_base_flat_idx,\n",
    "                \"to_base_flat_idx\": to_base_flat_idx,                \n",
    "            }\n",
    "\n",
    "def run_experiment_equiv_level(\n",
    "        experiment_name, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        device: str = \"cpu\",\n",
    "        verbose=False,\n",
    "        num_samples=100, max_num_vector_samples=250,\n",
    "        seed=None,\n",
    "        prediction_equivalences: Optional[PredictionEquivalenceCollection] = None,\n",
    "        exclude_idxs_from_predictions: Optional[dict[int, list[int]]] = None,\n",
    "        include_idxs_in_predictions: Optional[dict[int, list[int]]] = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction_equivalences: defines a collection of equivalence classes\n",
    "            instantiating different prediction evaluations. Each equivalence \n",
    "            class specifies, for a given prediction instance, a set of flat\n",
    "            indices (indices into `agg` which should be counted as \"correct\")\n",
    "            for that prediction instance. The config item `prediction_equivalence_keys`\n",
    "            determines which properties of a sample returned by `iter_equivalences`\n",
    "            are used to map from item to prediction instance.\n",
    "\n",
    "            If this is `None`, then prediction success is determined based on\n",
    "            `include_idxs_in_predictions` (label indices), with a backup to\n",
    "            simply matching the single ground truth inflected label.\n",
    "    \"\"\"\n",
    "    print(experiment_name)\n",
    "\n",
    "    prediction_equivalences_tensor = None\n",
    "    if prediction_equivalences is not None:\n",
    "        if include_idxs_in_predictions is not None:\n",
    "            raise ValueError(\"Cannot specify both `prediction_equivalences` and `include_idxs_in_predictions`\")\n",
    "        if \"prediction_equivalence_keys\" not in config:\n",
    "            raise ValueError(\"`prediction_equivalence_keys` must be specified in `config` if `prediction_equivalences` is provided\")\n",
    "        \n",
    "        prediction_equivalences_tensor = {\n",
    "            key: {\n",
    "                equiv: torch.tensor(flat_idxs)\n",
    "                for equiv, flat_idxs in items.items()\n",
    "            }\n",
    "            for key, items in prediction_equivalences.items()\n",
    "        }\n",
    "\n",
    "    # move data to device\n",
    "    agg = torch.tensor(agg).to(device)\n",
    "    agg_src = torch.tensor(agg_src).to(device)\n",
    "    \n",
    "    results = []\n",
    "    for sample in iter_equivalences(\n",
    "            config, all_cross_instances, agg_src,\n",
    "            num_samples=num_samples,\n",
    "            max_num_vector_samples=max_num_vector_samples,\n",
    "            seed=seed):\n",
    "\n",
    "        from_inflected_flat_idx = sample[\"from_inflected_flat_idx\"]\n",
    "        from_base_flat_idx = sample[\"from_base_flat_idx\"]\n",
    "        to_base_flat_idx = sample[\"to_base_flat_idx\"]\n",
    "\n",
    "        # Critical analogy logic\n",
    "        pair_difference = agg[from_inflected_flat_idx] - agg[from_base_flat_idx]\n",
    "        pair_base = agg[to_base_flat_idx]\n",
    "\n",
    "        pair_predicted = pair_base + pair_difference\n",
    "        pair_predicted /= torch.norm(pair_predicted, dim=1, keepdim=True)\n",
    "\n",
    "        ### Compute ranks over entire set of word tokens\n",
    "\n",
    "        references, references_src = agg, agg_src\n",
    "        with torch.no_grad():\n",
    "            dists = 1 - nxn_cos_sim(pair_predicted, references)\n",
    "            # mean over instances of pair\n",
    "            dists = dists.mean(0)\n",
    "\n",
    "        if exclude_idxs_from_predictions is not None:\n",
    "            invalid_idxs = torch.tensor(list(exclude_idxs_from_predictions[sample[\"inflection_to\"], sample[\"to_idx\"]]))\n",
    "            invalid_flat_idxs = torch.where(torch.isin(references_src[:, 0], invalid_idxs))[0]\n",
    "            dists[invalid_flat_idxs] = torch.inf\n",
    "\n",
    "        sorted_indices = dists.argsort()\n",
    "        ranks = torch.zeros_like(sorted_indices)\n",
    "        ranks[sorted_indices] = torch.arange(len(sorted_indices)).to(sorted_indices)\n",
    "\n",
    "        ### Prepare evaluations\n",
    "\n",
    "        # Map from evaluation name to a tensor of valid flat idxs for this prediction problem\n",
    "        evaluations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "        if prediction_equivalences_tensor is not None:\n",
    "            prediction_equivalence_keys = config[\"prediction_equivalence_keys\"]\n",
    "            prediction_equivalence_keys = tuple(sample[key] for key in prediction_equivalence_keys)\n",
    "            if prediction_equivalence_keys not in prediction_equivalences_tensor:\n",
    "                continue\n",
    "\n",
    "            for subexperiment, valid_flat_idxs in prediction_equivalences_tensor[prediction_equivalence_keys].items():\n",
    "                evaluations[subexperiment] = valid_flat_idxs\n",
    "        else:\n",
    "            if include_idxs_in_predictions is not None:\n",
    "                valid_label_idxs = torch.tensor(list(include_idxs_in_predictions[sample[\"to_idx\"]])).to(device)\n",
    "            else:\n",
    "                valid_label_idxs = torch.tensor([sample[\"to_idx\"]]).to(device)\n",
    "            valid_flat_idxs = torch.where(torch.isin(references_src[:, 0], valid_label_idxs))[0]\n",
    "            evaluations[\"\"] = valid_flat_idxs\n",
    "\n",
    "        ### Run evaluations\n",
    "\n",
    "        evaluation_results = {}\n",
    "        for evaluation, valid_flat_idxs in evaluations.items():\n",
    "            nearest_neighbor = references_src[sorted_indices[0]]\n",
    "\n",
    "            # terminology\n",
    "            # target: nearest valid embedding for this evaluation\n",
    "            target_rank, target_subidx = torch.min(ranks[valid_flat_idxs], dim=0)\n",
    "            target_rank, target_idx = target_rank.item(), valid_flat_idxs[target_subidx].item()\n",
    "            target_distance = dists[target_idx].item()\n",
    "            target_label_idx = references_src[target_idx, 0].item()\n",
    "            target_instance_idx = references_src[target_idx, 1].item()\n",
    "            target_phoneme_idx = references_src[target_idx, 2].item()\n",
    "            target_label = state_space_spec.labels[target_label_idx]\n",
    "            target_phones = cut_phonemic_forms.loc[target_label].loc[target_instance_idx]\n",
    "\n",
    "            # predicted: nearest neighbor\n",
    "            predicted_label_idx = nearest_neighbor[0].item()\n",
    "            predicted_instance_idx = nearest_neighbor[1].item()\n",
    "            predicted_label = state_space_spec.labels[predicted_label_idx]\n",
    "\n",
    "            evaluation_results[evaluation] = {\n",
    "                \"target_rank\": target_rank,\n",
    "                \"target_distance\": target_distance,\n",
    "                \"target_label_idx\": target_label_idx,\n",
    "                \"target_instance_idx\": target_instance_idx,\n",
    "                \"target_phoneme_idx\": target_phoneme_idx,\n",
    "                \"target_label\": target_label,\n",
    "                \"target_phones\": target_phones,\n",
    "\n",
    "                \"predicted_distance\": dists[sorted_indices[0]].item(),\n",
    "                \"predicted_label_idx\": predicted_label_idx,\n",
    "                \"predicted_instance_idx\": predicted_instance_idx,\n",
    "                \"predicted_phoneme_idx\": nearest_neighbor[2].item(),\n",
    "                \"predicted_label\": predicted_label,\n",
    "                \"predicted_phones\": cut_phonemic_forms.loc[predicted_label].loc[predicted_instance_idx],\n",
    "            }\n",
    "\n",
    "        if verbose:\n",
    "            for dist, (label_idx, instance_idx, _) in zip(dists[sorted_indices[:5]], references_src[sorted_indices[:5]]):\n",
    "                print(f\"{sample['group']} {sample['from_equiv_label_i']} -> {sample['to_equiv_label_i']} {evaluation}: {state_space_spec.labels[label_idx]} {instance_idx}\")\n",
    "\n",
    "        # Merge into a single flat dictionary\n",
    "        results_i = {}\n",
    "        for evaluation_name, evaluation in evaluation_results.items():\n",
    "            for key, value in evaluation.items():\n",
    "                output_key = f\"{evaluation_name}_{key}\" if evaluation_name else key\n",
    "                results_i[output_key] = value\n",
    "        results_i.update({\n",
    "            \"group\": sample[\"group\"],\n",
    "\n",
    "            \"from\": sample[\"from_label\"],\n",
    "            \"to\": sample[\"to_label\"],\n",
    "\n",
    "            \"from_inflected_phones\": sample[\"from_inflected_phones\"],\n",
    "            \"from_base_phones\": sample[\"from_base_phones\"],\n",
    "            \"from_post_divergence\": sample[\"from_post_divergence\"],\n",
    "\n",
    "            \"to_inflected_phones\": sample[\"to_inflected_phones\"],\n",
    "            \"to_base_phones\": sample[\"to_base_phones\"],\n",
    "            \"to_post_divergence\": sample[\"to_post_divergence\"],\n",
    "\n",
    "            \"inflection_from\": sample[\"inflection_from\"],\n",
    "            \"inflection_to\": sample[\"inflection_to\"],\n",
    "            \"from_equiv_label\": sample[\"from_equiv_label_i\"],\n",
    "            \"to_equiv_label\": sample[\"to_equiv_label_i\"],\n",
    "        })\n",
    "\n",
    "        results.append(results_i)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    f\"{a}_to_{b}\": {\n",
    "        \"base_query\": f\"inflection == '{a}'\",\n",
    "        \"inflected_query\": f\"inflection == '{b}'\",\n",
    "        \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "        \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "    }\n",
    "    for a, b in itertools.product(next_phon_set, repeat=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_targets = all_instances_df[all_instances_df.inflection.str.startswith(\"small-\")].inflection.str.split(\"small-\").str[1].unique()\n",
    "for phone in small_targets:\n",
    "    for source_phone in next_phon_set:\n",
    "        experiments[f\"{source_phone}-to-small-{phone}\"] = {\n",
    "            \"base_query\": f\"inflection == '{source_phone}'\",\n",
    "            \"inflected_query\": f\"inflection == 'small-{phone}'\",\n",
    "            \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "            \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22a4764462a44f7b1ec6bf34c615768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bd0e6b492542728bd5d47a3d5bf416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ret = run_experiment_equiv_level(\n",
    "#     \"tst\",\n",
    "#     # config={\"base_query\": \"inflection == 'D'\",\n",
    "#     #         \"inflected_query\": \"inflection == 'T'\"},\n",
    "#     config=experiments[\"D_to_D\"],\n",
    "#     state_space_spec=state_space_spec,\n",
    "#     all_cross_instances=all_instances_df,\n",
    "#     prediction_equivalences=all_prediction_equivalences,\n",
    "#     agg=agg,\n",
    "#     agg_src=agg_src,\n",
    "#     num_samples=20,\n",
    "#     device=\"cpu\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_instances_df,\n",
    "        agg, agg_src,\n",
    "        prediction_equivalences=all_prediction_equivalences,\n",
    "        num_samples=1000,\n",
    "        max_num_vector_samples=100,\n",
    "        seed=seed,\n",
    "        device=\"cuda:2\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/pseudocausal_broad_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_phone(row):\n",
    "    predicted_phones = row.predicted_phones.split(\" \")\n",
    "    predicted_phone_idx = target_cohort_length\n",
    "    if len(predicted_phones) - 1 < predicted_phone_idx:\n",
    "        return None\n",
    "    return predicted_phones[predicted_phone_idx]\n",
    "def get_predicted_base_phones(row):\n",
    "    predicted_phones = row.predicted_phones.split(\" \")\n",
    "    return \" \".join(predicted_phones[:target_cohort_length])\n",
    "\n",
    "experiment_results[\"predicted_phone\"] = experiment_results.apply(get_predicted_phone, axis=1)\n",
    "experiment_results[\"predicted_base_phones\"] = experiment_results.apply(get_predicted_base_phones, axis=1)\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_phone == experiment_results.to_post_divergence\n",
    "experiment_results[\"correct_base\"] = experiment_results.predicted_base_phones == experiment_results.to_base_phones\n",
    "experiment_results[\"correct_distinct_base\"] = experiment_results.correct_base & (experiment_results.predicted_label_idx != experiment_results.gt_label_idx)\n",
    "experiment_results[\"correct_or_correct_base\"] = experiment_results.correct | experiment_results.correct_base\n",
    "experiment_results[\"control\"] = experiment_results.inflection_to.str.split(\"-\").str[-1] != experiment_results.inflection_from\n",
    "\n",
    "post_div_set = experiment_results.groupby(\"to_base_phones\").apply(lambda xs: frozenset(xs.to_post_divergence))\n",
    "experiment_results[\"post_div_set\"] = experiment_results.to_base_phones.map(post_div_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results = experiment_results[~experiment_results.index.get_level_values(0).str.contains(\"to-small-\")]\n",
    "small_results = experiment_results[experiment_results.index.get_level_values(0).str.contains(\"to-small-\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().groupby(\"control\").apply(lambda xs: xs / xs.sum()).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = False\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    full_phone_list += sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = main_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1.2, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = True\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    # full_phone_list += sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "    # DEV plot just the non-studied phones\n",
    "    full_phone_list = sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = main_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control and not correct\")[[\"from_inflected_phones\", \"gt_label\", \"to_base_phones\", \"correct\", \"correct_base\", \"predicted_label\", \"predicted_phones\", \"from_post_divergence\", \"predicted_phone\"]].sample(20).sort_values([\"correct\", \"correct_base\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=main_results,\n",
    "            x=\"inflection_to\", y=\"correct\", hue=\"control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(main_results.query(\"not control\").groupby([\"from\", \"control\"]).correct.mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(main_results.query(\"not control\").groupby([\"to\", \"control\"]).correct.mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = main_results.query(\"not control\").groupby([\"from_base_phones\"]).predicted_base_phones.value_counts(normalize=True).unstack().fillna(0)\n",
    "e = -(d * np.log2(d)).sum(axis=1)\n",
    "e.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity = main_results.query(\"not control\").groupby([\"from\", \"from_base_phones\", \"from_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0)\n",
    "entropy = - (diversity * np.log2(diversity)).sum(axis=1)\n",
    "entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy.sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_diversity = main_results.query(\"not control\").groupby([\"from\", \"from_base_phones\", \"from_post_divergence\"]).predicted_base_phones.value_counts(normalize=True).unstack().fillna(0)\n",
    "base_entropy = - (diversity * np.log2(diversity)).sum(axis=1)\n",
    "base_entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_diversity = main_results.query(\"not control\").groupby([\"to\", \"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0)\n",
    "to_entropy = - (to_diversity * np.log2(to_diversity)).sum(axis=1)\n",
    "to_entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_diversity.loc[\"careering\"].melt().sort_values(\"value\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_diversity.loc[\"unseen\"].melt().sort_values(\"value\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[\"from_base_final\"] = main_results.from_base_phones.str.split(\" \").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control and `to` == 'licenses'\")[[\"from\", \"from_base_phones\", \"correct\", \"correct_base\", \"to\", \"predicted_label\", \"predicted_base_phones\", \"predicted_phone\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control\").groupby([\"to\", \"to_base_phones\"]).correct.agg([\"mean\", \"count\"]).query(\"count >= 10\").sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control\").groupby([\"from_base_phones\"]).correct_base.agg([\"mean\", \"count\"]).query(\"count >= 4\").sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_with_freq = pd.merge(main_results, word_freq_df.LogFreq.rename(\"from_freq\"),\n",
    "                            left_on=\"from\", right_index=True)\n",
    "main_with_freq = pd.merge(main_with_freq, word_freq_df.LogFreq.rename(\"to_freq\"),\n",
    "                            left_on=\"to\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass(group):\n",
    "    group = group.drop_duplicates(\"inflected\")\n",
    "    group = pd.merge(group, word_freq_df.LogFreq,\n",
    "                     left_on=\"inflected\", right_index=True).set_index([\"inflected\", \"post_divergence\"])\n",
    "    # mass = group.LogFreq ** 10 / (group.LogFreq ** 10).sum()\n",
    "    mass = group.LogFreq / group.LogFreq.sum()\n",
    "    return mass\n",
    "    \n",
    "masses = all_instances_df[~all_instances_df.inflection.str.startswith(\"small-\")].groupby(\"base_phones\").apply(get_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=masses.reset_index(), x=\"LogFreq\", hue=\"base_phones\", kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=main_with_freq.query(\"not control\").groupby([\"from\", \"from_freq\"]).correct.agg([\"mean\", \"count\"]).reset_index().query(\"count >= 20\"),\n",
    "                x=\"from_freq\", y=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=main_with_freq.query(\"not control\").groupby([\"to\", \"to_freq\"]).correct.agg([\"mean\", \"count\"]).reset_index().query(\"count >= 20\"),\n",
    "                x=\"to_freq\", y=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().groupby(\"control\").apply(lambda xs: xs / xs.sum()).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[\"attested\"] = small_results.apply(lambda x: x.from_post_divergence in eval(x.post_div_set) if isinstance(x.post_div_set, str) else x.from_post_divergence in x.post_div_set, axis=1)\n",
    "small_results[\"condition\"] = \"main\"\n",
    "small_results.loc[small_results.control & small_results.attested, \"condition\"] = \"control_attested\"\n",
    "small_results.loc[small_results.control & ~small_results.attested, \"condition\"] = \"control_unattested\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=small_results, x=\"inflection_to\", y=\"correct\", hue=\"condition\", kind=\"bar\", aspect=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = False\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    full_phone_list += sorted(set(small_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = small_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1.2, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results.query(\"not control\")[[\"from_inflected_phones\", \"gt_label\", \"to_inflected_phones\", \"correct\", \"correct_base\", \"predicted_label\", \"predicted_phones\"]].sample(20) \\\n",
    "    .sort_values([\"correct\", \"correct_base\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[experiment_results.inflection_to.str.startswith(\"small-\")].groupby([\"to_base_phones\", \"inflection_from\"]).predicted_phone \\\n",
    "    .value_counts(normalize=True).unstack().fillna(0)#.reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look at a bunch of individual prediction examples to get an intuition for what is happening here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[experiment_results.inflection_to.str.startswith(\"small-\")].groupby(\"post_div_set\").correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[(experiment_results.to_base_phones == \"AA F\") & (experiment_results.to_post_divergence == \"T\")][[\"from_inflected_phones\", \"from_post_divergence\", \"to\", \"predicted_label\", \"predicted_phones\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted phone distributions for predicted words with correct base\n",
    "small_cohort_results = small_results.query(\"correct_base\").groupby([\"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0).reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)\n",
    "small_cohort_results\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(small_cohort_results.index.get_level_values(\"to_base_phones\").nunique() / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "for i, (ax, ((base_phones, target), row)) in enumerate(zip(axes.flat, small_cohort_results.sample(n_rows * n_cols).sort_index().iterrows())):\n",
    "    row = row.rename(\"accuracy\").to_frame().reset_index()\n",
    "    row[\"in_cohort\"] = row.predicted_phone.isin(expt_cohort_small.loc[base_phones])\n",
    "    sns.barplot(data=row, x=\"predicted_phone\", y=\"accuracy\", ax=ax)\n",
    "    ax.set_title(f\"{base_phones} + {target}\")\n",
    "    ax.set_xlabel(\"Predicted phone\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()bhdvh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted phone distributions for predicted words with correct base\n",
    "small_cohort_results = small_results.query(\"not correct_base\").groupby([\"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0).reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)\n",
    "small_cohort_results\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(small_cohort_results.index.get_level_values(\"to_base_phones\").nunique() / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "for i, (ax, ((base_phones, target), row)) in enumerate(zip(axes.flat, small_cohort_results.sample(n_rows * n_cols).sort_index().iterrows())):\n",
    "    row = row.rename(\"accuracy\").to_frame().reset_index()\n",
    "    row[\"in_cohort\"] = row.predicted_phone.isin(expt_cohort_small.loc[base_phones])\n",
    "    sns.barplot(data=row, x=\"predicted_phone\", y=\"accuracy\", hue=\"in_cohort\", ax=ax)\n",
    "    ax.set_title(f\"{base_phones} + {target}\")\n",
    "    ax.set_xlabel(\"Predicted phone\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(title=\"In attested cohort?\")\n",
    "    ax.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=pd.concat({\"small\": small_results.query(\"not control\").groupby(\"to_post_divergence\").correct.mean(),\n",
    "\"main\": main_results.query(\"not control\").groupby(\"to_post_divergence\").correct.mean()}, names=[\"size\"]).reset_index(),\n",
    "    x=\"to_post_divergence\", y=\"correct\", hue=\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/pseudocausal_broad_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.read_csv(f\"{output_dir}/pseudocausal_broad_experiment_results.csv\", index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
