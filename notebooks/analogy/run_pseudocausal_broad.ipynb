{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import analogy, analogy_pseudocausal\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"ffff_32-pc-mAP1\"#discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/analogy/inputs/{train_dataset}/w2v2_pc/state_space_spec.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    (\"mean_within_cut\", \"phoneme\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, agg_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "agg_flat_idxs = pd.Series(list(range(len(agg_src))),\n",
    "                          index=pd.MultiIndex.from_tuples([tuple(xs) for xs in agg_src],\n",
    "                                                          names=[\"label_idx\", \"instance_idx\", \"frame_idx\"]))\n",
    "cuts_df = pd.merge(cuts_df, agg_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "word_freq_df = word_freq_df.loc[~word_freq_df.index.duplicated()]\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_phon_set = set(\"AH ER IH L S Z T D M N\".split())\n",
    "target_cohort_length = 2\n",
    "# defines an alternative \"small\" cohort: prefixes which have only N of the above phones\n",
    "target_small_cohort_size = 3\n",
    "assert target_small_cohort_size < len(next_phon_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = defaultdict(set)\n",
    "for phones in tqdm(cut_phonemic_forms.unique()):\n",
    "    phones = tuple(phones.split())\n",
    "    for i in range(len(phones)):\n",
    "        cohorts[phones[:i + 1]].add(phones)\n",
    "\n",
    "csz_next = pd.DataFrame([(\" \".join(coh), \" \".join(item), item[len(coh)]) for coh, items in cohorts.items()\n",
    "                            for item in items if len(item) > len(coh)],\n",
    "                            columns=[\"cohort\", \"item\", \"next_phoneme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_cohort = csz_next[csz_next.cohort.str.count(\" \") == target_cohort_length - 1] \\\n",
    "    .groupby(\"cohort\").filter(lambda xs: set(xs.next_phoneme) >= next_phon_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n",
    "expt_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for type-small cohorts -- cohorts which only have N of the phone set\n",
    "expt_cohort_small = csz_next[csz_next.cohort.str.count(\" \") == target_cohort_length - 1].groupby(\"cohort\").filter(lambda xs: len(set(xs.next_phoneme)) == target_small_cohort_size and set(xs.next_phoneme) <= next_phon_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_phoneme)))\n",
    "expt_cohort_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare instance-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances = []\n",
    "all_prediction_equivalences = {}\n",
    "\n",
    "# Sample at most this many combinations of cohort + next phone\n",
    "max_items_per_cohort_and_next_phone = 15\n",
    "\n",
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}\n",
    "for cohort, next_phons in tqdm(expt_cohort.items(), total=len(expt_cohort)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_phon_set:\n",
    "            continue\n",
    "\n",
    "        inflected_phones = f\"{cohort} {phon}\"\n",
    "        instances = cut_phonemic_forms[cut_phonemic_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_phone:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_phone).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "            print(cohort, phon, len(instances))\n",
    "        \n",
    "        equiv_key = (inflected_phones,)\n",
    "        if equiv_key not in all_prediction_equivalences:\n",
    "            all_prediction_equivalences[equiv_key] = \\\n",
    "                analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms, cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": phon,\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_phon_set,\n",
    "\n",
    "                \"cohort_length\": target_cohort_length,\n",
    "                \"next_phoneme_idx\": target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort, next_phons in tqdm(expt_cohort_small.items(), total=len(expt_cohort_small)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_phon_set:\n",
    "            continue\n",
    "        inflected_phones = f\"{cohort} {phon}\"\n",
    "        instances = cut_phonemic_forms[cut_phonemic_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_phone:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_phone).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "\n",
    "        equiv_key = (inflected_phones,)\n",
    "        if equiv_key not in all_prediction_equivalences:\n",
    "            all_prediction_equivalences[equiv_key] = \\\n",
    "                analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms,\n",
    "                                                                     cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": f\"small-{phon}\",\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_phon_set,\n",
    "\n",
    "                \"cohort_length\": target_cohort_length,\n",
    "                \"next_phoneme_idx\": target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df = pd.DataFrame(all_instances)\n",
    "all_instances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df.to_csv(f\"{output_dir}/pseudocausal_broad_instances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_prediction_equivalences, f\"{output_dir}/pseudocausal_broad_prediction_equivalences.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df.groupby([\"base_phones\", \"post_divergence\"]).apply(lambda xs: len(xs.inflected.unique())).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    f\"{a}_to_{b}\": {\n",
    "        \"base_query\": f\"inflection == '{a}'\",\n",
    "        \"inflected_query\": f\"inflection == '{b}'\",\n",
    "        \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "        \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "    }\n",
    "    for a, b in itertools.product(next_phon_set, repeat=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_targets = all_instances_df[all_instances_df.inflection.str.startswith(\"small-\")].inflection.str.split(\"small-\").str[1].unique()\n",
    "for phone in small_targets:\n",
    "    for source_phone in next_phon_set:\n",
    "        experiments[f\"{source_phone}-to-small-{phone}\"] = {\n",
    "            \"base_query\": f\"inflection == '{source_phone}'\",\n",
    "            \"inflected_query\": f\"inflection == 'small-{phone}'\",\n",
    "            \"equivalence_keys\": [\"inflected_phones\", \"inflected\"],\n",
    "            \"prediction_equivalence_keys\": [\"to_inflected_phones\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy_pseudocausal.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_instances_df,\n",
    "        agg, agg_src,\n",
    "        cut_phonemic_forms=cut_phonemic_forms,\n",
    "        prediction_equivalences=all_prediction_equivalences,\n",
    "        num_samples=1000,\n",
    "        max_num_vector_samples=100,\n",
    "        seed=seed,\n",
    "        device=\"cuda:2\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[\"control\"] = experiment_results.inflection_to.str.split(\"-\").str[-1] != experiment_results.inflection_from\n",
    "\n",
    "experiment_results[\"matches_cohort_correct\"] = experiment_results.matches_cohort_target_rank == 0\n",
    "experiment_results[\"matches_next_phoneme_correct\"] = experiment_results.matches_next_phoneme_target_rank == 0\n",
    "experiment_results.to_csv(f\"{output_dir}/pseudocausal_broad_experiment_results-{model_class}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_div_set = experiment_results.groupby(\"to_base_phones\").apply(lambda xs: frozenset(xs.to_post_divergence))\n",
    "# experiment_results[\"post_div_set\"] = experiment_results.to_base_phones.map(post_div_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results = experiment_results[~experiment_results.index.get_level_values(0).str.contains(\"to-small-\")]\n",
    "small_results = experiment_results[experiment_results.index.get_level_values(0).str.contains(\"to-small-\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[[\"control\", \"matches_cohort_correct\", \"matches_next_phoneme_correct\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().groupby(\"control\").apply(lambda xs: xs / xs.sum()).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = False\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    full_phone_list += sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = main_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1.2, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = True\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    # full_phone_list += sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "    # DEV plot just the non-studied phones\n",
    "    full_phone_list = sorted(set(main_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = main_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control and not correct\")[[\"from_inflected_phones\", \"gt_label\", \"to_base_phones\", \"correct\", \"correct_base\", \"predicted_label\", \"predicted_phones\", \"from_post_divergence\", \"predicted_phone\"]].sample(20).sort_values([\"correct\", \"correct_base\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=main_results,\n",
    "            x=\"inflection_to\", y=\"correct\", hue=\"control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(main_results.query(\"not control\").groupby([\"from\", \"control\"]).correct.mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(main_results.query(\"not control\").groupby([\"to\", \"control\"]).correct.mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = main_results.query(\"not control\").groupby([\"from_base_phones\"]).predicted_base_phones.value_counts(normalize=True).unstack().fillna(0)\n",
    "e = -(d * np.log2(d)).sum(axis=1)\n",
    "e.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity = main_results.query(\"not control\").groupby([\"from\", \"from_base_phones\", \"from_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0)\n",
    "entropy = - (diversity * np.log2(diversity)).sum(axis=1)\n",
    "entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy.sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_diversity = main_results.query(\"not control\").groupby([\"from\", \"from_base_phones\", \"from_post_divergence\"]).predicted_base_phones.value_counts(normalize=True).unstack().fillna(0)\n",
    "base_entropy = - (diversity * np.log2(diversity)).sum(axis=1)\n",
    "base_entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_diversity = main_results.query(\"not control\").groupby([\"to\", \"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0)\n",
    "to_entropy = - (to_diversity * np.log2(to_diversity)).sum(axis=1)\n",
    "to_entropy.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_diversity.loc[\"careering\"].melt().sort_values(\"value\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_diversity.loc[\"unseen\"].melt().sort_values(\"value\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results[\"from_base_final\"] = main_results.from_base_phones.str.split(\" \").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control and `to` == 'licenses'\")[[\"from\", \"from_base_phones\", \"correct\", \"correct_base\", \"to\", \"predicted_label\", \"predicted_base_phones\", \"predicted_phone\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control\").groupby([\"to\", \"to_base_phones\"]).correct.agg([\"mean\", \"count\"]).query(\"count >= 10\").sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.query(\"not control\").groupby([\"from_base_phones\"]).correct_base.agg([\"mean\", \"count\"]).query(\"count >= 4\").sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_with_freq = pd.merge(main_results, word_freq_df.LogFreq.rename(\"from_freq\"),\n",
    "                            left_on=\"from\", right_index=True)\n",
    "main_with_freq = pd.merge(main_with_freq, word_freq_df.LogFreq.rename(\"to_freq\"),\n",
    "                            left_on=\"to\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass(group):\n",
    "    group = group.drop_duplicates(\"inflected\")\n",
    "    group = pd.merge(group, word_freq_df.LogFreq,\n",
    "                     left_on=\"inflected\", right_index=True).set_index([\"inflected\", \"post_divergence\"])\n",
    "    # mass = group.LogFreq ** 10 / (group.LogFreq ** 10).sum()\n",
    "    mass = group.LogFreq / group.LogFreq.sum()\n",
    "    return mass\n",
    "    \n",
    "masses = all_instances_df[~all_instances_df.inflection.str.startswith(\"small-\")].groupby(\"base_phones\").apply(get_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=masses.reset_index(), x=\"LogFreq\", hue=\"base_phones\", kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=main_with_freq.query(\"not control\").groupby([\"from\", \"from_freq\"]).correct.agg([\"mean\", \"count\"]).reset_index().query(\"count >= 20\"),\n",
    "                x=\"from_freq\", y=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=main_with_freq.query(\"not control\").groupby([\"to\", \"to_freq\"]).correct.agg([\"mean\", \"count\"]).reset_index().query(\"count >= 20\"),\n",
    "                x=\"to_freq\", y=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[[\"control\", \"correct_base\", \"correct\"]].value_counts().groupby(\"control\").apply(lambda xs: xs / xs.sum()).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results[\"attested\"] = small_results.apply(lambda x: x.from_post_divergence in eval(x.post_div_set) if isinstance(x.post_div_set, str) else x.from_post_divergence in x.post_div_set, axis=1)\n",
    "small_results[\"condition\"] = \"main\"\n",
    "small_results.loc[small_results.control & small_results.attested, \"condition\"] = \"control_attested\"\n",
    "small_results.loc[small_results.control & ~small_results.attested, \"condition\"] = \"control_unattested\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=small_results, x=\"inflection_to\", y=\"correct\", hue=\"condition\", kind=\"bar\", aspect=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_phones = False\n",
    "\n",
    "full_phone_list = sorted(next_phon_set)\n",
    "if plot_all_phones:\n",
    "    full_phone_list += sorted(set(small_results.predicted_phone.fillna(\"NA\").unique()) - set(full_phone_list))\n",
    "heatmap_results = small_results \\\n",
    "    .groupby([\"control\", \"correct_base\", \"inflection_from\"]).predicted_phone.value_counts(normalize=True) \\\n",
    "    .reindex(pd.MultiIndex.from_product([[False, True], [False, True], sorted(next_phon_set), full_phone_list],\n",
    "                                        names=[\"control\", \"correct_base\", \"inflection_from\", \"predicted_phone\"])).fillna(0)\n",
    "\n",
    "g = sns.FacetGrid(data=heatmap_results.reset_index(), row=\"control\", col=\"correct_base\", height=5, aspect=2 if plot_all_phones else 1.2, sharex=False, sharey=False)\n",
    "def f(data, **kwargs):\n",
    "    sns.heatmap(data.pivot_table(index=\"inflection_from\", columns=\"predicted_phone\", values=\"proportion\").reindex(full_phone_list, axis=1))\n",
    "g.map_dataframe(f, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_results.query(\"not control\")[[\"from_inflected_phones\", \"gt_label\", \"to_inflected_phones\", \"correct\", \"correct_base\", \"predicted_label\", \"predicted_phones\"]].sample(20) \\\n",
    "    .sort_values([\"correct\", \"correct_base\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[experiment_results.inflection_to.str.startswith(\"small-\")].groupby([\"to_base_phones\", \"inflection_from\"]).predicted_phone \\\n",
    "    .value_counts(normalize=True).unstack().fillna(0)#.reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look at a bunch of individual prediction examples to get an intuition for what is happening here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[experiment_results.inflection_to.str.startswith(\"small-\")].groupby(\"post_div_set\").correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results[(experiment_results.to_base_phones == \"AA F\") & (experiment_results.to_post_divergence == \"T\")][[\"from_inflected_phones\", \"from_post_divergence\", \"to\", \"predicted_label\", \"predicted_phones\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted phone distributions for predicted words with correct base\n",
    "small_cohort_results = small_results.query(\"correct_base\").groupby([\"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0).reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)\n",
    "small_cohort_results\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(small_cohort_results.index.get_level_values(\"to_base_phones\").nunique() / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "for i, (ax, ((base_phones, target), row)) in enumerate(zip(axes.flat, small_cohort_results.sample(n_rows * n_cols).sort_index().iterrows())):\n",
    "    row = row.rename(\"accuracy\").to_frame().reset_index()\n",
    "    row[\"in_cohort\"] = row.predicted_phone.isin(expt_cohort_small.loc[base_phones])\n",
    "    sns.barplot(data=row, x=\"predicted_phone\", y=\"accuracy\", ax=ax)\n",
    "    ax.set_title(f\"{base_phones} + {target}\")\n",
    "    ax.set_xlabel(\"Predicted phone\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()bhdvh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted phone distributions for predicted words with correct base\n",
    "small_cohort_results = small_results.query(\"not correct_base\").groupby([\"to_base_phones\", \"to_post_divergence\"]).predicted_phone.value_counts(normalize=True).unstack().fillna(0).reindex(columns=next_phon_set).fillna(0).sort_index().sort_index(axis=1)\n",
    "small_cohort_results\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(small_cohort_results.index.get_level_values(\"to_base_phones\").nunique() / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "for i, (ax, ((base_phones, target), row)) in enumerate(zip(axes.flat, small_cohort_results.sample(n_rows * n_cols).sort_index().iterrows())):\n",
    "    row = row.rename(\"accuracy\").to_frame().reset_index()\n",
    "    row[\"in_cohort\"] = row.predicted_phone.isin(expt_cohort_small.loc[base_phones])\n",
    "    sns.barplot(data=row, x=\"predicted_phone\", y=\"accuracy\", hue=\"in_cohort\", ax=ax)\n",
    "    ax.set_title(f\"{base_phones} + {target}\")\n",
    "    ax.set_xlabel(\"Predicted phone\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(title=\"In attested cohort?\")\n",
    "    ax.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=pd.concat({\"small\": small_results.query(\"not control\").groupby(\"to_post_divergence\").correct.mean(),\n",
    "\"main\": main_results.query(\"not control\").groupby(\"to_post_divergence\").correct.mean()}, names=[\"size\"]).reset_index(),\n",
    "    x=\"to_post_divergence\", y=\"correct\", hue=\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/pseudocausal_broad_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.read_csv(f\"{output_dir}/pseudocausal_broad_experiment_results.csv\", index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
