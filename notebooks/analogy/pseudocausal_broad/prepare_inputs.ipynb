{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from loguru import logger as L\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1395d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"ffff_32-pc-mAP1\"#discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "dataset = train_dataset\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "experiment = \"phoneme_at_1\"\n",
    "\n",
    "output_dir = f\"outputs/analogy_pseudocausal_broad/inputs/{train_dataset}/w2v2_pc/{experiment}\"\n",
    "\n",
    "seed = 42\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    (\"mean_within_cut\", \"phoneme\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OmegaConf from yaml with `experiment`\n",
    "config = OmegaConf.load(f\"conf/experiments/analogy_pseudocausal/{experiment}.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe338f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(config.unit_level, level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(cuts_df.description.iloc[0]) == tuple:\n",
    "    cuts_df[\"description\"] = cuts_df.description.apply(''.join)\n",
    "cut_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "word_freq_df = word_freq_df.loc[~word_freq_df.index.duplicated()]\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979df6",
   "metadata": {},
   "source": [
    "## Prepare cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.next_units is None or not config.next_units.strip():\n",
    "    all_next_units = cuts_df.description.value_counts()\n",
    "    if len(all_next_units) > 100:\n",
    "        L.warning(\"Next unit set is large, taking the top 100\")\n",
    "        all_next_units = all_next_units[:100]\n",
    "    next_unit_set = set(all_next_units.index)\n",
    "else:\n",
    "    next_unit_set = set(config.next_units.strip().split())\n",
    "assert config.target_small_cohort_size < len(next_unit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = defaultdict(set)\n",
    "for units in tqdm(cut_forms.unique()):\n",
    "    units = tuple(units.split())\n",
    "    for i in range(len(units) + 1):\n",
    "        cohorts[units[:i]].add(units)\n",
    "\n",
    "csz_next = pd.DataFrame([(\" \".join(coh), \" \".join(item), item[len(coh)]) for coh, items in cohorts.items()\n",
    "                            for item in items if len(item) > len(coh)],\n",
    "                            columns=[\"cohort\", \"item\", \"next_unit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.target_cohort_length == 0:\n",
    "    expt_cohort = csz_next[csz_next.cohort == \"\"]\n",
    "else:\n",
    "    expt_cohort = csz_next[(csz_next.cohort != \"\") & (csz_next.cohort.str.count(\" \") == config.target_cohort_length - 1)]\n",
    "\n",
    "# removed constraint from below -- don't only include cohorts which cover all next units\n",
    "# .groupby(\"cohort\").filter(lambda xs: set(xs.next_unit) >= next_unit_set) \\\n",
    "\n",
    "expt_cohort = expt_cohort \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n",
    "expt_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for type-small cohorts -- cohorts which only have N of the phone set\n",
    "if config.target_cohort_length == 0:\n",
    "    expt_cohort_small = csz_next[csz_next.cohort == \"\"]\n",
    "else:\n",
    "    expt_cohort_small = csz_next[(csz_next.cohort != \"\") & (csz_next.cohort.str.count(\" \") == config.target_cohort_length - 1)]\n",
    "expt_cohort_small = expt_cohort_small \\\n",
    "    .groupby(\"cohort\").filter(lambda xs: len(set(xs.next_unit)) == config.target_small_cohort_size and set(xs.next_unit) <= next_unit_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n",
    "expt_cohort_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f08afd",
   "metadata": {},
   "source": [
    "### Prepare instance-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ceac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances = []\n",
    "\n",
    "# Sample at most this many combinations of cohort + next unit\n",
    "max_items_per_cohort_and_next_unit = 15\n",
    "\n",
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}\n",
    "for cohort, next_units in tqdm(expt_cohort.items(), total=len(expt_cohort)):\n",
    "    for unit in next_units:\n",
    "        if unit not in next_unit_set:\n",
    "            continue\n",
    "\n",
    "        inflected_phones = f\"{cohort} {unit}\" if cohort else unit\n",
    "        instances = cut_forms[cut_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_unit:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_unit).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "            print(cohort, unit, len(instances))\n",
    "\n",
    "        # equiv_key = (inflected_phones,)\n",
    "        # if equiv_key not in all_prediction_equivalences:\n",
    "        #     all_prediction_equivalences[equiv_key] = \\\n",
    "        #         analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms, cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": unit,\n",
    "\n",
    "                \"inflection\": unit,\n",
    "                \"next_unit_in_restricted_set\": unit in next_unit_set,\n",
    "\n",
    "                \"cohort_length\": config.target_cohort_length,\n",
    "                \"next_phoneme_idx\": config.target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort, next_phons in tqdm(expt_cohort_small.items(), total=len(expt_cohort_small)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_unit_set:\n",
    "            continue\n",
    "        inflected_phones = f\"{cohort} {phon}\" if cohort else phon\n",
    "        instances = cut_forms[cut_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_unit:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_unit).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "\n",
    "        # equiv_key = (inflected_phones,)\n",
    "        # if equiv_key not in all_prediction_equivalences:\n",
    "        #     all_prediction_equivalences[equiv_key] = \\\n",
    "        #         analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms,\n",
    "        #                                                              cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": f\"small-{phon}\",\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_unit_set,\n",
    "\n",
    "                \"cohort_length\": config.target_cohort_length,\n",
    "                \"next_phoneme_idx\": config.target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eab8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df = pd.DataFrame(all_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec.to_hdf5(f\"{output_dir}/state_space_spec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3820ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df.to_csv(f\"{output_dir}/instances.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
