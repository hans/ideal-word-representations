{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d30d2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from loguru import logger as L\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2de1395d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"ffff_32-pc-mAP1\"#discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "dataset = train_dataset\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "experiment = \"syllable_at_0\"\n",
    "\n",
    "output_dir = f\"outputs/analogy_pseudocausal_broad/inputs/{train_dataset}/w2v2_pc/{experiment}\"\n",
    "\n",
    "seed = 42\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    (\"mean_within_cut\", \"phoneme\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89fe704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OmegaConf from yaml with `experiment`\n",
    "config = OmegaConf.load(f\"conf/experiments/analogy_pseudocausal/{experiment}.yaml\")\n",
    "config.unit_level = \"syllable\"\n",
    "# DEV\n",
    "config.next_units = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe338f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3825872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(config.unit_level, level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b2cf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(cuts_df.description.iloc[0]) == tuple:\n",
    "    cuts_df[\"description\"] = cuts_df.description.apply(''.join)\n",
    "cut_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d73a6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "word_freq_df = word_freq_df.loc[~word_freq_df.index.duplicated()]\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq_rel\", \"TwitterFreq_rel\", \"NewsFreq_rel\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979df6",
   "metadata": {},
   "source": [
    "## Prepare cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05b3374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-13 15:48:37.828\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[33m\u001b[1mNext unit set is large, taking the top 100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if config.next_units is None or not config.next_units.strip():\n",
    "    all_next_units = cuts_df.description.value_counts()\n",
    "    if len(all_next_units) > 100:\n",
    "        L.warning(\"Next unit set is large, taking the top 100\")\n",
    "        all_next_units = all_next_units[:100]\n",
    "    next_unit_set = set(all_next_units.index)\n",
    "else:\n",
    "    next_unit_set = set(config.next_units.strip().split())\n",
    "assert config.target_small_cohort_size < len(next_unit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c19d5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf606990b124fab93b2b25ce5f1c651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohorts = defaultdict(set)\n",
    "for units in tqdm(cut_forms.unique()):\n",
    "    units = tuple(units.split())\n",
    "    for i in range(len(units) + 1):\n",
    "        cohorts[units[:i]].add(units)\n",
    "\n",
    "csz_next = pd.DataFrame([(\" \".join(coh), \" \".join(item), item[len(coh)]) for coh, items in cohorts.items()\n",
    "                            for item in items if len(item) > len(coh)],\n",
    "                            columns=[\"cohort\", \"item\", \"next_unit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a04fd1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1896260/1031661443.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cohort\n",
       "    [AA, AAB, AAD, AADZ, AAFT, AAG, AAK, AAKS, AAL...\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.target_cohort_length == 0:\n",
    "    expt_cohort = csz_next[csz_next.cohort == \"\"]\n",
    "else:\n",
    "    expt_cohort = csz_next[csz_next.cohort.str.count(\" \") == config.target_cohort_length - 1]\n",
    "\n",
    "# removed constraint from below -- don't only include cohorts which cover all next units\n",
    "# .groupby(\"cohort\").filter(lambda xs: set(xs.next_unit) >= next_unit_set) \\\n",
    "\n",
    "expt_cohort = expt_cohort \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n",
    "expt_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17f7d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1896260/2133935529.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>item</th>\n",
       "      <th>next_unit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cohort</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cohort, item, next_unit]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now search for type-small cohorts -- cohorts which only have N of the phone set\n",
    "if config.target_cohort_length == 0:\n",
    "    expt_cohort_small = csz_next[csz_next.cohort == \"\"]\n",
    "else:\n",
    "    expt_cohort_small = csz_next[csz_next.cohort.str.count(\" \") == config.target_cohort_length - 1]\n",
    "expt_cohort_small = expt_cohort_small \\\n",
    "    .groupby(\"cohort\").filter(lambda xs: len(set(xs.next_unit)) == config.target_small_cohort_size and set(xs.next_unit) <= next_unit_set) \\\n",
    "    .groupby(\"cohort\").apply(lambda xs: sorted(set(xs.next_unit)))\n",
    "expt_cohort_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f08afd",
   "metadata": {},
   "source": [
    "### Prepare instance-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e9ceac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4953d84fd5401fa413886ad2147fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AA 544\n",
      " AE 559\n",
      " AH 1441\n",
      " AHN 807\n",
      " AHS 100\n",
      " AY 690\n",
      " BAHL 16\n",
      " BER 126\n",
      " BIH 971\n",
      " BIY 725\n",
      " CHER 45\n",
      " DAH 326\n",
      " DAHN 130\n",
      " DER 329\n",
      " DIH 690\n",
      " DIHD 100\n",
      " DIY 218\n",
      " EH 713\n",
      " EHN 415\n",
      " ER 596\n",
      " EY 305\n",
      " FAH 244\n",
      " FAOR 558\n",
      " FER 591\n",
      " FIH 489\n",
      " HHAE 703\n",
      " IH 524\n",
      " IHK 861\n",
      " IHM 451\n",
      " IHN 904\n",
      " IHNG 255\n",
      " IY 570\n",
      " JHAH 199\n",
      " KAAN 356\n",
      " KAH 688\n",
      " KAHL 82\n",
      " KAHM 572\n",
      " KAHN 742\n",
      " KIHNG 180\n",
      " LAH 213\n",
      " LER 72\n",
      " LEY 605\n",
      " LIH 640\n",
      " LIHNG 84\n",
      " LIY 339\n",
      " LOW 342\n",
      " MAE 639\n",
      " MAH 483\n",
      " MAHN 35\n",
      " MEH 860\n",
      " MER 232\n",
      " MIH 604\n",
      " MIY 345\n",
      " NAH 265\n",
      " NER 109\n",
      " NEY 550\n",
      " NIH 120\n",
      " NIY 275\n",
      " OW 739\n",
      " PAA 514\n",
      " PAH 524\n",
      " PER 790\n",
      " PIY 468\n",
      " PRAH 394\n",
      " RAH 228\n",
      " REH 311\n",
      " RIH 646\n",
      " RIY 471\n",
      " SAH 751\n",
      " SAHN 390\n",
      " SAY 370\n",
      " SEH 397\n",
      " SER 728\n",
      " SIH 873\n",
      " SIHNG 279\n",
      " SIHZ 17\n",
      " SIY 589\n",
      " TAH 316\n",
      " TEH 373\n",
      " TER 243\n",
      " TEY 389\n",
      " TIH 117\n",
      " TIHNG 17\n",
      " TIY 265\n",
      " VER 215\n",
      " VIH 483\n",
      " WIH 579\n",
      " YUW 560\n"
     ]
    }
   ],
   "source": [
    "all_instances = []\n",
    "\n",
    "# Sample at most this many combinations of cohort + next unit\n",
    "max_items_per_cohort_and_next_unit = 15\n",
    "\n",
    "label2idx = {l: i for i, l in enumerate(state_space_spec.labels)}\n",
    "for cohort, next_units in tqdm(expt_cohort.items(), total=len(expt_cohort)):\n",
    "    for unit in next_units:\n",
    "        if unit not in next_unit_set:\n",
    "            continue\n",
    "\n",
    "        inflected_phones = f\"{cohort} {unit}\" if cohort else unit\n",
    "        instances = cut_forms[cut_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_unit:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_unit).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "            print(cohort, unit, len(instances))\n",
    "\n",
    "        # equiv_key = (inflected_phones,)\n",
    "        # if equiv_key not in all_prediction_equivalences:\n",
    "        #     all_prediction_equivalences[equiv_key] = \\\n",
    "        #         analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms, cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": unit,\n",
    "\n",
    "                \"inflection\": unit,\n",
    "                \"next_unit_in_restricted_set\": unit in next_unit_set,\n",
    "\n",
    "                \"cohort_length\": config.target_cohort_length,\n",
    "                \"next_phoneme_idx\": config.target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d8f9790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a5ffde42bb4b1aac081f8cf29688d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cohort, next_phons in tqdm(expt_cohort_small.items(), total=len(expt_cohort_small)):\n",
    "    for phon in next_phons:\n",
    "        if phon not in next_unit_set:\n",
    "            continue\n",
    "        inflected_phones = f\"{cohort} {phon}\" if cohort else phon\n",
    "        instances = cut_forms[cut_forms.str.match(f\"{inflected_phones}\\\\b\")].index\n",
    "\n",
    "        # Pick the top K labels with the highest frequency from the cohort.\n",
    "        coh_labels = instances.get_level_values(\"label\").str.replace(\"'s$\", \"\", regex=True)\n",
    "        if len(coh_labels) > max_items_per_cohort_and_next_unit:\n",
    "            label_freqs = word_freq_df.reindex(coh_labels.unique()).LogFreq.fillna(word_freq_df.LogFreq.min())\n",
    "            keep_labels = label_freqs.nlargest(max_items_per_cohort_and_next_unit).index\n",
    "            instances = instances[coh_labels.isin(keep_labels)]\n",
    "\n",
    "        # equiv_key = (inflected_phones,)\n",
    "        # if equiv_key not in all_prediction_equivalences:\n",
    "        #     all_prediction_equivalences[equiv_key] = \\\n",
    "        #         analogy_pseudocausal.prepare_prediction_equivalences(cuts_df, cut_phonemic_forms,\n",
    "        #                                                              cohort, phon)\n",
    "\n",
    "        for label, instance_idx in instances:\n",
    "            all_instances.append({\n",
    "                \"base_phones\": cohort,\n",
    "                \"inflected_phones\": inflected_phones,\n",
    "                \"post_divergence\": phon,\n",
    "\n",
    "                \"inflection\": f\"small-{phon}\",\n",
    "                \"next_phoneme_in_restricted_set\": phon in next_unit_set,\n",
    "\n",
    "                \"cohort_length\": config.target_cohort_length,\n",
    "                \"next_phoneme_idx\": config.target_cohort_length,\n",
    "\n",
    "                \"inflected\": label,\n",
    "                \"inflected_idx\": label2idx[label],\n",
    "                \"inflected_instance_idx\": instance_idx,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3eab8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df = pd.DataFrame(all_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3e1b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jgauthier/transformers/lib/python3.10/site-packages/tables/attributeset.py:295: DataTypeWarning: Unsupported type for attribute 'labels_are_repr' in node '/'. Offending HDF5 class: 8\n",
      "  value = self._g_getattr(self._v_node, name)\n",
      "/userdata/jgauthier/projects/ideal-word-representations/src/analysis/state_space.py:89: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['description'], dtype='object')]\n",
      "\n",
      "  self.cuts.to_hdf(path, key=cuts_key, mode=\"a\")\n"
     ]
    }
   ],
   "source": [
    "state_space_spec.to_hdf5(f\"{output_dir}/state_space_spec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3820ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances_df.to_csv(f\"{output_dir}/instances.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
