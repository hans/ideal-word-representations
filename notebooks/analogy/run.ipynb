{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from fastdist import fastdist\n",
    "import lemminflect\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "inflection_results_path = \"inflection_results.parquet\"\n",
    "all_cross_instances_path = \"all_cross_instances.parquet\"\n",
    "most_common_allomorphs_path = \"most_common_allomorphs.csv\"\n",
    "false_friends_path = \"false_friends.csv\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"state_space_spec.h5\"\n",
    "embeddings_path = \"ID\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_agg = prepare_state_trajectory(model_representations, state_space_spec, \n",
    "                                          agg_fn_spec=\"mean\", agg_fn_dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.read_parquet(all_cross_instances_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = pd.read_parquet(inflection_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_allomorphs = pd.read_csv(most_common_allomorphs_path)\n",
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general queries for all experiments to exclude special edge cases;\n",
    "# logic doesn't make sense in most experiments\n",
    "all_query = \"not exclude_main\"\n",
    "\n",
    "experiments = {\n",
    "    \"basic\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"regular\": {\n",
    "        \"group_by\": [\"inflection\", \"is_regular\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    # \"NNS_to_VBZ\": {\n",
    "    #     \"base_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    # },\n",
    "    # \"VBZ_to_NNS\": {\n",
    "    #     \"base_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    # },\n",
    "    \"regular_to_irregular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"not is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"irregular_to_regular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"not is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"nn_vb_ambiguous\": {\n",
    "        \"group_by\": [\"inflection\", \"base_ambig_NN_VB\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_NNS\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'NNS'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_VBZ\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'VBZ'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"false_friends\": {\n",
    "        \"all_query\": \"inflection.str.contains('FF')\",\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"equivalence_keys\": [\"base\", \"inflected\", \"post_divergence\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from each of top allomorphs in NNS, VBZ\n",
    "# to each other\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_unambiguous_transfer = [\"NNS\", \"VBZ\"]\n",
    "for infl1, infl2 in itertools.product(study_unambiguous_transfer, repeat=2):\n",
    "    for allomorph1 in transfer_allomorphs[infl1]:\n",
    "        for allomorph2 in transfer_allomorphs[infl2]:\n",
    "            experiments[f\"unambiguous-{infl1}_{allomorph1}_to_{infl2}_{allomorph2}\"] = {\n",
    "                \"base_query\": f\"inflection == '{infl1}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph1}'\",\n",
    "                \"inflected_query\": f\"inflection == '{infl2}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph2}'\",\n",
    "                \"all_query\": all_query,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from\n",
    "# 1. false friend allomorph to matching inflection allomorph\n",
    "# 2. false friend allomorph to non-matching inflection allomorph\n",
    "# 3. inflection allomorph to matching false friend allomorph\n",
    "# 4. inflection allomorph to non-matching false friend allomorph\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_false_friends = [\"NNS\", \"VBZ\"]\n",
    "for (inflection, post_divergence), _ in false_friends_df.groupby([\"inflection\", \"post_divergence\"]):\n",
    "    if inflection not in study_false_friends:\n",
    "        continue\n",
    "    for transfer_allomorph in transfer_allomorphs[inflection]:\n",
    "        experiments[f\"{inflection}-FF-{post_divergence}-to-{inflection}_{transfer_allomorph}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "        }\n",
    "        experiments[f\"{inflection}_{transfer_allomorph}-to-{inflection}-FF-{post_divergence}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "        }\n",
    "\n",
    "for inflection in study_false_friends:\n",
    "    for t1, t2 in itertools.combinations(transfer_allomorphs[inflection], 2):\n",
    "        experiments[f\"{inflection}-FF-{t1}-to-{inflection}-FF-{t2}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{t1}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{t2}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        num_samples=1000,\n",
    "        seed=seed,\n",
    "        device=\"cuda\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_label == experiment_results.gt_label\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regular_inflections = inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size().unstack().fillna(0)\n",
    "plot_regular_inflections = plot_regular_inflections[plot_regular_inflections.min(1) > 0]\n",
    "plot_regular_inflections = sorted(plot_regular_inflections.index) + [\"VBG\", \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, len(plot_regular_inflections), figsize=(3 * len(plot_regular_inflections), 2.5),\n",
    "                     squeeze=True)\n",
    "\n",
    "regular_df = experiment_results.loc[\"regular\"]\n",
    "regular_df = pd.concat([regular_df, pd.DataFrame(regular_df.group.tolist()).add_prefix(\"group\")], axis=1)\n",
    "regular_transfer_df = experiment_results.loc[\"regular_to_irregular\"]\n",
    "regular_transfer_df[\"group\"] = regular_transfer_df.group.str[0]\n",
    "irregular_transfer_df = experiment_results.loc[\"irregular_to_regular\"]\n",
    "irregular_transfer_df[\"group\"] = irregular_transfer_df.group.str[0]\n",
    "regular_results = {}\n",
    "for inflection in plot_regular_inflections:\n",
    "    regular_results[inflection] = np.array([\n",
    "        [regular_df.query(\"group0 == @inflection and group1 == True\").correct.mean(),\n",
    "         regular_transfer_df.query(\"group == @inflection\").correct.mean()],\n",
    "        [irregular_transfer_df.query(\"group == @inflection\").correct.mean(),\n",
    "         regular_df.query(\"group0 == @inflection and group1 == False\").correct.mean()],\n",
    "    ])\n",
    "\n",
    "vmin = min(v.min() for v in regular_results.values())\n",
    "vmax = max(v.max() for v in regular_results.values())\n",
    "for ax, inflection in zip(ax, plot_regular_inflections):\n",
    "    sns.heatmap(regular_results[inflection], annot=True, fmt=\".2f\", ax=ax,\n",
    "                vmin=vmin, vmax=vmax, cbar=True,\n",
    "                xticklabels=[\"Regular\", \"Irregular\"],\n",
    "                yticklabels=[\"Regular\", \"Irregular\"])\n",
    "    ax.set_title(inflection)\n",
    "    ax.set_xlabel(\"Test\")\n",
    "    ax.set_ylabel(\"Train\")\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root NN/VB ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_nnvb_results = []\n",
    "\n",
    "nnvb_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "nnvb_expts = nnvb_expts[nnvb_expts.str.contains(\"unambiguous-\")]\n",
    "\n",
    "for expt in nnvb_expts:\n",
    "    inflection_from, allomorph_from, inflection_to, allomorph_to = \\\n",
    "        re.findall(r\"unambiguous-(\\w+)_([\\w\\s]+)_to_(\\w+)_([\\w\\s]+)\", expt)[0]\n",
    "    expt_df = experiment_results.loc[expt].copy()\n",
    "\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "    # DEV\n",
    "    # if num_seen_words < 10:\n",
    "    #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "    #     continue\n",
    "\n",
    "    expt_df[\"inflection_from\"] = inflection_from\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"inflection_to\"] = inflection_to\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    agg_nnvb_results.append(expt_df)\n",
    "\n",
    "all_nnvb_results = pd.concat(agg_nnvb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_freq\"),\n",
    "                              left_on=\"base_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "all_nnvb_results[\"to_freq_bin\"] = pd.cut(all_nnvb_results.to_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_nnvb_results[\"from_freq_bin\"] = pd.cut(all_nnvb_results.from_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                                 \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary[\"source_label\"] = nnvb_results_summary.inflection_from + \" \" + nnvb_results_summary.allomorph_from\n",
    "nnvb_results_summary[\"target_label\"] = nnvb_results_summary.inflection_to + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "nnvb_results_summary[\"transfer_label\"] = nnvb_results_summary.inflection_from + \" -> \" + nnvb_results_summary.inflection_to\n",
    "nnvb_results_summary[\"phon_label\"] = nnvb_results_summary.allomorph_from + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "nnvb_results_summary[\"complement_exists\"] = nnvb_results_summary.apply(lambda row: len(nnvb_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "nnvb_results_summary = nnvb_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nnvb_results_summary.set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", hue=\"phon_label\", kind=\"swarm\", aspect=2)\n",
    "order = nnvb_results_summary.groupby(\"transfer_label\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", kind=\"box\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_label_rank.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Mean rank of GT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_distance.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Median distance to GT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friend_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "false_friend_expts = false_friend_expts[false_friend_expts.str.contains(\"FF\")]\n",
    "# false_friend_expts = false_friend_expts.tolist() + [\"false_friends\"]\n",
    "sorted(false_friend_expts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results = []\n",
    "\n",
    "for expt_name in false_friend_expts:\n",
    "    expt_df = experiment_results.loc[expt_name].copy()\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "\n",
    "    # DEV\n",
    "    # if num_seen_words < 10:\n",
    "    #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "    #     continue\n",
    "\n",
    "    if expt_name.count(\"-FF-\") == 2:\n",
    "        allomorph_from, allomorph_to = re.findall(r\"-FF-([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "        ff_from, ff_to = True, True\n",
    "    else:\n",
    "        try:\n",
    "            allomorph_from, allomorph_to = re.findall(r\"_([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "            # is the false friend on the \"from\" side?\n",
    "            ff_from, ff_to = False, True\n",
    "        except:\n",
    "            allomorph_from, allomorph_to = re.findall(r\".+FF-([\\w\\s]+)-to-.+_([\\w\\s]+)\", expt_name)[0]\n",
    "            ff_from, ff_to = True, False\n",
    "\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    if ff_from:\n",
    "        expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    if ff_to:\n",
    "        expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "    all_ff_results.append(expt_df)\n",
    "\n",
    "all_ff_results = pd.concat(all_ff_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_df = experiment_results.loc[\"false_friends\"].copy()\n",
    "expt_df[\"allomorph_from\"] = expt_df.inflection_from.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"allomorph_to\"] = expt_df.inflection_to.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "expt_df = expt_df[expt_df.inflection_from.isin(all_ff_results.inflection_from.unique())]\n",
    "\n",
    "all_ff_results = pd.concat([all_ff_results, expt_df])\n",
    "\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_freq\"), left_on=\"base_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_freq\"), left_on=\"base_to\", right_index=True)\n",
    "\n",
    "all_ff_results[\"from_freq_bin\"] = pd.qcut(all_ff_results.from_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_ff_results[\"to_freq_bin\"] = pd.qcut(all_ff_results.to_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\")].assign(inflection_from_base=lambda xs: xs.inflection_from.str.replace(\"-FF\", \"\")),\n",
    "         false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "            .rename(columns={\n",
    "                \"inflection\": \"inflection_from_base\",\n",
    "                \"base\": \"base_from\",\n",
    "                \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                \"strong\": \"is_strong_ff\"}),\n",
    "         on=[\"inflection_from_base\", \"base_from\"]).is_strong_ff\n",
    "\n",
    "all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\")]\n",
    "                .assign(inflection_to_base=lambda xs: xs.inflection_to.str.replace(\"-FF\", \"\"))\n",
    "                .drop(columns=[\"is_strong_ff\"]),\n",
    "            false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "                .rename(columns={\n",
    "                    \"inflection\": \"inflection_to_base\",\n",
    "                    \"base\": \"base_to\",\n",
    "                    \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                    \"strong\": \"is_strong_ff\"}),\n",
    "            on=[\"inflection_to_base\", \"base_to\"]).is_strong_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY STRONG\n",
    "all_ff_results = all_ff_results[all_ff_results.is_strong_ff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary = all_ff_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "        .correct.agg([\"count\", \"mean\"]) \\\n",
    "        .query(\"count >= 0\") \\\n",
    "        .reset_index()\n",
    "\n",
    "ff_results_summary[\"source_label\"] = ff_results_summary.inflection_from + \" \" + ff_results_summary.allomorph_from\n",
    "ff_results_summary[\"target_label\"] = ff_results_summary.inflection_to + \" \" + ff_results_summary.allomorph_to\n",
    "\n",
    "ff_results_summary[\"transfer_label\"] = ff_results_summary.inflection_from + \" -> \" + ff_results_summary.inflection_to\n",
    "ff_results_summary[\"phon_label\"] = ff_results_summary.allomorph_from + \" \" + ff_results_summary.allomorph_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FF1->FF2 results, fill out the upper triangle\n",
    "extra_rows = []\n",
    "for _, row in ff_results_summary[ff_results_summary.inflection_from.str.contains(\"-FF\") & ff_results_summary.inflection_to.str.contains(\"-FF\")].iterrows():\n",
    "    if row.allomorph_to == row.allomorph_from:\n",
    "        continue\n",
    "    extra_rows.append(row.copy())\n",
    "    extra_rows[-1].inflection_from, extra_rows[-1].inflection_to = extra_rows[-1].inflection_to, extra_rows[-1].inflection_from\n",
    "    extra_rows[-1].allomorph_from, extra_rows[-1].allomorph_to = extra_rows[-1].allomorph_to, extra_rows[-1].allomorph_from\n",
    "    extra_rows[-1].source_label, extra_rows[-1].target_label = extra_rows[-1].target_label, extra_rows[-1].source_label\n",
    "    extra_rows[-1].transfer_label = extra_rows[-1].transfer_label.replace(\" -> \", \" <- \")\n",
    "\n",
    "ff_results_summary = pd.concat([ff_results_summary, pd.DataFrame(extra_rows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pd.concat([nnvb_results_summary, ff_results_summary]).set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary[\"ff_allomorph\"] = ff_results_summary.apply(\n",
    "    lambda row: row.allomorph_from if row.inflection_from.endswith(\"-FF\") else row.allomorph_to, axis=1)\n",
    "ff_results_summary[\"inflection_base\"] = ff_results_summary.inflection_from.str.replace(\"-FF\", \"\")\n",
    "ff_results_summary[\"ff_direction\"] = ff_results_summary.inflection_from.str.contains(\"-FF\").map({True: \"from\", False: \"to\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ff_results_summary.groupby(\"ff_allomorph\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "g = sns.catplot(data=ff_results_summary, x=\"ff_allomorph\", y=\"mean\", hue=\"inflection_base\", col=\"ff_direction\",\n",
    "                order=order, kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = sns.catplot(data=nnvb_results_summary.query(\"inflection_from == inflection_to\"),\n",
    "                 x=\"allomorph_from\", y=\"mean\", hue=\"inflection_from\", kind=\"point\")\n",
    "# g2.axes.flat[0].set_ylim(g.axes.flat[0].get_ylim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2 = all_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_results_summary2 = pd.concat([ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.to_csv(f\"{output_dir}/all_ff_results.csv\")\n",
    "all_nnvb_results.to_csv(f\"{output_dir}/all_nnvb_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary.to_csv(f\"{output_dir}/ff_results_summary.csv\")\n",
    "nnvb_results_summary.to_csv(f\"{output_dir}/nnvb_results_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
