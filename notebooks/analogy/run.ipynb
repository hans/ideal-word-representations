{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "\n",
    "model_class = \"discrim-rnn_32-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "inflection_results_path = \"inflection_results.parquet\"\n",
    "all_cross_instances_path = \"all_cross_instances.parquet\"\n",
    "most_common_allomorphs_path = \"most_common_allomorphs.csv\"\n",
    "false_friends_path = \"false_friends.csv\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"state_space_spec.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jgauthier/transformers/lib/python3.10/site-packages/tables/attributeset.py:295: DataTypeWarning: Unsupported type for attribute 'labels_are_repr' in node '/'. Offending HDF5 class: 8\n",
      "  value = self._g_getattr(self._v_node, name)\n"
     ]
    }
   ],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deea8ad32e3b4060abb211a847382674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory_agg = prepare_state_trajectory(model_representations, state_space_spec, \n",
    "                                          agg_fn_spec=\"mean\", agg_fn_dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.read_parquet(all_cross_instances_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = pd.read_parquet(inflection_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'most_common_allomorphs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m most_common_allomorphs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmost_common_allomorphs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m false_friends_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(false_friends_path)\n",
      "File \u001b[0;32m/scratch/jgauthier/transformers/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jgauthier/transformers/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/scratch/jgauthier/transformers/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jgauthier/transformers/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/scratch/jgauthier/transformers/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'most_common_allomorphs.csv'"
     ]
    }
   ],
   "source": [
    "most_common_allomorphs = pd.read_csv(most_common_allomorphs_path)\n",
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general queries for all experiments to exclude special edge cases;\n",
    "# logic doesn't make sense in most experiments\n",
    "all_query = \"not exclude_main\"\n",
    "\n",
    "experiments = {\n",
    "    \"basic\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"regular\": {\n",
    "        \"group_by\": [\"inflection\", \"is_regular\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    # \"NNS_to_VBZ\": {\n",
    "    #     \"base_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    # },\n",
    "    # \"VBZ_to_NNS\": {\n",
    "    #     \"base_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    # },\n",
    "    \"regular_to_irregular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"not is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"irregular_to_regular\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"base_query\": \"not is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"nn_vb_ambiguous\": {\n",
    "        \"group_by\": [\"inflection\", \"base_ambig_NN_VB\"],\n",
    "        \"base_query\": \"is_regular\",\n",
    "        \"inflected_query\": \"is_regular\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_NNS\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'NNS'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_VBZ\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'VBZ'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"false_friends\": {\n",
    "        \"all_query\": \"inflection.str.contains('FF')\",\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"equivalence_keys\": [\"base\", \"inflected\", \"post_divergence\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from each of top allomorphs in NNS, VBZ\n",
    "# to each other\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_unambiguous_transfer = [\"NNS\", \"VBZ\"]\n",
    "for infl1, infl2 in itertools.product(study_unambiguous_transfer, repeat=2):\n",
    "    for allomorph1 in transfer_allomorphs[infl1]:\n",
    "        for allomorph2 in transfer_allomorphs[infl2]:\n",
    "            experiments[f\"unambiguous-{infl1}_{allomorph1}_to_{infl2}_{allomorph2}\"] = {\n",
    "                \"base_query\": f\"inflection == '{infl1}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph1}'\",\n",
    "                \"inflected_query\": f\"inflection == '{infl2}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{allomorph2}'\",\n",
    "                \"all_query\": all_query,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from\n",
    "# 1. false friend allomorph to matching inflection allomorph\n",
    "# 2. false friend allomorph to non-matching inflection allomorph\n",
    "# 3. inflection allomorph to matching false friend allomorph\n",
    "# 4. inflection allomorph to non-matching false friend allomorph\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()\n",
    "study_false_friends = [\"NNS\", \"VBZ\"]\n",
    "for (inflection, post_divergence), _ in false_friends_df.groupby([\"inflection\", \"post_divergence\"]):\n",
    "    if inflection not in study_false_friends:\n",
    "        continue\n",
    "    for transfer_allomorph in transfer_allomorphs[inflection]:\n",
    "        experiments[f\"{inflection}-FF-{post_divergence}-to-{inflection}_{transfer_allomorph}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "        }\n",
    "        experiments[f\"{inflection}_{transfer_allomorph}-to-{inflection}-FF-{post_divergence}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}' and is_regular and base_ambig_NN_VB == False and post_divergence == '{transfer_allomorph}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "        }\n",
    "\n",
    "for inflection in study_false_friends:\n",
    "    for t1, t2 in itertools.combinations(transfer_allomorphs[inflection], 2):\n",
    "        experiments[f\"{inflection}-FF-{t1}-to-{inflection}-FF-{t2}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{t1}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{t2}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments for forced-choice analysis\n",
    "\n",
    "fc_types = [infl for infl in all_cross_instances.inflection.unique() if infl.startswith(\"FC\")]\n",
    "fc_types = set(re.findall(r\"FC-([\\w\\s]+)_([\\w\\s]+)\", infl)[0] for infl in fc_types)\n",
    "\n",
    "for phoneme_pair in fc_types:\n",
    "    for p1, p2 in itertools.permutations(phoneme_pair, 2):\n",
    "        experiments[f\"FC-{'_'.join(phoneme_pair)}-{p1}-to-FC-{'_'.join(phoneme_pair)}-{p2}\"] = {\n",
    "            \"base_query\": f\"inflection == 'FC-{'_'.join(phoneme_pair)}-{p1}'\", \n",
    "            \"inflected_query\": f\"inflection == 'FC-{'_'.join(phoneme_pair)}-{p2}'\",\n",
    "        }\n",
    "    for p in phoneme_pair:\n",
    "        experiments[f\"FC-{'_'.join(phoneme_pair)}-{p}-to-FC-{'_'.join(phoneme_pair)}-{p}\"] = {\n",
    "            \"base_query\": f\"inflection == 'FC-{'_'.join(phoneme_pair)}-{p}'\", \n",
    "            \"inflected_query\": f\"inflection == 'FC-{'_'.join(phoneme_pair)}-{p}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC-Z_S-Z-to-FC-Z_S-Z\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b4119684249549aa5f69369e5e98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2fb0354ea645de8bfa4fedd632f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# experiment, config = \"FC-Z_S-Z-to-FC-Z_S-Z\", experiments[f\"FC-Z_S-Z-to-FC-Z_S-Z\"]\n",
    "# ret = analogy.run_experiment_equiv_level(\n",
    "#     experiment, config, state_space_spec, all_cross_instances,\n",
    "#     agg, agg_src,\n",
    "#     num_samples=2,\n",
    "#     device=\"cpu\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        num_samples=1000,\n",
    "        seed=seed,\n",
    "        device=\"cuda\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_label == experiment_results.gt_label\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regular_inflections = inflection_results_df.groupby([\"inflection\", \"is_regular\"]).size().unstack().fillna(0)\n",
    "plot_regular_inflections = plot_regular_inflections[plot_regular_inflections.min(1) > 0]\n",
    "plot_regular_inflections = sorted(plot_regular_inflections.index) + [\"VBG\", \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, len(plot_regular_inflections), figsize=(3 * len(plot_regular_inflections), 2.5),\n",
    "                     squeeze=True)\n",
    "\n",
    "regular_df = experiment_results.loc[\"regular\"]\n",
    "regular_df = pd.concat([regular_df, pd.DataFrame(regular_df.group.tolist()).add_prefix(\"group\")], axis=1)\n",
    "regular_transfer_df = experiment_results.loc[\"regular_to_irregular\"]\n",
    "regular_transfer_df[\"group\"] = regular_transfer_df.group.str[0]\n",
    "irregular_transfer_df = experiment_results.loc[\"irregular_to_regular\"]\n",
    "irregular_transfer_df[\"group\"] = irregular_transfer_df.group.str[0]\n",
    "regular_results = {}\n",
    "for inflection in plot_regular_inflections:\n",
    "    regular_results[inflection] = np.array([\n",
    "        [regular_df.query(\"group0 == @inflection and group1 == True\").correct.mean(),\n",
    "         regular_transfer_df.query(\"group == @inflection\").correct.mean()],\n",
    "        [irregular_transfer_df.query(\"group == @inflection\").correct.mean(),\n",
    "         regular_df.query(\"group0 == @inflection and group1 == False\").correct.mean()],\n",
    "    ])\n",
    "\n",
    "vmin = min(v.min() for v in regular_results.values())\n",
    "vmax = max(v.max() for v in regular_results.values())\n",
    "for ax, inflection in zip(ax, plot_regular_inflections):\n",
    "    sns.heatmap(regular_results[inflection], annot=True, fmt=\".2f\", ax=ax,\n",
    "                vmin=vmin, vmax=vmax, cbar=True,\n",
    "                xticklabels=[\"Regular\", \"Irregular\"],\n",
    "                yticklabels=[\"Regular\", \"Irregular\"])\n",
    "    ax.set_title(inflection)\n",
    "    ax.set_xlabel(\"Test\")\n",
    "    ax.set_ylabel(\"Train\")\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root NN/VB ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df.groupby([\"inflection\", \"base_ambig_NN_VB\"]).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_nnvb_results = []\n",
    "\n",
    "nnvb_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "nnvb_expts = nnvb_expts[nnvb_expts.str.contains(\"unambiguous-\")]\n",
    "\n",
    "for expt in nnvb_expts:\n",
    "    inflection_from, allomorph_from, inflection_to, allomorph_to = \\\n",
    "        re.findall(r\"unambiguous-(\\w+)_([\\w\\s]+)_to_(\\w+)_([\\w\\s]+)\", expt)[0]\n",
    "    expt_df = experiment_results.loc[expt].copy()\n",
    "\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "    # DEV\n",
    "    # if num_seen_words < 10:\n",
    "    #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "    #     continue\n",
    "\n",
    "    expt_df[\"inflection_from\"] = inflection_from\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"inflection_to\"] = inflection_to\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    agg_nnvb_results.append(expt_df)\n",
    "\n",
    "all_nnvb_results = pd.concat(agg_nnvb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"from_freq\"),\n",
    "                              left_on=\"base_from\", right_index=True)\n",
    "all_nnvb_results = pd.merge(all_nnvb_results, word_freq_df.LogFreq.rename(\"to_freq\"),\n",
    "                              left_on=\"base_to\", right_index=True)\n",
    "all_nnvb_results[\"to_freq_bin\"] = pd.cut(all_nnvb_results.to_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_nnvb_results[\"from_freq_bin\"] = pd.cut(all_nnvb_results.from_freq, bins=5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                                 \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "    .correct.agg([\"count\", \"mean\"]) \\\n",
    "    .query(\"count >= 0\") \\\n",
    "    .reset_index()\n",
    "\n",
    "nnvb_results_summary[\"source_label\"] = nnvb_results_summary.inflection_from + \" \" + nnvb_results_summary.allomorph_from\n",
    "nnvb_results_summary[\"target_label\"] = nnvb_results_summary.inflection_to + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "nnvb_results_summary[\"transfer_label\"] = nnvb_results_summary.inflection_from + \" -> \" + nnvb_results_summary.inflection_to\n",
    "nnvb_results_summary[\"phon_label\"] = nnvb_results_summary.allomorph_from + \" \" + nnvb_results_summary.allomorph_to\n",
    "\n",
    "# only retain cases where we have data in both transfer directions from source <-> target within inflection\n",
    "nnvb_results_summary[\"complement_exists\"] = nnvb_results_summary.apply(lambda row: len(nnvb_results_summary.query(\"source_label == @row.target_label and target_label == @row.source_label\")), axis=1)\n",
    "nnvb_results_summary = nnvb_results_summary.query(\"complement_exists > 0\").drop(columns=[\"complement_exists\"])\n",
    "\n",
    "nnvb_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nnvb_results_summary.set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", hue=\"phon_label\", kind=\"swarm\", aspect=2)\n",
    "order = nnvb_results_summary.groupby(\"transfer_label\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "sns.catplot(data=nnvb_results_summary, x=\"transfer_label\", y=\"mean\", kind=\"box\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnvb_results_summary2 = all_nnvb_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "nnvb_results_summary2[\"transfer_label\"] = nnvb_results_summary2.inflection_from + \" -> \" + nnvb_results_summary2.inflection_to\n",
    "nnvb_results_summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_label_rank.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Mean rank of GT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(nnvb_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).gt_distance.unstack(\"inflection_to\"),\n",
    "            annot=True)\n",
    "ax.set_xlabel(\"Test\")\n",
    "ax.set_ylabel(\"Train\")\n",
    "ax.set_title(\"Median distance to GT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_friend_expts = experiment_results.index.get_level_values(\"experiment\").unique()\n",
    "false_friend_expts = false_friend_expts[false_friend_expts.str.contains(\"FF\")]\n",
    "# false_friend_expts = false_friend_expts.tolist() + [\"false_friends\"]\n",
    "sorted(false_friend_expts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results = []\n",
    "\n",
    "for expt_name in false_friend_expts:\n",
    "    expt_df = experiment_results.loc[expt_name].copy()\n",
    "    num_seen_words = min(len(expt_df.base_from.unique()), len(expt_df.base_to.unique()))\n",
    "\n",
    "    # DEV\n",
    "    # if num_seen_words < 10:\n",
    "    #     print(f\"Skipping {expt} due to only {num_seen_words} seen words\")\n",
    "    #     continue\n",
    "\n",
    "    if expt_name.count(\"-FF-\") == 2:\n",
    "        allomorph_from, allomorph_to = re.findall(r\"-FF-([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "        ff_from, ff_to = True, True\n",
    "    else:\n",
    "        try:\n",
    "            allomorph_from, allomorph_to = re.findall(r\"_([\\w\\s]+)-to-.+FF-([\\w\\s]+)\", expt_name)[0]\n",
    "            # is the false friend on the \"from\" side?\n",
    "            ff_from, ff_to = False, True\n",
    "        except:\n",
    "            allomorph_from, allomorph_to = re.findall(r\".+FF-([\\w\\s]+)-to-.+_([\\w\\s]+)\", expt_name)[0]\n",
    "            ff_from, ff_to = True, False\n",
    "\n",
    "    expt_df[\"allomorph_from\"] = allomorph_from\n",
    "    expt_df[\"allomorph_to\"] = allomorph_to\n",
    "\n",
    "    if ff_from:\n",
    "        expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "    if ff_to:\n",
    "        expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "    all_ff_results.append(expt_df)\n",
    "\n",
    "all_ff_results = pd.concat(all_ff_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_df = experiment_results.loc[\"false_friends\"].copy()\n",
    "expt_df[\"allomorph_from\"] = expt_df.inflection_from.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"allomorph_to\"] = expt_df.inflection_to.str.extract(r\"FF-(.+)$\")\n",
    "expt_df[\"inflection_from\"] = expt_df.inflection_from.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "expt_df[\"inflection_to\"] = expt_df.inflection_to.str.replace(\"-FF-.+\", \"-FF\", regex=True)\n",
    "\n",
    "expt_df = expt_df[expt_df.inflection_from.isin(all_ff_results.inflection_from.unique())]\n",
    "\n",
    "all_ff_results = pd.concat([all_ff_results, expt_df])\n",
    "\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"from_freq\"), left_on=\"base_from\", right_index=True)\n",
    "all_ff_results = pd.merge(all_ff_results, word_freq_df[\"LogFreq\"].rename(\"to_freq\"), left_on=\"base_to\", right_index=True)\n",
    "\n",
    "all_ff_results[\"from_freq_bin\"] = pd.qcut(all_ff_results.from_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])\n",
    "all_ff_results[\"to_freq_bin\"] = pd.qcut(all_ff_results.to_freq, 5, labels=[f\"Q{i}\" for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_from.str.contains(\"-FF\")].assign(inflection_from_base=lambda xs: xs.inflection_from.str.replace(\"-FF\", \"\")),\n",
    "         false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "            .rename(columns={\n",
    "                \"inflection\": \"inflection_from_base\",\n",
    "                \"base\": \"base_from\",\n",
    "                \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                \"strong\": \"is_strong_ff\"}),\n",
    "         on=[\"inflection_from_base\", \"base_from\"]).is_strong_ff\n",
    "\n",
    "all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\"), \"is_strong_ff\"] = \\\n",
    "    pd.merge(all_ff_results.loc[all_ff_results.inflection_to.str.contains(\"-FF\")]\n",
    "                .assign(inflection_to_base=lambda xs: xs.inflection_to.str.replace(\"-FF\", \"\"))\n",
    "                .drop(columns=[\"is_strong_ff\"]),\n",
    "            false_friends_df.reset_index()[[\"inflection\", \"base\", \"strong_expected\", \"strong\"]]\n",
    "                .rename(columns={\n",
    "                    \"inflection\": \"inflection_to_base\",\n",
    "                    \"base\": \"base_to\",\n",
    "                    \"strong_expected\": \"strong_ff_allomorph\",\n",
    "                    \"strong\": \"is_strong_ff\"}),\n",
    "            on=[\"inflection_to_base\", \"base_to\"]).is_strong_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY STRONG\n",
    "all_ff_results = all_ff_results[all_ff_results.is_strong_ff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary = all_ff_results.groupby([\"inflection_from\", \"inflection_to\",\n",
    "                                             \"allomorph_from\", \"allomorph_to\"]) \\\n",
    "        .correct.agg([\"count\", \"mean\"]) \\\n",
    "        .query(\"count >= 0\") \\\n",
    "        .reset_index()\n",
    "\n",
    "ff_results_summary[\"source_label\"] = ff_results_summary.inflection_from + \" \" + ff_results_summary.allomorph_from\n",
    "ff_results_summary[\"target_label\"] = ff_results_summary.inflection_to + \" \" + ff_results_summary.allomorph_to\n",
    "\n",
    "ff_results_summary[\"transfer_label\"] = ff_results_summary.inflection_from + \" -> \" + ff_results_summary.inflection_to\n",
    "ff_results_summary[\"phon_label\"] = ff_results_summary.allomorph_from + \" \" + ff_results_summary.allomorph_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FF1->FF2 results, fill out the upper triangle\n",
    "extra_rows = []\n",
    "for _, row in ff_results_summary[ff_results_summary.inflection_from.str.contains(\"-FF\") & ff_results_summary.inflection_to.str.contains(\"-FF\")].iterrows():\n",
    "    if row.allomorph_to == row.allomorph_from:\n",
    "        continue\n",
    "    extra_rows.append(row.copy())\n",
    "    extra_rows[-1].inflection_from, extra_rows[-1].inflection_to = extra_rows[-1].inflection_to, extra_rows[-1].inflection_from\n",
    "    extra_rows[-1].allomorph_from, extra_rows[-1].allomorph_to = extra_rows[-1].allomorph_to, extra_rows[-1].allomorph_from\n",
    "    extra_rows[-1].source_label, extra_rows[-1].target_label = extra_rows[-1].target_label, extra_rows[-1].source_label\n",
    "    extra_rows[-1].transfer_label = extra_rows[-1].transfer_label.replace(\" -> \", \" <- \")\n",
    "\n",
    "ff_results_summary = pd.concat([ff_results_summary, pd.DataFrame(extra_rows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pd.concat([nnvb_results_summary, ff_results_summary]).set_index([\"source_label\", \"target_label\"]).sort_index()[\"mean\"].unstack(\"target_label\"), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary[\"ff_allomorph\"] = ff_results_summary.apply(\n",
    "    lambda row: row.allomorph_from if row.inflection_from.endswith(\"-FF\") else row.allomorph_to, axis=1)\n",
    "ff_results_summary[\"inflection_base\"] = ff_results_summary.inflection_from.str.replace(\"-FF\", \"\")\n",
    "ff_results_summary[\"ff_direction\"] = ff_results_summary.inflection_from.str.contains(\"-FF\").map({True: \"from\", False: \"to\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ff_results_summary.groupby(\"ff_allomorph\")[\"mean\"].mean().sort_values(ascending=False).index\n",
    "g = sns.catplot(data=ff_results_summary, x=\"ff_allomorph\", y=\"mean\", hue=\"inflection_base\", col=\"ff_direction\",\n",
    "                order=order, kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = sns.catplot(data=nnvb_results_summary.query(\"inflection_from == inflection_to\"),\n",
    "                 x=\"allomorph_from\", y=\"mean\", hue=\"inflection_from\", kind=\"point\")\n",
    "# g2.axes.flat[0].set_ylim(g.axes.flat[0].get_ylim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary2 = all_ff_results.groupby([\"inflection_from\", \"inflection_to\"]) \\\n",
    "    [[\"correct\", \"gt_label_rank\", \"gt_distance\"]].mean().reset_index()\n",
    "\n",
    "ff_results_summary2[\"transfer_label\"] = ff_results_summary2.inflection_from + \" -> \" + ff_results_summary2.inflection_to\n",
    "\n",
    "# add in data for NNS->NNS and VBZ->VBZ\n",
    "ff_results_summary2 = pd.concat([ff_results_summary2, nnvb_results_summary2.query(\"inflection_from == inflection_to\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ff_results_summary2.set_index([\"inflection_from\", \"inflection_to\"]).correct.unstack(\"inflection_to\"),\n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results[\"transfer_label\"] = all_ff_results.inflection_from + \" -> \" + all_ff_results.inflection_to\n",
    "g = sns.catplot(data=all_ff_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\", \n",
    "                col=\"transfer_label\", col_wrap=2,\n",
    "                height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    col_name = ax.get_title().replace(\"transfer_label = \", \"\")  # Extract facet label\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_ff_results[all_ff_results[\"transfer_label\"] == col_name]\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"from_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"from_freq_bin\").apply(lambda xs: xs.base_from.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of source word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=all_nnvb_results, x=\"to_freq_bin\", y=\"correct\", kind=\"point\",\n",
    "                row=\"inflection_from\", col=\"inflection_to\", height=3, aspect=1.5)\n",
    "\n",
    "# Add twin axes and histograms\n",
    "for ax in g.axes.flat:\n",
    "    twin_ax = ax.twinx()  # Create a twin y-axis\n",
    "    inflection_from, inflection_to = re.findall(r\"inflection_from = (.+) \\| inflection_to = (.+)\", ax.get_title())[0]\n",
    "\n",
    "    # Get subset of data for this facet\n",
    "    subset = all_nnvb_results.query(\"inflection_from == @inflection_from and inflection_to == @inflection_to\")\n",
    "\n",
    "    # Plot histogram on twin axis\n",
    "    sns.barplot(data=subset.groupby(\"to_freq_bin\").apply(lambda xs: xs.base_to.nunique()),\n",
    "                ax=twin_ax, alpha=0.3, color=\"blue\")\n",
    "\n",
    "    twin_ax.set_ylabel(\"Count\")  # Label twin axis\n",
    "    twin_ax.grid(False)  # Remove extra gridlines for clarity\n",
    "    for spine in twin_ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "g.figure.suptitle(\"Frequency effect of target word\")\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ff_results.to_csv(f\"{output_dir}/all_ff_results.csv\")\n",
    "all_nnvb_results.to_csv(f\"{output_dir}/all_nnvb_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_results_summary.to_csv(f\"{output_dir}/ff_results_summary.csv\")\n",
    "nnvb_results_summary.to_csv(f\"{output_dir}/nnvb_results_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
