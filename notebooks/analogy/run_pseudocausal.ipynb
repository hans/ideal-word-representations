{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.analysis import analogy\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechHiddenStateDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_pc_8\"\n",
    "\n",
    "model_class = \"discrim-rnn_32-pc-mAP1\"\n",
    "model_name = \"word_broad_10frames_fixedlen25\"\n",
    "\n",
    "inflection_results_path = \"inflection_results.parquet\"\n",
    "all_cross_instances_path = \"all_cross_instances.parquet\"\n",
    "most_common_allomorphs_path = \"most_common_allomorphs.csv\"\n",
    "false_friends_path = \"false_friends.csv\"\n",
    "\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "# hidden_states_path = f\"outputs/hidden_states/{base_model}/{train_dataset}.h5\"\n",
    "hidden_states_path = f\"/scratch/jgauthier/{base_model}_{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/analogy/inputs/{train_dataset}/w2v2_pc/state_space_spec.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}/{train_dataset}.npy\"\n",
    "\n",
    "output_dir = f\".\"\n",
    "\n",
    "pos_counts_path = \"data/pos_counts.pkl\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    (\"mean_within_cut\", \"phoneme\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general queries for all experiments to exclude special edge cases;\n",
    "# logic doesn't make sense in most experiments\n",
    "all_query = \"not exclude_main\"\n",
    "\n",
    "experiments = {\n",
    "    \"basic\": {\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"regular\": {\n",
    "        \"group_by\": [\"inflection\", \"is_regular\"],\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    # \"NNS_to_VBZ\": {\n",
    "    #     \"base_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    # },\n",
    "    # \"VBZ_to_NNS\": {\n",
    "    #     \"base_query\": \"inflection == 'VBZ' and is_regular\",\n",
    "    #     \"inflected_query\": \"inflection == 'NNS' and is_regular\",\n",
    "    # },\n",
    "    # \"regular_to_irregular\": {\n",
    "    #     \"group_by\": [\"inflection\"],\n",
    "    #     \"base_query\": \"is_regular == True\",\n",
    "    #     \"inflected_query\": \"is_regular == False\",\n",
    "    #     \"all_query\": all_query,\n",
    "    # },\n",
    "    # \"irregular_to_regular\": {\n",
    "    #     \"group_by\": [\"inflection\"],\n",
    "    #     \"base_query\": \"is_regular == False\",\n",
    "    #     \"inflected_query\": \"is_regular == True\",\n",
    "    #     \"all_query\": all_query,\n",
    "    # },\n",
    "    \"nn_vb_ambiguous\": {\n",
    "        \"group_by\": [\"inflection\", \"base_ambig_NN_VB\"],\n",
    "        \"base_query\": \"is_regular == True\",\n",
    "        \"inflected_query\": \"is_regular == True\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_NNS\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'NNS'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"random_to_VBZ\": {\n",
    "        \"base_query\": \"inflection == 'random'\",\n",
    "        \"inflected_query\": \"inflection == 'VBZ'\",\n",
    "        \"all_query\": all_query,\n",
    "    },\n",
    "    \"false_friends\": {\n",
    "        \"all_query\": \"inflection.str.contains('FF')\",\n",
    "        \"group_by\": [\"inflection\"],\n",
    "        \"equivalence_keys\": [\"base\", \"inflected\", \"post_divergence\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO document\n",
    "study_unambiguous_transfer = [\"NNS\", \"VBZ\"]\n",
    "study_false_friends = [\"NNS\", \"VBZ\", \"VBD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_experiments = {\n",
    "    (\"Z\", \"S\"): {\n",
    "        \"source_inflections\": [\"VBZ\", \"NNS\"],\n",
    "    },\n",
    "    (\"D\", \"T\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "    (\"D\", \"IH D\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "    (\"T\", \"IH D\"): {\n",
    "        \"source_inflections\": [\"VBD\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings_path == \"ID\":\n",
    "    model_representations = SpeechHiddenStateDataset.from_hdf5(hidden_states_path).states\n",
    "else:\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path)\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec)\n",
    "trajectory = aggregate_state_trajectory(trajectory, state_space_spec, agg_fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg, agg_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label\", \"instance_idx\", \"frame_idx\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_phonemic_forms = cuts_df.groupby([\"label\", \"instance_idx\"]).description.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/WorldLex_Eng_US.Freq.2.txt\", sep=\"\\t\", index_col=\"Word\")\n",
    "# compute weighted average frequency across domains\n",
    "word_freq_df[\"BlogFreq_rel\"] = word_freq_df.BlogFreq / word_freq_df.BlogFreq.sum()\n",
    "word_freq_df[\"TwitterFreq_rel\"] = word_freq_df.TwitterFreq / word_freq_df.TwitterFreq.sum()\n",
    "word_freq_df[\"NewsFreq_rel\"] = word_freq_df.NewsFreq / word_freq_df.NewsFreq.sum()\n",
    "word_freq_df[\"Freq\"] = word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].mean(axis=1) \\\n",
    "    * word_freq_df[[\"BlogFreq\", \"TwitterFreq\", \"NewsFreq\"]].sum().mean()\n",
    "word_freq_df[\"LogFreq\"] = np.log10(word_freq_df.Freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cross_instances = pd.read_parquet(all_cross_instances_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_results_df = pd.read_parquet(inflection_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_allomorphs = pd.read_csv(most_common_allomorphs_path)\n",
    "false_friends_df = pd.read_csv(false_friends_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inclusions / exclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include homophones of the target as valid predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "pron2label = defaultdict(set)\n",
    "for label, rows in cut_phonemic_forms.groupby(\"label\"):\n",
    "    for pron in set(rows):\n",
    "        pron2label[pron].add(label)\n",
    "\n",
    "homophone_map = defaultdict(set)\n",
    "for label_idx, label in enumerate(state_space_spec.labels):\n",
    "    for pron in set(cut_phonemic_forms.loc[label]):\n",
    "        homophone_map[label] |= pron2label[pron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to include predictions of homophones from analogy evaluations.\n",
    "# create a map from inflected label idx -> all label idxs which should be ignored.\n",
    "include_inflected_map = {state_space_spec.labels.index(label): {state_space_spec.labels.index(hom) for hom in homs}\n",
    "                         for label, homs in homophone_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude the base and all homophones as a valid prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to exclude base and any homophones from analogy evaluations\n",
    "# create a map from inflection + inflected label idx -> all label idxs which should be ignored.\n",
    "exclude_inflected_map = {}\n",
    "for (inflection, base, inflected, base_idx, inflected_idx), _ in all_cross_instances.groupby([\"inflection\", \"base\", \"inflected\", \"base_idx\", \"inflected_idx\"]):\n",
    "    exclude_inflected_map[inflection, inflected_idx] = {state_space_spec.labels.index(hom) for hom in homophone_map[base]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 3 most frequent allomorphs of each inflection\n",
    "transfer_allomorphs = most_common_allomorphs.groupby(\"inflection\").most_common_allomorph.apply(lambda xs: xs.value_counts().head(3).index.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from each of top allomorphs in NNS, VBZ\n",
    "# to each other\n",
    "for infl1, infl2 in itertools.product(study_unambiguous_transfer, repeat=2):\n",
    "    for allomorph1 in transfer_allomorphs[infl1]:\n",
    "        for allomorph2 in transfer_allomorphs[infl2]:\n",
    "            experiments[f\"unambiguous-{infl1}_{allomorph1}_to_{infl2}_{allomorph2}\"] = {\n",
    "                \"base_query\": f\"inflection == '{infl1}' and is_regular == True and base_ambig_NN_VB == False and post_divergence == '{allomorph1}'\",\n",
    "                \"inflected_query\": f\"inflection == '{infl2}' and is_regular == True and base_ambig_NN_VB == False and post_divergence == '{allomorph2}'\",\n",
    "                \"all_query\": all_query,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments testing transfer from\n",
    "# 1. false friend allomorph to matching inflection allomorph\n",
    "# 2. false friend allomorph to non-matching inflection allomorph\n",
    "# 3. inflection allomorph to matching false friend allomorph\n",
    "# 4. inflection allomorph to non-matching false friend allomorph\n",
    "for (inflection, post_divergence), _ in false_friends_df.groupby([\"inflection\", \"post_divergence\"]):\n",
    "    if inflection not in study_false_friends:\n",
    "        continue\n",
    "    for transfer_allomorph in transfer_allomorphs[inflection]:\n",
    "        if inflection in [\"NNS\", \"VBZ\"]:\n",
    "            ambig_clause = \"base_ambig_NN_VB == {ambig} and \"\n",
    "        else:\n",
    "            ambig_clause = \"\"\n",
    "\n",
    "        ambig_positive = ambig_clause.format(ambig=\"True\")\n",
    "        ambig_negative = ambig_clause.format(ambig=\"False\")\n",
    "\n",
    "        experiments[f\"{inflection}-FF-{post_divergence}-to-{inflection}_{transfer_allomorph}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}' and is_regular == True and {ambig_negative} post_divergence == '{transfer_allomorph}'\",\n",
    "        }\n",
    "        experiments[f\"{inflection}_{transfer_allomorph}-to-{inflection}-FF-{post_divergence}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}' and is_regular == True and {ambig_negative} post_divergence == '{transfer_allomorph}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{post_divergence}'\",\n",
    "        }\n",
    "\n",
    "for inflection in study_false_friends:\n",
    "    for t1, t2 in itertools.combinations(transfer_allomorphs[inflection], 2):\n",
    "        experiments[f\"{inflection}-FF-{t1}-to-{inflection}-FF-{t2}\"] = {\n",
    "            \"base_query\": f\"inflection == '{inflection}-FF-{t1}'\",\n",
    "            \"inflected_query\": f\"inflection == '{inflection}-FF-{t2}'\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate experiments for forced-choice analysis\n",
    "fc_types = [infl for infl in all_cross_instances.inflection.unique() if infl.startswith(\"FC\")]\n",
    "fc_types = set(re.findall(r\"FC-([\\w\\s]+)_([\\w\\s]+)\", infl)[0] for infl in fc_types)\n",
    "\n",
    "for fc_pair, config in fc_experiments.items():\n",
    "    fc_pair_name = \"_\".join(fc_pair)\n",
    "    if fc_pair not in fc_types:\n",
    "        raise ValueError(f\"FC pair {fc_pair} not found in FC stimuli\")\n",
    "    \n",
    "    for source_inflection in config[\"source_inflections\"]:\n",
    "        for source_allomorph in transfer_allomorphs[source_inflection]:\n",
    "            experiments[f\"FC-{fc_pair_name}-from_{source_inflection}-{source_allomorph}\"] = {\n",
    "                \"base_query\": f\"inflection == '{source_inflection}' and post_divergence == '{source_allomorph}'\",\n",
    "                \"inflected_query\": f\"inflection == 'FC-{fc_pair_name}'\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_dev = all_cross_instances.drop(columns=[\"base\", \"base_idx\", \"base_phones\", \"base_instance_idx\"]).drop_duplicates([\"inflection\", \"inflected_idx\", \"inflected_instance_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_dev[\"divergence_phoneme_idx\"] = instances_dev.inflected_phones.str.count(\" \") - instances_dev.post_divergence.str.count(\" \")\n",
    "instances_dev[\"last_phoneme_idx\"] = instances_dev.inflected_phones.str.count(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_dev = instances_dev[instances_dev.divergence_phoneme_idx > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "import torch\n",
    "from src.analysis.analogy import nxn_cos_sim\n",
    "import logging\n",
    "\n",
    "L = logging.getLogger(__name__)\n",
    "\n",
    "def iter_equivalences(\n",
    "        config, all_cross_instances, agg_src: np.ndarray,\n",
    "        num_samples=100, max_num_vector_samples=250,\n",
    "        divergence_index: Literal[\"first\", \"last\"] = \"last\",\n",
    "        seed=None,):\n",
    "    \n",
    "    # Pre-compute lookup from label idx, instance idx to flat idx\n",
    "    if isinstance(agg_src, torch.Tensor):\n",
    "        agg_src = agg_src.cpu().numpy()\n",
    "    flat_idx_lookup = {(label_idx, instance_idx, phoneme_idx): flat_idx\n",
    "                       for flat_idx, (label_idx, instance_idx, phoneme_idx) in enumerate(agg_src)}\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if \"group_by\" in config:\n",
    "        grouper = all_cross_instances.groupby(config[\"group_by\"])\n",
    "    else:\n",
    "        grouper = [(\"\", all_cross_instances)]\n",
    "\n",
    "    for group, rows in tqdm(grouper, leave=False):\n",
    "        print(group)\n",
    "\n",
    "        try:\n",
    "            if \"base_query\" in config:\n",
    "                rows_from = rows.query(config[\"base_query\"])\n",
    "            else:\n",
    "                rows_from = rows\n",
    "\n",
    "            if \"inflected_query\" in config:\n",
    "                rows_to = rows.query(config[\"inflected_query\"])\n",
    "            else:\n",
    "                rows_to = rows\n",
    "\n",
    "            if \"all_query\" in config:\n",
    "                rows_from = rows_from.query(config[\"all_query\"])\n",
    "                rows_to = rows_to.query(config[\"all_query\"])\n",
    "\n",
    "            inflection_from = rows_from.inflection.iloc[0]\n",
    "            inflection_to = rows_to.inflection.iloc[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        if len(rows_from) == 0 or len(rows_to) == 0:\n",
    "            continue\n",
    "\n",
    "        # prepare equivalences for 'from' and 'to' groups.\n",
    "        # equivalences define the set of instances over which we can average representations\n",
    "        # before computing the analogy.\n",
    "        if \"equivalence_keys\" in config:\n",
    "            from_equivalence_keys = config[\"equivalence_keys\"]\n",
    "            to_equivalence_keys = config[\"equivalence_keys\"]\n",
    "        else:\n",
    "            from_equivalence_keys = [\"inflected\", \"inflected_phones\"]\n",
    "            to_equivalence_keys = [\"inflected\", \"inflected_phones\"]\n",
    "\n",
    "        # we must group on at least the forms themselves\n",
    "        assert set([\"inflected\", \"inflected_phones\"]) <= set(from_equivalence_keys)\n",
    "        assert set([\"inflected\", \"inflected_phones\"]) <= set(to_equivalence_keys)\n",
    "\n",
    "        from_equiv = rows_from.groupby(from_equivalence_keys)\n",
    "        to_equiv = rows_to.groupby(to_equivalence_keys)\n",
    "        from_equiv_labels = [k for k, count in from_equiv.size().items() if count >= 1]\n",
    "        to_equiv_labels = [k for k, count in to_equiv.size().items() if count >= 1]\n",
    "\n",
    "        if len(set(from_equiv_labels) | set(to_equiv_labels)) <= 1:\n",
    "            # not enough labels to support transfer.\n",
    "            L.error(f\"Skipping {group} due to insufficient labels\")\n",
    "            continue\n",
    "\n",
    "        # Make sure labels are tuples\n",
    "        if not isinstance(from_equiv_labels[0], tuple):\n",
    "            from_equiv_labels = [(label,) for label in from_equiv_labels]\n",
    "        if not isinstance(to_equiv_labels[0], tuple):\n",
    "            to_equiv_labels = [(label,) for label in to_equiv_labels]\n",
    "\n",
    "        # sample pairs of base forms\n",
    "        candidate_pairs = [(x, y) for x, y in itertools.product(from_equiv_labels, to_equiv_labels) if x != y]\n",
    "        num_samples_i = min(num_samples, len(candidate_pairs))\n",
    "        samples = np.random.choice(len(candidate_pairs), num_samples_i, replace=False)\n",
    "\n",
    "        for idx in tqdm(samples, leave=False):\n",
    "            from_equiv_label_i, to_equiv_label_i = candidate_pairs[idx]\n",
    "            rows_from_i = from_equiv.get_group(tuple(from_equiv_label_i))\n",
    "            rows_to_i = to_equiv.get_group(tuple(to_equiv_label_i))\n",
    "\n",
    "            # sample pairs for comparison across the two forms\n",
    "            n = min(max_num_vector_samples, max(len(rows_from_i), len(rows_to_i)))\n",
    "            if len(rows_from_i) < n:\n",
    "                rows_from_i = rows_from_i.sample(n, replace=True)\n",
    "            elif len(rows_from_i) > n:\n",
    "                rows_from_i = rows_from_i.sample(n, replace=False)\n",
    "\n",
    "            if len(rows_to_i) < n:\n",
    "                rows_to_i = rows_to_i.sample(n, replace=True)\n",
    "            elif len(rows_to_i) > n:\n",
    "                rows_to_i = rows_to_i.sample(n, replace=False)\n",
    "\n",
    "            from_label = rows_from_i.inflected.iloc[0]\n",
    "            from_idx = rows_from_i.inflected_idx.iloc[0]\n",
    "            to_label = rows_to_i.inflected.iloc[0]\n",
    "            to_idx = rows_to_i.inflected_idx.iloc[0]\n",
    "\n",
    "            # what are the \"base\" and \"inflected\" forms?\n",
    "            from_inflected_phones = rows_from_i.inflected_phones.iloc[0].split(\" \")\n",
    "            from_base_phones = from_inflected_phones[:rows_from_i.divergence_phoneme_idx.iloc[0]]\n",
    "            from_post_divergence = from_inflected_phones[rows_from_i.divergence_phoneme_idx.iloc[0]:]\n",
    "            to_inflected_phones = rows_to_i.inflected_phones.iloc[0].split(\" \")\n",
    "            to_base_phones = to_inflected_phones[:rows_to_i.divergence_phoneme_idx.iloc[0]]\n",
    "            to_post_divergence = to_inflected_phones[rows_to_i.divergence_phoneme_idx.iloc[0]:]\n",
    "\n",
    "            # compute individual representation indices\n",
    "            if divergence_index == \"first\":\n",
    "                # draw representation of the inflected form from the first diverging phoneme\n",
    "                from_inflected_flat_idx = torch.tensor(\n",
    "                    [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.divergence_phoneme_idx)]\n",
    "                    for _, row in rows_from_i.iterrows()])\n",
    "            elif divergence_index == \"last\":\n",
    "                # draw representation of the inflected form from the last phoneme of the word\n",
    "                from_inflected_flat_idx = torch.tensor(\n",
    "                    [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.last_phoneme_idx)]\n",
    "                    for _, row in rows_from_i.iterrows()])\n",
    "\n",
    "            # TODO design choice: do we take repr from previous phoneme or averaged over all previous\n",
    "            # phonemes?\n",
    "            from_base_flat_idx = torch.tensor(\n",
    "                [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.divergence_phoneme_idx - 1)]\n",
    "                 for _, row in rows_from_i.iterrows()])\n",
    "            to_base_flat_idx = torch.tensor(\n",
    "                [flat_idx_lookup[(row.inflected_idx, row.inflected_instance_idx, row.divergence_phoneme_idx - 1)]\n",
    "                 for _, row in rows_to_i.iterrows()])\n",
    "            \n",
    "            yield {\n",
    "                \"group\": group,\n",
    "\n",
    "                \"from_label\": from_label,\n",
    "                \"from_idx\": from_idx,\n",
    "                \"to_label\": to_label,\n",
    "                \"to_idx\": to_idx,\n",
    "\n",
    "                \"from_inflected_phones\": \" \".join(from_inflected_phones),\n",
    "                \"from_base_phones\": \" \".join(from_base_phones),\n",
    "                \"from_post_divergence\": \" \".join(from_post_divergence),\n",
    "\n",
    "                \"to_inflected_phones\": \" \".join(to_inflected_phones),\n",
    "                \"to_base_phones\": \" \".join(to_base_phones),\n",
    "                \"to_post_divergence\": \" \".join(to_post_divergence),\n",
    "\n",
    "                \"inflection_from\": inflection_from,\n",
    "                \"inflection_to\": inflection_to,\n",
    "                \"from_equiv_label_i\": from_equiv_label_i,\n",
    "                \"to_equiv_label_i\": to_equiv_label_i,\n",
    "                \n",
    "                \"from_inflected_flat_idx\": from_inflected_flat_idx,\n",
    "                \"from_base_flat_idx\": from_base_flat_idx,\n",
    "                \"to_base_flat_idx\": to_base_flat_idx,                \n",
    "            }\n",
    "\n",
    "def run_experiment_equiv_level(\n",
    "        experiment_name, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        device: str = \"cpu\",\n",
    "        verbose=False,\n",
    "        num_samples=100, max_num_vector_samples=250,\n",
    "        seed=None,\n",
    "        exclude_idxs_from_predictions: Optional[dict[int, list[int]]] = None,\n",
    "        include_idxs_in_predictions: Optional[dict[int, list[int]]] = None):\n",
    "    print(experiment_name)\n",
    "\n",
    "    # move data to device\n",
    "    agg = torch.tensor(agg).to(device)\n",
    "    agg_src = torch.tensor(agg_src).to(device)\n",
    "    \n",
    "    results = []\n",
    "    for sample in iter_equivalences(\n",
    "            config, all_cross_instances, agg_src,\n",
    "            num_samples=num_samples,\n",
    "            max_num_vector_samples=max_num_vector_samples,\n",
    "            seed=seed):\n",
    "\n",
    "        from_inflected_flat_idx = sample[\"from_inflected_flat_idx\"]\n",
    "        from_base_flat_idx = sample[\"from_base_flat_idx\"]\n",
    "        to_base_flat_idx = sample[\"to_base_flat_idx\"]\n",
    "\n",
    "        # Critical analogy logic\n",
    "        pair_difference = agg[from_inflected_flat_idx] - agg[from_base_flat_idx]\n",
    "        pair_base = agg[to_base_flat_idx]\n",
    "\n",
    "        pair_predicted = pair_base + pair_difference\n",
    "        pair_predicted /= torch.norm(pair_predicted, dim=1, keepdim=True)\n",
    "\n",
    "        references, references_src = agg, agg_src\n",
    "        with torch.no_grad():\n",
    "            dists = 1 - nxn_cos_sim(pair_predicted, references)\n",
    "            # mean over instances of pair\n",
    "            dists = dists.mean(0)\n",
    "        ranks = dists.argsort()\n",
    "\n",
    "        if exclude_idxs_from_predictions is not None:\n",
    "            valid_idxs = torch.tensor(list(exclude_idxs_from_predictions[sample[\"inflection_to\"], sample[\"to_idx\"]]))\n",
    "            ranks = ranks[~torch.isin(ranks, valid_idxs.to(ranks))]\n",
    "        if include_idxs_in_predictions is not None:\n",
    "            valid_idxs = torch.tensor(list(include_idxs_in_predictions[sample[\"to_idx\"]]))\n",
    "            gt_rank = torch.where(torch.isin(references_src[ranks, 0], valid_idxs.to(ranks)))[0][0].item()\n",
    "        else:\n",
    "            gt_rank = torch.where(references_src[ranks, 0] == sample[\"to_idx\"])[0][0].item()\n",
    "\n",
    "        gt_distance = dists[gt_rank].item()\n",
    "        predicted_phoneme_idx = references_src[gt_rank, 2].item()\n",
    "\n",
    "        if verbose:\n",
    "            for dist, (label_idx, instance_idx, _) in zip(dists[ranks[:5]], references_src[ranks[:5]]):\n",
    "                print(f\"{sample['group']} {sample['from_equiv_label_i']} -> {sample['to_equiv_label_i']}: {state_space_spec.labels[label_idx]} {instance_idx}\")\n",
    "\n",
    "        nearest_neighbor = references_src[ranks[0]]\n",
    "        results.append({\n",
    "            \"group\": sample[\"group\"],\n",
    "\n",
    "            \"from\": sample[\"from_label\"],\n",
    "            \"to\": sample[\"to_label\"],\n",
    "\n",
    "            \"from_inflected_phones\": sample[\"from_inflected_phones\"],\n",
    "            \"from_base_phones\": sample[\"from_base_phones\"],\n",
    "            \"from_post_divergence\": sample[\"from_post_divergence\"],\n",
    "\n",
    "            \"to_inflected_phones\": sample[\"to_inflected_phones\"],\n",
    "            \"to_base_phones\": sample[\"to_base_phones\"],\n",
    "            \"to_post_divergence\": sample[\"to_post_divergence\"],\n",
    "\n",
    "            \"inflection_from\": sample[\"inflection_from\"],\n",
    "            \"inflection_to\": sample[\"inflection_to\"],\n",
    "            \"from_equiv_label\": sample[\"from_equiv_label_i\"],\n",
    "            \"to_equiv_label\": sample[\"to_equiv_label_i\"],\n",
    "\n",
    "            \"predicted_label_idx\": nearest_neighbor[0].item(),\n",
    "            \"predicted_label\": state_space_spec.labels[nearest_neighbor[0]],\n",
    "            \"predicted_instance_idx\": nearest_neighbor[1].item(),\n",
    "            \"predicted_phoneme_idx\": predicted_phoneme_idx,\n",
    "            \"gt_label\": sample[\"to_label\"],\n",
    "            \"gt_label_idx\": sample[\"to_idx\"],\n",
    "            \"gt_label_rank\": gt_rank,\n",
    "            \"gt_distance\": gt_distance,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = \"basic\"\n",
    "# config = experiments[experiment]\n",
    "# ret = run_experiment_equiv_level(\n",
    "#     experiment, config, state_space_spec, instances_dev,\n",
    "#     agg, agg_src,\n",
    "#     num_samples=20,\n",
    "#     device=\"cpu\",\n",
    "#     include_idxs_in_predictions=include_inflected_map,\n",
    "#     exclude_idxs_from_predictions=exclude_inflected_map,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = pd.concat({\n",
    "    experiment: analogy.run_experiment_equiv_level(\n",
    "        experiment, config,\n",
    "        state_space_spec, all_cross_instances,\n",
    "        agg, agg_src,\n",
    "        num_samples=1000,\n",
    "        seed=seed,\n",
    "        device=\"cuda\")\n",
    "    for experiment, config in tqdm(experiments.items(), unit=\"experiment\")\n",
    "}, names=[\"experiment\"])\n",
    "experiment_results[\"correct\"] = experiment_results.predicted_label == experiment_results.gt_label\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv(f\"{output_dir}/experiment_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
