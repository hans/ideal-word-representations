{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import prepare_state_trajectory, StateSpaceAnalysisSpec\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/timit/w2v2_6/rnn_8/phoneme\"\n",
    "output_dir = \"outputs/notebooks/timit/w2v2_6/rnn_8/phoneme/plot\"\n",
    "dataset_path = \"outputs/preprocessed_data/timit\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/timit/w2v2_6/phoneme/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/timit/w2v2_6/hidden_states.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/timit/w2v2_6/state_space_specs.h5\"\n",
    "embeddings_path = \"outputs/model_embeddings/timit/w2v2_6/rnn_8/phoneme/embeddings.npy\"\n",
    "\n",
    "# Add 4 frames prior to phoneme onset to each trajectory\n",
    "expand_frame_window = (4, 0)\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(equivalence_path, \"rb\") as f:\n",
    "    equiv_dataset: SpeechEquivalenceDataset = torch.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"phoneme\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, expand_window=expand_frame_window, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_temporal_generalization(trajectory, lengths, train_frame, test_frame):\n",
    "    X, Y = [], []\n",
    "    for traj_i, lengths_i in zip(trajectory, lengths):\n",
    "        analyze = ((lengths_i > test_frame) & (lengths_i > train_frame)).nonzero()[0]\n",
    "        for idx in analyze:\n",
    "            X.append(traj_i[idx, train_frame])\n",
    "            Y.append(traj_i[idx, test_frame])\n",
    "\n",
    "    if len(X) < 100:\n",
    "        return np.nan\n",
    "\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "\n",
    "    # Fit linear model\n",
    "    model = RidgeCV(cv=KFold(3, shuffle=True))\n",
    "    return cross_val_score(model, X, Y, cv=KFold(3, shuffle=True), scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_generalization_scores = np.zeros((trajectory[0].shape[1], trajectory[0].shape[1])) * np.nan\n",
    "for train_frame, test_frame in tqdm(list(itertools.product(range(trajectory[0].shape[1]), repeat=2))):\n",
    "    scores = evaluate_temporal_generalization(trajectory, lengths, train_frame, test_frame)\n",
    "    temporal_generalization_scores[train_frame, test_frame] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_generalization_df = pd.DataFrame(temporal_generalization_scores, columns=pd.Index(range(trajectory[0].shape[1]), name=\"test_frame\"),\n",
    "                                          index=pd.Index(range(trajectory[0].shape[1]), name=\"train_frame\"))\n",
    "temporal_generalization_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_generalization_df.to_csv(Path(output_dir) / \"temporal_generalization.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = temporal_generalization_df.iloc[:30, :30]\n",
    "\n",
    "ax = sns.heatmap(plot_df, cmap=\"RdBu_r\", center=0, xticklabels=10, yticklabels=10)\n",
    "\n",
    "assert expand_frame_window[1] == 0\n",
    "# Draw phoneme onset\n",
    "if expand_frame_window[0] != 0:\n",
    "    ax.axvline(expand_frame_window[0], color=\"gray\", linestyle=\"--\")\n",
    "    ax.axhline(expand_frame_window[0], color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"Test frame\")\n",
    "ax.set_ylabel(\"Train frame\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explore310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
