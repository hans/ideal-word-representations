{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare state space trajectories for a lexical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.analysis.trf import estimate_trf_cv\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset, SpeechHiddenStateDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/librispeech-train-clean-100/w2v2_8/rnn_32-aniso1/word_broad_10frames\"\n",
    "output_dir = \".\"\n",
    "dataset_path = \"outputs/preprocessed_data/librispeech-train-clean-100\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/librispeech-train-clean-100/w2v2_8/word_broad_10frames/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/librispeech-train-clean-100/w2v2_8/hidden_states.h5\"\n",
    "phoneme_equivalence_path = \"outputs/equivalence_datasets/librispeech-train-clean-100/w2v2_8/phoneme_10frames/equivalence.pkl\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/librispeech-train-clean-100/w2v2_8/state_space_specs.pkl\"\n",
    "embeddings_path = \"outputs/model_embeddings/librispeech-train-clean-100/w2v2_8/rnn_32-aniso1/word_broad_10frames/embeddings.npy\"\n",
    "\n",
    "strffeat_path = \"/userdata/jgauthier/ilina_timit/speech_analysis/out_sentence_details_timit_all_loudness.mat\"\n",
    "strffeat_extract_features = [\"aud\",\n",
    "                            #  \"F0\",  # TODO copy over pitch analysis code\n",
    "                             \"maxDtL\",\n",
    "                             \"phnfeatConsOnset\",\n",
    "                             \"formantMedOnset\"]\n",
    "\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(phoneme_equivalence_path, \"rb\") as f:\n",
    "    equiv_dataset: SpeechEquivalenceDataset = torch.load(f)\n",
    "hidden_state_dataset = SpeechHiddenStateDataset.from_hdf5(hidden_states_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_by_item = hidden_state_dataset.frames_by_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2).fit(model_representations)\n",
    "model_representations_pca = pca.transform(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_corpus = datasets.load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemes = set(equiv_dataset.class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_features = {\n",
    "    \"AA\": \"low back\",\n",
    "    \"AE\": \"low front\",\n",
    "    \"AH\": \"low\",\n",
    "    \"AO\": \"low back\",\n",
    "    \"AW\": \"back rounded\",\n",
    "    \"AY\": \"high front\",\n",
    "    \"B\": \"bilabial plosive\",\n",
    "    \"CH\": \"voiceless palato-alveolar affricate\",\n",
    "    \"D\": \"alveolar plosive\",\n",
    "    \"DH\": \"dental fricative\",\n",
    "    \"EH\": \"front\",\n",
    "    \"ER\": \"\",\n",
    "    \"EY\": \"front rounded\",\n",
    "    \"F\": \"voiceless labiodental fricative\",\n",
    "    \"G\": \"velar plosive\",\n",
    "    \"HH\": \"voiceless glottal fricative\",\n",
    "    \"IH\": \"high front\",\n",
    "    \"IY\": \"high front rounded\",\n",
    "    \"JH\": \"palato-alveolar affricate\",\n",
    "    \"K\": \"voiceless velar plosive\",\n",
    "    \"L\": \"alveolar lateral approximant\",\n",
    "    \"M\": \"bilabial nasal\",\n",
    "    \"N\": \"alveolar nasal\",\n",
    "    \"NG\": \"velar nasal\",\n",
    "    \"OW\": \"back rounded\",\n",
    "    \"OY\": \"back rounded\",\n",
    "    \"P\": \"voiceless bilabial plosive\",\n",
    "    \"R\": \"alveolar approximant\",\n",
    "    \"S\": \"voiceless alveolar fricative\",\n",
    "    \"SH\": \"voiceless palato-alveolar fricative\",\n",
    "    \"T\": \"voiceless alveolar plosive\",\n",
    "    \"TH\": \"voiceless dental fricative\",\n",
    "    \"UH\": \"high back rounded\",\n",
    "    \"UW\": \"high back rounded\",\n",
    "    \"V\": \"labiodental fricative\",\n",
    "    \"W\": \"labio-velar approximant\",\n",
    "    \"Y\": \"palatal approximant\",\n",
    "    \"Z\": \"alveolar fricative\",\n",
    "    \"ZH\": \"palato-alveolar fricative\",\n",
    "}\n",
    "cmudict_features = {k: v.split() for k, v in cmudict_features.items()}\n",
    "\n",
    "# NB there is no \"voiced\" feature -- this is to avoid overcomplete representation / singular\n",
    "# matrix issues\n",
    "# NB vowel height has no \"mid\" feature -- avoid overcompleteness\n",
    "# NB vowel height has no \"central\" feature -- avoid overcompleteness\n",
    "# NB vowels have no \"unrounded\" feature -- avoid overcompleteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(sorted(set(itertools.chain(*cmudict_features.values()))))\n",
    "phon_feature2idx = {f: i for i, f in enumerate(all_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare categorization of these features for a feature selection analysis\n",
    "feature_categories = {\n",
    "    \"consonant_place\": \"alveolar bilabial dental glottal labio-velar labiodental palatal palato-alveolar velar\".split(),\n",
    "    \"consonant_manner\": \"affricate approximant fricative lateral nasal plosive\".split(),\n",
    "    \"vowel\": \"back front low high rounded\".split(),\n",
    "    \"voicing\": \"voiceless\".split(),\n",
    "}\n",
    "\n",
    "for c1, c2 in itertools.combinations(feature_categories, 2):\n",
    "    assert not set(feature_categories[c1]) & set(feature_categories[c2])\n",
    "assert set(itertools.chain.from_iterable(feature_categories.values())) == set(all_features), \\\n",
    "    f\"Missing features: {set(all_features) - set(itertools.chain.from_iterable(feature_categories.values()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_feature_idxs = {k: [phon_feature2idx[f] for f in v] for k, v in cmudict_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phon_feature_to_phonemes = {f: [k for k, v in cmudict_features.items() if f in v] for f in all_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(type(label) == str for label in equiv_dataset.class_labels), \"Assumes dataset with phoneme labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TIMIT features from STRF encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "\n",
    "# strffeat = loadmat(strffeat_path, simplify_cells=True)[\"sentdet\"]\n",
    "# strffeat_all_names = [x[\"name\"] for x in strffeat]\n",
    "\n",
    "# # Prepare relevant feature shortnames\n",
    "# for trial in strffeat:\n",
    "#     trial[\"onset\"] = trial[\"onsOff\"][0]\n",
    "#     trial[\"offset\"] = trial[\"onsOff\"][1]\n",
    "\n",
    "#     trial[\"phnfeatConsOnset\"] = trial[\"phnfeatonset\"][[0, 1, 2, 7, 8, 10]]\n",
    "    \n",
    "#     trial[\"maxDtL\"] = trial[\"loudnessall\"][5]\n",
    "\n",
    "#     # auditory and spectral features\n",
    "#     trial[\"aud\"] = trial[\"aud\"][:80]\n",
    "#     # trial[\"F0\"] = trial[\"f0\"][0]\n",
    "#     trial[\"formantMedOnset\"] = trial[\"frmMedOns\"][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_to_item_idx, name_to_frame_bounds, compression_ratios = {}, {}, {}\n",
    "# def process_item(item, idx):\n",
    "#     name = Path(item[\"file\"]).parent.stem.lower() + \"_\" + item[\"id\"].lower()\n",
    "#     if name in strffeat_all_names:\n",
    "#         name_to_item_idx[name] = idx\n",
    "\n",
    "#         frame_start, frame_end = equiv_dataset.hidden_state_dataset.frames_by_item[idx]\n",
    "#         name_to_frame_bounds[name] = (frame_start, frame_end)\n",
    "#         compression_ratios[name] = (frame_end - frame_start) / len(item[\"input_values\"])\n",
    "# timit_corpus.map(process_item, with_indices=True)\n",
    "\n",
    "# item_idx_to_name = {v: k for k, v in name_to_item_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure that sentence features and model embeddings are of approximately the same length,\n",
    "# # modulo sampling differences. Compute length of each sentence in seconds according\n",
    "# # to two sources:\n",
    "# comparisons = [(strffeat_i[\"aud\"].shape[1] / 100 - 1, # remove padding\n",
    "#                 (name_to_frame_bounds[strffeat_i['name']][1] - name_to_frame_bounds[strffeat_i['name']][0]) / compression_ratios[strffeat_i[\"name\"]] / 16000)\n",
    "#                 for strffeat_i in strffeat if strffeat_i[\"name\"] in name_to_frame_bounds]\n",
    "# np.testing.assert_allclose(*zip(*comparisons), atol=0.04,\n",
    "#                             err_msg=\"ECoG data and model embeddings should be of approximately the same length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre-compute the total number of features\n",
    "# strffeat_feature_names = []\n",
    "# for feature_set in strffeat_extract_features:\n",
    "#     feature_example = strffeat[0][feature_set]\n",
    "#     if feature_example.ndim == 2:\n",
    "#         strffeat_feature_names.extend([f\"{feature_set}_{i}\" for i in range(feature_example.shape[0])])\n",
    "#     else:\n",
    "#         strffeat_feature_names.append(feature_set)\n",
    "\n",
    "# strffeat_feature_dim = len(strffeat_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import resample\n",
    "\n",
    "# strffeat_feature_matrices: dict[int, np.ndarray] = {}\n",
    "# for trial_i in tqdm(strffeat):\n",
    "#     name = trial[\"name\"]\n",
    "#     if name not in name_to_item_idx:\n",
    "#         continue\n",
    "\n",
    "#     frame_start, frame_end = name_to_frame_bounds[name]\n",
    "#     ret_features_i = np.zeros((strffeat_feature_dim, frame_end - frame_start))\n",
    "\n",
    "#     feature_offset = 0\n",
    "#     for feature_set in strffeat_extract_features:\n",
    "#         feature_ij = trial[feature_set]\n",
    "#         if feature_ij.ndim == 1:\n",
    "#             feature_ij = feature_ij[None, :]\n",
    "#         # feature_ij : (n_features, n_samples)\n",
    "\n",
    "#         # Trim padding\n",
    "#         before_pad, after_pad = trial[\"befaft\"]\n",
    "#         before_pad = int(trial[\"dataf\"] * before_pad)\n",
    "#         after_pad = int(trial[\"dataf\"] * after_pad)\n",
    "#         feature_ij = feature_ij[:, before_pad:-after_pad]\n",
    "\n",
    "#         # Resample to match the model's sampling rate\n",
    "#         # If this is an onset feature, do this manually by visiting each nonzero\n",
    "#         # value and finding the corresponding frame\n",
    "#         is_onset_feature = set(itertools.chain.from_iterable(feature_ij.tolist())) == {0, 1}\n",
    "#         if is_onset_feature:\n",
    "#             for feature_idx, sample in zip(*feature_ij.nonzero()):\n",
    "#                 # convert to model sample index\n",
    "#                 sample = int(sample / trial[\"dataf\"] * 16000 * compression_ratios[name])\n",
    "#                 ret_features_i[feature_offset + feature_idx, sample] = 1.\n",
    "#         else:\n",
    "#             for feature_idx in range(feature_ij.shape[0]):\n",
    "#                 ret_features_i[feature_offset + feature_idx] = \\\n",
    "#                     resample(feature_ij[feature_idx], frame_end - frame_start)\n",
    "\n",
    "#         feature_offset += feature_ij.shape[0]\n",
    "\n",
    "#     strffeat_feature_matrices[name_to_item_idx[name]] = ret_features_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_to_item_idx[\"mzmb0_si1796\"]\n",
    "# np.array(timit_corpus[1133][\"phonemic_detail\"][\"start\"]) / 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"onset_item\", \"onset_word\", \"onset_phoneme\"]\n",
    "\n",
    "use_strffeat_features = False\n",
    "if use_strffeat_features:\n",
    "    feature_names += strffeat_feature_names\n",
    "else:\n",
    "    feature_names += list(phon_feature_to_phonemes.keys())\n",
    "\n",
    "feature2idx = {name: i for i, name in enumerate(feature_names)}\n",
    "design_matrix = np.zeros((model_representations.shape[0], len(feature_names)))\n",
    "fit_mask = np.ones(model_representations.shape[0], dtype=bool)\n",
    "\n",
    "def update_design_matrix(item, idx):\n",
    "    start_frame, end_frame = frames_by_item[idx]\n",
    "    num_frames = end_frame - start_frame\n",
    "    compression_ratio = num_frames / len(item[\"input_values\"])\n",
    "\n",
    "    design_matrix[start_frame, feature2idx[\"onset_item\"]] = 1.\n",
    "\n",
    "    if use_strffeat_features and idx not in strffeat_feature_matrices:\n",
    "        fit_mask[start_frame:end_frame] = False\n",
    "        return\n",
    "\n",
    "    for word in item[\"word_phonemic_detail\"]:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "\n",
    "        word_start = start_frame + int(word[0][\"start\"] * compression_ratio)\n",
    "        design_matrix[word_start, feature2idx[\"onset_word\"]] = 1.\n",
    "\n",
    "        for phoneme in word:\n",
    "            phoneme_start = start_frame + int(phoneme[\"start\"] * compression_ratio)\n",
    "            design_matrix[phoneme_start, feature2idx[\"onset_phoneme\"]] = 1.\n",
    "\n",
    "            if not use_strffeat_features:\n",
    "                for feature in cmudict_features[phoneme[\"phone\"]]:\n",
    "                    design_matrix[phoneme_start, feature2idx[feature]] = 1.\n",
    "\n",
    "    if use_strffeat_features:\n",
    "        for strffeat_feature_idx, feature_name in enumerate(strffeat_feature_names):\n",
    "            design_matrix[start_frame:end_frame, feature2idx[feature_name]] = \\\n",
    "                strffeat_feature_matrices[idx][strffeat_feature_idx]\n",
    "\n",
    "timit_corpus.map(update_design_matrix, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate feature correlation in design matrix\n",
    "feature_correlation = np.corrcoef(design_matrix[fit_mask].T)\n",
    "feature_correlation[np.diag_indices_from(feature_correlation)] = 0\n",
    "pd.DataFrame(feature_correlation, index=feature_names, columns=feature_names) \\\n",
    "    .reset_index().melt(id_vars=[\"index\"]).sort_values(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = estimate_trf_cv(\n",
    "    design_matrix,\n",
    "    model_representations,\n",
    "    [str(idx) for idx in range(model_representations.shape[1])],\n",
    "    feature_names=feature_names, tmin=-1, tmax=5, sfreq=1,\n",
    "    n_splits=3,\n",
    "    return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf.coefs.to_csv(Path(output_dir) / \"trf_df.csv\")\n",
    "trf.scores.to_csv(Path(output_dir) / \"trf_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_coefs = trf.coefs.groupby([\"feature\"]).apply(lambda xs: xs.groupby(\"output_name\").coef.apply(np.linalg.norm).agg([\"idxmax\", \"max\"])).sort_values(\"max\").iloc[-30:]\n",
    "largest_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_df = pd.merge(trf.coefs, largest_coefs.reset_index(),\n",
    "                        left_on=[\"feature\", \"output_name\"], right_on=[\"feature\", \"idxmax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline by pre-onset activation\n",
    "highlight_df = pd.merge(highlight_df, highlight_df.groupby([\"feature\", \"output_name\", \"fold\"]).apply(lambda xs: xs[xs.lag <= 0].coef.mean()).rename(\"baseline\"),\n",
    "                        left_on=[\"feature\", \"output_name\", \"fold\"], right_index=True)\n",
    "highlight_df[\"coef_baselined\"] = highlight_df[\"coef\"] - highlight_df[\"baseline\"]\n",
    "highlight_df[\"feature\"] = highlight_df.feature.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(data=highlight_df, col=\"feature\", col_wrap=4,\n",
    "                x=\"time\", y=\"coef_baselined\", hue=\"output_name\", kind=\"line\",\n",
    "                facet_kws=dict(sharex=False, sharey=False))\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_coefs = trf.coefs.groupby([\"feature\"]).apply(lambda xs: xs.groupby(\"output_name\").coef.apply(np.linalg.norm).agg([\"idxmin\", \"min\"])).sort_values(\"min\").iloc[:30]\n",
    "highlight_df = pd.merge(trf.coefs, smallest_coefs.reset_index(),\n",
    "                        left_on=[\"feature\", \"output_name\"], right_on=[\"feature\", \"idxmin\"])\n",
    "# baseline by pre-onset activation\n",
    "highlight_df = pd.merge(highlight_df, highlight_df.groupby([\"feature\", \"output_name\", \"fold\"]).apply(lambda xs: xs[xs.lag <= 0].coef.mean()).rename(\"baseline\"),\n",
    "                        left_on=[\"feature\", \"output_name\", \"fold\"], right_index=True)\n",
    "highlight_df[\"coef_baselined\"] = highlight_df[\"coef\"] - highlight_df[\"baseline\"]\n",
    "highlight_df[\"feature\"] = highlight_df.feature.astype(str)\n",
    "\n",
    "g = sns.relplot(data=highlight_df, col=\"feature\", col_wrap=4,\n",
    "                x=\"time\", y=\"coef_baselined\", hue=\"output_name\", kind=\"line\",\n",
    "                facet_kws=dict(sharex=False, sharey=False))\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item(item_idx, ax):\n",
    "    item = timit_corpus[item_idx]\n",
    "    start_frame, end_frame = frames_by_item[item_idx]\n",
    "    compression_ratio = (end_frame - start_frame) / len(item[\"input_values\"])\n",
    "\n",
    "    times = np.linspace(0, len(item[\"input_values\"]) / 16000, int(len(item[\"input_values\"]) / 16000 * 1000))\n",
    "    ax.plot(times, np.interp(times, np.arange(len(item[\"input_values\"])) / 16000,\n",
    "                            item[\"input_values\"]),\n",
    "            alpha=0.3)\n",
    "\n",
    "    # plot word and phoneme boundaries\n",
    "    for i, word in enumerate(item[\"word_phonemic_detail\"]):\n",
    "        word_str = item[\"word_detail\"][\"utterance\"][i]\n",
    "\n",
    "        word_start, word_stop = word[0][\"start\"] / 16000, word[-1][\"stop\"] / 16000\n",
    "        ax.axvline(word_start, color=\"black\", linestyle=\"--\")\n",
    "        ax.text(word_start, -6, word_str, rotation=90, verticalalignment=\"top\")\n",
    "\n",
    "        for j, phoneme in enumerate(word):\n",
    "            phoneme_str = phoneme[\"phone\"]\n",
    "            phoneme_start, phoneme_stop = phoneme[\"start\"] / 16000, phoneme[\"stop\"] / 16000\n",
    "\n",
    "            if j > 0:\n",
    "                color = \"black\" if phoneme[\"idx_in_syllable\"] == 0 else \"gray\"\n",
    "                ax.axvline(phoneme_start, color=color, linestyle=\":\")\n",
    "            ax.text(phoneme_start + 0.01, -5, phoneme_str, rotation=90, verticalalignment=\"bottom\", fontdict={\"size\": 8})\n",
    "\n",
    "    model_ax = ax.twinx()\n",
    "    palette = sns.color_palette(\"tab10\", model_representations.shape[1])\n",
    "    for dim in range(model_representations.shape[1]):\n",
    "        model_ax.plot(times, np.interp(times, np.arange(0, end_frame - start_frame) / compression_ratio / 16000,\n",
    "                                model_representations[start_frame:end_frame, dim]),\n",
    "                      label=f\"dim{dim}\", color=palette[dim])\n",
    "\n",
    "    # align at origin\n",
    "    ax.set_ylim((-8, 8))\n",
    "    model_ax.set_ylim((-2, 2))\n",
    "    model_ax.legend()\n",
    "\n",
    "    ax.set_title(f\"{item['speaker_id']}_{item['id']}: {item['text']}\")\n",
    "\n",
    "    return ax, model_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item_idx = np.random.choice(list(frames_by_item.keys()))\n",
    "start_frame, end_frame = frames_by_item[plot_item_idx]\n",
    "compression_ratio = (end_frame - start_frame) / len(timit_corpus[plot_item_idx][\"input_values\"])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(18, 8))\n",
    "ax, twinx = plot_item(plot_item_idx, ax)\n",
    "\n",
    "# plot predicted time course\n",
    "predicted = trf.predictions[start_frame:end_frame]\n",
    "times = np.linspace(0, len(timit_corpus[plot_item_idx][\"input_values\"]) / 16000, predicted.shape[0], endpoint=False)\n",
    "palette = sns.color_palette(\"tab10\", predicted.shape[1])\n",
    "for dim in range(predicted.shape[1]):\n",
    "    twinx.plot(times, predicted[:, dim], label=f\"dim{dim}\", linestyle=\":\", alpha=0.7, color=palette[dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature category forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_category_list = list(feature_categories.keys())\n",
    "n_categories_to_select = len(feature_category_list) - 1\n",
    "\n",
    "def get_design_matrix_mask(feature_category_mask):\n",
    "    mask = np.ones(len(feature_names), dtype=bool)\n",
    "\n",
    "    assert len(feature_category_list) == len(feature_category_mask)\n",
    "    for category, include in zip(feature_category_list, feature_category_mask):\n",
    "        for feature in feature_categories[category]:\n",
    "            mask[feature2idx[feature]] = include\n",
    "    return mask\n",
    "\n",
    "\n",
    "def estimate_model_with_selection(feature_category_mask):\n",
    "    mask = get_design_matrix_mask(feature_category_mask)\n",
    "    feature_names_masked = [name for name, mask_i in zip(feature_names, mask) if mask_i]\n",
    "    trf = estimate_trf_cv(\n",
    "        design_matrix[:, mask],\n",
    "        model_representations,\n",
    "        [str(idx) for idx in range(model_representations.shape[1])],\n",
    "        feature_names=feature_names_masked, tmin=-1, tmax=5, sfreq=1,\n",
    "        n_splits=3)\n",
    "    return trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "mne.set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mask = np.zeros(len(feature_category_list), dtype=bool)\n",
    "best_trf = estimate_model_with_selection(best_mask)\n",
    "category_trajectory = [None]\n",
    "score_trajectory = [best_trf.scores.mean().mean()]\n",
    "\n",
    "for _ in trange(n_categories_to_select):\n",
    "    candidate_category_idxs = np.flatnonzero(~best_mask)\n",
    "    candidate_models = []\n",
    "\n",
    "    for i, category_idx in enumerate(tqdm(candidate_category_idxs)):\n",
    "        candidate_mask = best_mask.copy()\n",
    "        candidate_mask[category_idx] = True\n",
    "        candidate_model = estimate_model_with_selection(candidate_mask)\n",
    "        candidate_models.append(candidate_model)\n",
    "\n",
    "        print(f\"New score with {feature_category_list[category_idx]}: {candidate_model.scores.mean().mean()}\")\n",
    "\n",
    "    candidate_score_means = np.array([model.scores.mean().mean() for model in candidate_models])\n",
    "    best_candidate_score = candidate_score_means.max()\n",
    "    if best_candidate_score > best_trf.scores.mean().mean():\n",
    "        best_candidate_idx = candidate_score_means.argmax()\n",
    "        print(f\"Best improvement with {feature_category_list[candidate_category_idxs[best_candidate_idx]]}: {best_candidate_score}\")\n",
    "\n",
    "        best_new_category_idx = candidate_category_idxs[best_candidate_idx]\n",
    "        best_mask[best_new_category_idx] = True\n",
    "        best_trf = candidate_models[best_candidate_idx]\n",
    "\n",
    "        category_trajectory.append(feature_category_list[best_new_category_idx])\n",
    "        score_trajectory.append(best_candidate_score)\n",
    "\n",
    "    print(f\"=== Best category set: {', '.join(feature_category_list[idx] for idx in best_mask.nonzero()[0])}; {best_trf.scores.mean().mean()} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_traj = pd.DataFrame({\"category\": category_trajectory, \"score\": score_trajectory})\n",
    "selection_traj.to_csv(Path(output_dir) / \"trf_feature_selection_trajectory.csv\")\n",
    "selection_traj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
