{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import coherence\n",
    "from src.analysis.state_space import prepare_state_trajectory, StateSpaceAnalysisSpec\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/timit/w2v2_6/rnn_8/phoneme\"\n",
    "output_dir = \"outputs/notebooks/timit/w2v2_6/rnn_8/phoneme/plot\"\n",
    "dataset_path = \"outputs/preprocessed_data/timit\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/timit/w2v2_6/phoneme/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/timit/w2v2_6/hidden_states.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/timit/w2v2_6/state_space_specs.pkl\"\n",
    "embeddings_path = \"outputs/model_embeddings/timit/w2v2_6/rnn_8/phoneme/embeddings.npy\"\n",
    "\n",
    "output_dir = \".\"\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Retain syllables with N or more instances\n",
    "retain_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(equivalence_path, \"rb\") as f:\n",
    "    equiv_dataset: SpeechEquivalenceDataset = torch.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"syllable\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idxs = [idx for idx, target_frames in enumerate(state_space_spec.target_frame_spans)\n",
    "               if len(target_frames) < retain_n]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_label_strs = [\"\".join(phones) for phones in state_space_spec.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate within-syllable distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance, within_distance_offset = \\\n",
    "    coherence.estimate_within_distance(trajectory, lengths, state_space_spec, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(within_distance, center=1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_df = pd.DataFrame(within_distance, index=pd.Index(spec_label_strs, name=\"syllable\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"syllable\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_offset_df = pd.DataFrame(within_distance_offset, index=pd.Index(spec_label_strs, name=\"syllable\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"syllable\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate between-syllable distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distance, between_distance_offset = \\\n",
    "    coherence.estimate_between_distance(trajectory, lengths, state_space_spec, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_df = pd.DataFrame(np.nanmean(between_distance, axis=-1),\n",
    "                                    index=pd.Index(spec_label_strs, name=\"syllable\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"syllable\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_offset_df = pd.DataFrame(np.nanmean(between_distance_offset, axis=-1),\n",
    "                                    index=pd.Index(spec_label_strs, name=\"syllable\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"syllable\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([within_distance_df.assign(type=\"within\"), between_distances_df.assign(type=\"between\")])\n",
    "merged_df[\"time\"] = merged_df.frame / 20\n",
    "merged_df.to_csv(Path(output_dir) / \"distances.csv\", index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=merged_df.dropna(), x=\"time\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance within- and between-syllable\")\n",
    "ax.set_xlabel(\"Time since syllable onset (seconds)\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_offset_df = pd.concat([within_distance_offset_df.assign(type=\"within\"), between_distances_offset_df.assign(type=\"between\")])\n",
    "merged_offset_df[\"time\"] = (merged_offset_df.frame - 1) / 20\n",
    "merged_offset_df.to_csv(Path(output_dir) / \"distances_aligned_offset.csv\", index=False)\n",
    "merged_offset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=merged_offset_df.dropna(), x=\"time\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance within- and between-syllable\")\n",
    "ax.set_xlabel(\"Time from syllable offset (seconds)\")\n",
    "ax.invert_xaxis()\n",
    "ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate between-syllable distance, grouped by features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets = [syll[0] for syll in state_space_spec.labels]\n",
    "\n",
    "onset_distance_df, onset_distance_offset_df = coherence.estimate_category_within_between_distance(\n",
    "    trajectory, lengths, onsets, metric=metric, labels=state_space_spec.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_distance_df[\"time\"] = onset_distance_df.frame / 20\n",
    "onset_distance_df.to_csv(Path(output_dir) / \"distances-grouped_onset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=onset_distance_df.dropna(), x=\"time\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance by onset match/mismatch\")\n",
    "ax.set_xlabel(\"Time since syllable onset (seconds)\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_distance_offset_df[\"time\"] = (onset_distance_offset_df.frame - 1) / 20\n",
    "onset_distance_offset_df.to_csv(Path(output_dir) / \"distances-grouped_onset_aligned_offset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=onset_distance_offset_df.dropna(), x=\"time\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance by onset match/mismatch\")\n",
    "ax.set_xlabel(\"Time from syllable offset (seconds)\")\n",
    "ax.invert_xaxis()\n",
    "ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\", \"IY\", \"OW\", \"OY\", \"UH\", \"UW\"]\n",
    "nuclei = []\n",
    "for syll in state_space_spec.labels:\n",
    "    syll_nucleus = None\n",
    "    for phone in syll:\n",
    "        if phone in vowels:\n",
    "            syll_nucleus = phone\n",
    "            break\n",
    "    nuclei.append(syll_nucleus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nuclei).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus_distance_df, nucleus_distance_offset_df = coherence.estimate_category_within_between_distance(\n",
    "    trajectory, lengths, nuclei, metric=metric, labels=state_space_spec.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus_distance_df[\"time\"] = (nucleus_distance_df.frame) / 20\n",
    "nucleus_distance_df.to_csv(Path(output_dir) / \"distances-grouped_nucleus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=nucleus_distance_df.dropna(), x=\"frame\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance by nucleus match/mismatch\")\n",
    "ax.set_xlabel(\"Frames since syllable onset\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleus_distance_offset_df[\"time\"] = (nucleus_distance_offset_df.frame - 1) / 20\n",
    "nucleus_distance_offset_df.to_csv(Path(output_dir) / \"distances-grouped_nucleus_aligned_offset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=nucleus_distance_offset_df.dropna(), x=\"time\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance by nucleus match/mismatch\")\n",
    "ax.set_xlabel(\"Time from syllable offset (seconds)\")\n",
    "ax.invert_xaxis()\n",
    "ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = trajectory[0].shape[1]\n",
    "\n",
    "syllable_length = 2\n",
    "all_syllables = sorted([label for label in state_space_spec.labels if len(label) == syllable_length])\n",
    "\n",
    "# Prepare balanced sample of representations for each syllable in each category\n",
    "num_instances = min(len(state_space_spec.target_frame_spans[state_space_spec.labels.index(syllable)])\n",
    "                    for syllable in all_syllables)\n",
    "\n",
    "syllable_representations = {}\n",
    "for syllable in all_syllables:\n",
    "    sample_instance_idxs = np.random.choice(len(state_space_spec.target_frame_spans[state_space_spec.labels.index(syllable)]),\n",
    "                                            num_instances, replace=False)\n",
    "    syllable_representations[syllable] = np.array([trajectory[state_space_spec.labels.index(syllable)][idx]\n",
    "                                                   for idx in sample_instance_idxs])\n",
    "\n",
    "# Compute between-phoneme distances\n",
    "from src.analysis.coherence import get_mean_distance\n",
    "distances = np.zeros((len(all_syllables), len(all_syllables), trajectory[0].shape[1]))\n",
    "for p1, p2 in tqdm(list(itertools.product(list(range(len(all_syllables))), repeat=2))):\n",
    "    if p1 == p2:\n",
    "        continue\n",
    "    for k in range(num_frames):\n",
    "        distances[p1, p2, k] = get_mean_distance(syllable_representations[all_syllables[p1]][:, k, :],\n",
    "                                                 syllable_representations[all_syllables[p2]][:, k, :], metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syllable_labels = [\"\".join(phones) for phones in all_syllables]\n",
    "sns.clustermap(pd.DataFrame(np.nanmean(distances, axis=-1), index=all_syllable_labels, columns=all_syllable_labels),\n",
    "               center=1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset = np.random.choice(len(all_syllable_labels), size=30, replace=False)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.clustermap(np.nanmean(distances, axis=-1)[plot_subset][:, plot_subset], center=1, cmap=\"RdBu\",\n",
    "            xticklabels=[all_syllable_labels[i] for i in plot_subset],\n",
    "            yticklabels=[all_syllable_labels[i] for i in plot_subset])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
