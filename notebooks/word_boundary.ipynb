{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import prepare_state_trajectory, StateSpaceAnalysisSpec, flatten_trajectory\n",
    "from src.utils.timit import get_word_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/librispeech-train-clean-100/w2v2_8/rnn_32-hinge-mAP4/word_broad_10frames_fixedlen25\"\n",
    "output_dir = \".\"\n",
    "dataset_path = \"outputs/preprocessed_data/librispeech-train-clean-100\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/librispeech-train-clean-100/w2v2_8/word_broad_10frames_fixedlen25/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/librispeech-train-clean-100/w2v2_8/hidden_states.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/librispeech-train-clean-100/w2v2_8/state_space_specs.h5\"\n",
    "embeddings_path = \"outputs/model_embeddings/librispeech-train-clean-100/w2v2_8/rnn_32-hinge-mAP4/word_broad_10frames_fixedlen25/librispeech-train-clean-100.npy\"\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Retain words with N or more instances\n",
    "retain_n = 10\n",
    "\n",
    "subsample_instances = 50\n",
    "\n",
    "model_sfreq = 50\n",
    "\n",
    "expand_window = (15, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = state_space_spec.label_counts\n",
    "drop_labels = label_counts[label_counts < retain_n].index\n",
    "state_space_spec = state_space_spec.drop_labels(drop_names=drop_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(subsample_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_word_metadata(state_space_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan,\n",
    "                                      expand_window=expand_window)\n",
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_full, traj_full_flat_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectory), np.concatenate(lengths).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(n_components=4)\n",
    "traj_full_flat_pca = pca_full.fit_transform(traj_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare truncated trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_trunc = [traj_i[:, :(expand_window[0] + 15)] for traj_i in trajectory]\n",
    "traj_trunc_flat, traj_trunc_flat_src = flatten_trajectory(traj_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_times = np.concatenate([-np.arange(expand_window[0], -1, -1), np.arange(1, 15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "traj_trunc_flat_pca = pca.fit_transform(traj_trunc_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_full_flat_src_dict = {src: i for i, src in enumerate(map(tuple, traj_full_flat_src))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_space_binned(n_bins, groupby=None, return_data=False):\n",
    "    all_lengths = np.concatenate(lengths)\n",
    "    max_traj_length = all_lengths.max()\n",
    "\n",
    "    # bin word tokens by length\n",
    "    length_bins = pd.qcut(all_lengths, q=n_bins, labels=np.arange(n_bins), retbins=True)[1]\n",
    "    bin_time_edges = np.maximum(0, length_bins) / model_sfreq\n",
    "    bin_assignments = [np.digitize(traj_lengths, length_bins, right=True) - 1\n",
    "                       for traj_lengths in lengths]    \n",
    "    all_bin_assignments = np.concatenate(bin_assignments)\n",
    "    all_bin_assignments = all_bin_assignments[all_bin_assignments >= 0]\n",
    "\n",
    "    # key := bin + grouping variables\n",
    "    # build reverse map from key and frame index to list of (label_idx, instance_idx) tuples\n",
    "    bin_assignments_rev = defaultdict(list)\n",
    "    if groupby is not None:\n",
    "        group_lookup = metadata[groupby].to_dict()\n",
    "    for label_idx, assignments_i in enumerate(bin_assignments):\n",
    "        for j, bin_idx in enumerate(assignments_i):\n",
    "            if bin_idx < 0:\n",
    "                continue\n",
    "\n",
    "            if groupby is not None:\n",
    "                group_value = group_lookup[state_space_spec.labels[label_idx], j]\n",
    "                if group_value is None or (isinstance(group_value, float) and np.isnan(group_value)):\n",
    "                    continue\n",
    "                key = (bin_idx, group_value)\n",
    "            else:\n",
    "                key = (bin_idx,)\n",
    "            bin_assignments_rev[key].append((label_idx, j))\n",
    "\n",
    "    # Prepare per-frame and per-key means\n",
    "    bin_frame_means = defaultdict(list)\n",
    "    # optionally store all generating data\n",
    "    bin_frame_data = defaultdict(list)\n",
    "    bin_frame_src = defaultdict(list)\n",
    "    for key, traj_indices in tqdm(bin_assignments_rev.items(), desc=\"retrieving per-bin data\"):\n",
    "        for frame_idx in range(max_traj_length):\n",
    "            flat_idxs = [traj_full_flat_src_dict[(label_idx, instance_idx, frame_idx)]\n",
    "                        for label_idx, instance_idx in traj_indices\n",
    "                        if (label_idx, instance_idx, frame_idx) in traj_full_flat_src_dict]\n",
    "            \n",
    "            if len(flat_idxs) == 0:\n",
    "                bin_frame_means[key].append(np.full(traj_full_flat_pca.shape[1], np.nan))\n",
    "                bin_frame_data[key].append([])\n",
    "                bin_frame_src[key].append([])\n",
    "            else:\n",
    "                bin_frame_means[key].append(traj_full_flat_pca[flat_idxs].mean(axis=0))\n",
    "                bin_frame_data[key].append(traj_full_flat_pca[flat_idxs])\n",
    "                bin_frame_src[key].append(flat_idxs)\n",
    "\n",
    "    bin_frame_means = {key: np.array(frame_means_i) for key, frame_means_i in bin_frame_means.items()}\n",
    "\n",
    "    ## Plot\n",
    "    pcs = [[0, 1], [2, 3]]\n",
    "    f, axs = plt.subplots(1, len(pcs), figsize=(10 * len(pcs), 10), squeeze=False)\n",
    "\n",
    "    # bin_colors = sns.color_palette(\"tab10\", n_bins)\n",
    "    # get normalized continuous hue for bin edges\n",
    "    bin_colors = sns.color_palette(\"spring\", n_bins)\n",
    "    grouping_values = sorted(set(key[1:] for key in bin_frame_means.keys()))\n",
    "    grouping_styles = [\"-\", \"--\", \"-.\", \":\", (0, (3, 5, 1, 5)), (0, (3, 5, 1, 5, 1, 5))]\n",
    "    assert len(grouping_values) <= len(grouping_styles)\n",
    "\n",
    "    for i, (pcs_i, ax) in enumerate(zip(pcs, axs.flat)):\n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "        ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "        i, j = pcs_i\n",
    "        ax.set_xlabel(f\"PC {i+1}\")\n",
    "        ax.set_ylabel(f\"PC {j+1}\")\n",
    "\n",
    "        k = 0\n",
    "        for key in sorted(bin_frame_means.keys()):\n",
    "            bin_idx = key[0]\n",
    "            grouping_values_ij = key[1:]\n",
    "            means = bin_frame_means[key]\n",
    "            bin_edge = bin_time_edges[bin_idx]\n",
    "            ax.plot(means[:, i], means[:, j], label=f\"{key[1:]} {bin_edge:.2f} s\",\n",
    "                    color=bin_colors[bin_idx],\n",
    "                    linestyle=grouping_styles[grouping_values.index(grouping_values_ij)],\n",
    "                    alpha=0.7)\n",
    "\n",
    "            ax.quiver(means[:-1, i], means[:-1, j], means[1:, i] - means[:-1, i], means[1:, j] - means[:-1, j],\n",
    "                        angles='xy', scale_units='xy', scale=1, color=\"gray\", alpha=0.5)\n",
    "            \n",
    "            # O at start of word\n",
    "            word_start_frame = 0 + expand_window[0]\n",
    "            ax.scatter(means[word_start_frame, i], means[word_start_frame, j], color=\"blue\", marker=\"o\")\n",
    "\n",
    "            # X at middle of word\n",
    "            if np.isnan(means).any():\n",
    "                max_length = np.isnan(means).argmax(0).min() - 1\n",
    "            else:\n",
    "                max_length = means.shape[0]\n",
    "            word_midpoint_frame = (max_length - word_start_frame) // 2 + word_start_frame\n",
    "            ax.scatter(means[word_midpoint_frame, i], means[word_midpoint_frame, j], color=\"red\", marker=\"x\")\n",
    "\n",
    "        if i == len(pcs) - 1:\n",
    "            handles, labels = [], []\n",
    "            from matplotlib import patches as mpatches\n",
    "            from matplotlib.lines import Line2D\n",
    "            handles.append(mpatches.Patch(color=\"white\", label=\"\"))\n",
    "            labels.append(\"length bin\")\n",
    "            for bin in range(n_bins):\n",
    "                handles.append(mpatches.Patch(color=bin_colors[bin], label=f\"bin {bin}\"))\n",
    "                labels.append(f\"{bin_time_edges[bin]:.2f} s\")\n",
    "            handles.append(mpatches.Patch(color=\"white\", label=\"\"))\n",
    "            labels.append(groupby)\n",
    "            for group, style in zip(grouping_values, grouping_styles):\n",
    "                handles.append(Line2D([0], [0], color=\"black\", linestyle=style))\n",
    "                labels.append(group)\n",
    "            ax.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    if return_data:\n",
    "        return f, bin_frame_data, bin_frame_src, bin_time_edges\n",
    "    else:\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ss_data_all, ss_src_all, ss_edges_all = plot_state_space_binned(5, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ss_data_stress, ss_src_stress, ss_edges_stress = plot_state_space_binned(5, groupby=\"stress_primary_initial\", return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ss_data_freq, ss_src_freq, ss_edges_freq = plot_state_space_binned(5, groupby=\"word_frequency_quantile\", return_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_idxs_by_frame = []\n",
    "for frame_idx in range(traj_full_flat_src[:, 2].max() + 1):\n",
    "    traj_idxs_by_frame.append(np.where(traj_full_flat_src[:, 2] == frame_idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_means, frame_sds, frame_counts = [], [], []\n",
    "for frame_idx, traj_idxs_i in enumerate(traj_idxs_by_frame):\n",
    "    frame_means.append(traj_full_flat_pca[traj_idxs_i].mean(axis=0))\n",
    "    frame_sds.append(traj_full_flat_pca[traj_idxs_i].std(axis=0))\n",
    "    frame_counts.append(len(traj_idxs_i))\n",
    "\n",
    "frame_means = np.array(frame_means)\n",
    "frame_sds = np.array(frame_sds)\n",
    "frame_counts = np.array(frame_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "times = np.arange(-expand_window[0], frame_means.shape[0] - expand_window[0]) / model_sfreq\n",
    "ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "for component in range(frame_means.shape[1]):\n",
    "    ax.plot(times, frame_means[:, component], label=f\"PC {component+1}\")\n",
    "    ax.fill_between(times,\n",
    "                    frame_means[:, component] - frame_sds[:, component] / np.sqrt(frame_counts),\n",
    "                    frame_means[:, component] + frame_sds[:, component] / np.sqrt(frame_counts),\n",
    "                    alpha=0.3)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Distance from word boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"label_idx\"] = metadata.index.get_level_values(\"label\").map({label: idx for idx, label in enumerate(state_space_spec.labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_full_flat_src_dict = {src: i for i, src in enumerate(map(tuple, traj_full_flat_src))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_grouped(grouping_variable, num_components=4):\n",
    "    grouped_frame_means, grouped_frame_sds, grouped_frame_counts = {}, {}, {}\n",
    "    for group_values, rows in tqdm(metadata.reset_index().set_index([\"label_idx\", \"instance_idx\"]).groupby(grouping_variable)):\n",
    "        matched_word_instances = rows.index\n",
    "        \n",
    "        frame_means_i, frame_sds_i, frame_counts_i = [], [], []\n",
    "        for frame_idx in range(traj_full_flat_src[:, 2].max() + 1):\n",
    "            traj_idxs_i = [traj_full_flat_src_dict[(label_idx, instance_idx, frame_idx)] for label_idx, instance_idx in matched_word_instances\n",
    "                        if (label_idx, instance_idx, frame_idx) in traj_full_flat_src_dict]\n",
    "            if len(traj_idxs_i) == 0:\n",
    "                frame_means_i.append(np.full(traj_full_flat_pca.shape[1], np.nan))\n",
    "                frame_sds_i.append(np.full(traj_full_flat_pca.shape[1], np.nan))\n",
    "                frame_counts_i.append(0)\n",
    "            else:\n",
    "                frame_means_i.append(traj_full_flat_pca[traj_idxs_i].mean(axis=0))\n",
    "                frame_sds_i.append(traj_full_flat_pca[traj_idxs_i].std(axis=0))\n",
    "                frame_counts_i.append(len(traj_idxs_i))\n",
    "\n",
    "        grouped_frame_means[group_values] = np.array(frame_means_i)\n",
    "        grouped_frame_sds[group_values] = np.array(frame_sds_i)\n",
    "        grouped_frame_counts[group_values] = np.array(frame_counts_i)\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "\n",
    "    times = np.arange(-expand_window[0], frame_means.shape[0] - expand_window[0]) / model_sfreq\n",
    "    ax.axvline(0, color=\"gray\", linestyle=\"--\")\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "    component_palette = sns.color_palette(\"tab10\", frame_means.shape[1])\n",
    "    group_palette = [\"-\", \"--\", \":\"]\n",
    "    for i, group in enumerate(grouped_frame_means):\n",
    "        for component in range(min(num_components, frame_means.shape[1])):\n",
    "            ax.plot(times, grouped_frame_means[group][:, component],\n",
    "                    linestyle=group_palette[i], color=component_palette[component],\n",
    "                    label=f\"{group}-{component}\")\n",
    "            ax.fill_between(times,\n",
    "                            grouped_frame_means[group][:, component] - grouped_frame_sds[group][:, component] / np.sqrt(grouped_frame_counts[group]),\n",
    "                            grouped_frame_means[group][:, component] + grouped_frame_sds[group][:, component] / np.sqrt(grouped_frame_counts[group]),\n",
    "                            alpha=0.3)\n",
    "\n",
    "    handles, labels = [], []\n",
    "    from matplotlib import patches as mpatches\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles.append(mpatches.Patch(color=\"white\", label=\"\"))\n",
    "    labels.append(\"component\")\n",
    "    for component in range(min(num_components, frame_means.shape[1])):\n",
    "        handles.append(mpatches.Patch(color=component_palette[component], label=str(component + 1)))\n",
    "        labels.append(str(component + 1))\n",
    "    handles.append(mpatches.Patch(color=\"white\", label=\"\"))\n",
    "    labels.append(grouping_variable)\n",
    "    for i, group in enumerate(grouped_frame_means):\n",
    "        handles.append(Line2D([0], [0], color=\"black\", linestyle=group_palette[i]))\n",
    "        labels.append(group)\n",
    "    ax.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    return f, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plot_boundary_grouped(\"stress_primary_initial\")\n",
    "ax.axvline(4 / model_sfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plot_boundary_grouped(\"word_frequency_quantile\")\n",
    "ax.axvline(4 / model_sfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onset category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorization = {\n",
    "    \"consonant\": \"B CH D DH F G HH JH K L M N NG P R S SH T TH V W Y Z ZH\".split(\" \"),\n",
    "    \"vowel\": \"AA AE AH AO AW AY EH ER EY IH IY OW OY UH UW\".split(\" \"),\n",
    "}\n",
    "category_lookup = {label: category for category, labels in categorization.items() for label in labels}\n",
    "metadata[\"onset_phoneme_category\"] = metadata.onset_phoneme.map(category_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plot_boundary_grouped(\"onset_phoneme_category\")\n",
    "ax.axvline(4 / model_sfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_cluster_trunc = [traj_i[:, cluster_sample_idxs] for traj_i in traj_trunc]\n",
    "# traj_cluster_flat, traj_cluster_src = flatten_trajectory(traj_cluster_trunc)\n",
    "traj_cluster_trunc = traj_trunc\n",
    "traj_cluster_flat, traj_cluster_src = traj_trunc_flat, traj_trunc_flat_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_cluster = PCA(n_components=4)\n",
    "# traj_cluster_flat_pca = pca_cluster.fit_transform(traj_cluster_flat)\n",
    "pca_cluster = pca\n",
    "traj_cluster_flat_pca = traj_trunc_flat_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk points: wherever 1st or second column of traj_cluster_src changes\n",
    "chunk_points = np.where(np.any(np.diff(traj_cluster_src[:, :2], axis=0), axis=1))[0] + 1\n",
    "traj_cluster_word_level = np.split(traj_cluster_flat_pca, chunk_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cluster_word_level_src = np.split(traj_cluster_src, chunk_points)\n",
    "traj_cluster_word_level_src = np.array([src_i[0][:2] for src_i in traj_cluster_word_level_src])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(map(len, traj_cluster_word_level))\n",
    "traj_cluster_mat = np.zeros((len(traj_cluster_word_level), maxlen, traj_cluster_flat_pca.shape[1]))\n",
    "for i, traj_i in enumerate(tqdm(traj_cluster_word_level)):\n",
    "    traj_cluster_mat[i, :traj_i.shape[0], :] = traj_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline around center\n",
    "center_frame = traj_cluster_mat.shape[1] // 2\n",
    "baseline_data = traj_cluster_mat[:, center_frame - 4: center_frame + 4, :].mean(axis=1, keepdims=True)\n",
    "traj_cluster_mat -= baseline_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3, n_init=\"auto\")\n",
    "km.fit(traj_cluster_mat.reshape(traj_cluster_mat.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_.reshape((km.cluster_centers_.shape[0], *traj_cluster_mat.shape[1:]))\n",
    "\n",
    "# add baseline back in\n",
    "centroids += baseline_data.mean(0)[-1][None, None, :]\n",
    "\n",
    "f, axs = plt.subplots(1, centroids.shape[2], figsize=(6 * centroids.shape[2], 4))\n",
    "for i, (ax, centroid) in enumerate(zip(axs, centroids.transpose(2, 0, 1))):\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.set_title(f\"PCA {i + 1}\")\n",
    "    for j, c in enumerate(centroid):\n",
    "        ax.plot(trunc_times, c, label=f\"cluster {j}\")\n",
    "    ax.legend()\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "match_df = {}\n",
    "for km_label in range(len(km.cluster_centers_)):\n",
    "    matches = np.where(km.labels_ == km_label)[0]\n",
    "    matches_src = traj_cluster_word_level_src[matches]\n",
    "    matches_labels = [state_space_spec.labels[idx] for idx in matches_src[:, 0]]\n",
    "    match_df[km_label] = pd.DataFrame(Counter(matches_labels).items(), columns=[\"label\", \"count\"])\n",
    "match_df = pd.concat(match_df, names=[\"cluster\"]).droplevel(-1)\n",
    "cluster_df = pd.merge(match_df.reset_index(), metadata.groupby(\"label\").first().reset_index(), on=\"label\")\n",
    "cluster_df[\"word_log_frequency\"] = np.log10(cluster_df.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=cluster_df.groupby(\"cluster\")[\"word_frequency_quantile\"].value_counts(normalize=True).reset_index(),\n",
    "            hue=\"word_frequency_quantile\", x=\"cluster\", y=\"proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=cluster_df, x=\"cluster\", y=\"word_log_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=cluster_df.groupby(\"cluster\")[\"num_syllables\"].value_counts(normalize=True).reset_index(),\n",
    "            hue=\"num_syllables\", x=\"cluster\", y=\"proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.groupby(\"cluster\").onset_phoneme.value_counts(normalize=True).to_frame().unstack().T.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.groupby(\"cluster\").stress_primary_initial.value_counts(normalize=True).to_frame().unstack().T.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.groupby(\"cluster\").sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster entropy within word type\n",
    "word_cluster_entropy = cluster_df.groupby(\"label\").cluster.value_counts(normalize=True).groupby(\"label\").apply(lambda x: -np.sum(x * np.log2(x)))\n",
    "word_cluster_entropy.sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_entropy.sort_values().tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
