{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "# Add 4 frames prior to onset to each trajectory\n",
    "expand_frame_window = (4, 0)\n",
    "\n",
    "# Only use plot words for PCA or use whole vocabulary?\n",
    "pca_plot_words_only = False\n",
    "# Use words with this many or more instances to estimate embedding PCA space\n",
    "pca_freq_min = 15\n",
    "# Ignore words with this many or more instances when estimating embedding PCA space\n",
    "pca_freq_max = 10000\n",
    "\n",
    "# Use at most this many samples of each word in computing PCA (for computational efficiency)\n",
    "pca_max_samples_per_word = 100\n",
    "\n",
    "agg_method = \"mean\"\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.read_csv(\"data/SUBTLEXus74286wordstextversion.txt\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all words with frequency greater than cutoff to compute PCA\n",
    "word_freqs = {label: len(trajs) for trajs, label in\n",
    "            zip(state_space_spec.target_frame_spans, state_space_spec.labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all words with frequency between cutoffs to compute PCA\n",
    "pca_words = sorted([(freq, label) for label, freq in word_freqs.items()\n",
    "                    if freq >= pca_freq_min and freq < pca_freq_max], reverse=True)\n",
    "pca_words = [label for _, label in pca_words]\n",
    "\n",
    "drop_idxs = [idx for idx, word in enumerate(state_space_spec.labels)\n",
    "             if word not in pca_words]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample word instances\n",
    "state_space_spec = state_space_spec.subsample_instances(pca_max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "traj_agg = aggregate_state_trajectory(trajectory, state_space_spec, agg_method, keepdims=True)\n",
    "agg_flat, agg_src = flatten_trajectory(traj_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(Normalizer(), PCA(n_components=16))\n",
    "all_trajectories_pca = pipeline.fit_transform(agg_flat)\n",
    "\n",
    "pca = pipeline.named_steps[\"pca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot([0] + np.cumsum(pca.explained_variance_ratio_).tolist())\n",
    "ax.set_title(\"PCA explained variance\")\n",
    "ax.set_xlabel(\"Number of components\")\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_ylabel(\"Cumulative explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(\n",
    "    [(label_idx, instance_idx, *pca_coords)\n",
    "     for (label_idx, instance_idx, _), pca_coords\n",
    "     in zip(agg_src, all_trajectories_pca)],\n",
    "     columns=[\"label_idx\", \"instance_idx\"] + [f\"pca_{i}\" for i in range(pca.n_components_)])\n",
    "pca_df[\"label\"] = [state_space_spec.labels[label_idx] for label_idx in pca_df[\"label_idx\"]]\n",
    "pca_df = pca_df.set_index([\"label\", \"instance_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_phoneme = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").groupby([\"label\", \"instance_idx\"]).head(1).description.rename(\"first_phoneme\")\n",
    "last_phoneme = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").groupby([\"label\", \"instance_idx\"]).tail(1).description.rename(\"last_phoneme\")\n",
    "num_phonemes = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").groupby([\"label\", \"instance_idx\"]).size().rename(\"num_phonemes\")\n",
    "num_syllables = state_space_spec.cuts.xs(\"syllable\", level=\"level\").groupby([\"label\", \"instance_idx\"]).size().rename(\"num_syllables\")\n",
    "meta = pd.concat([first_phoneme, last_phoneme, num_phonemes, num_syllables], axis=1)\n",
    "pca_df = pd.merge(pca_df, meta, left_index=True, right_index=True)\n",
    "\n",
    "pca_df = pca_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.merge(pca_df, word_freq_df[\"Lg10WF\"], left_on=\"label\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df[\"log_frequency_bin\"] = pd.qcut(pca_df[\"Lg10WF\"], 10, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df[[f\"pca_{i}\" for i in range(pca.n_components_)] + [\"Lg10WF\", \"num_phonemes\", \"num_syllables\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df[\"hyp_0a\"] = pca_df.first_phoneme.isin((\"CH\", \"SH\", \"K\", \"P\", \"T\"))\n",
    "pca_df[\"hyp_0b\"] = pca_df.label.str.startswith((\"exp\", \"exe\", \"exa\", \"exc\", \"enc\", \"ext\", \"aca\"))\n",
    "pca_df[\"hyp_0\"] = pca_df.hyp_0a | pca_df.hyp_0b\n",
    "\n",
    "# pca_df[\"hyp_1a\"] = pca_df.last_phoneme.isin((\"T\", \"D\"))\n",
    "pca_df[\"hyp_1b\"] = pca_df.first_phoneme.isin((\"M\", \"N\", \"OY\", \"Y\", \"Z\"))\n",
    "pca_df[\"hyp_1c\"] = pca_df.last_phoneme.isin((\"Z\", \"JH\", \"ER\"))\n",
    "pca_df[\"hyp_1\"] = pca_df.hyp_1b | pca_df.hyp_1c\n",
    "\n",
    "pca_df[\"hyp_2\"] = pca_df.num_syllables  # better correlation than log-freq, and # phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = [col for col in pca_df.columns if col not in [\"label\", \"instance_idx\"] and not col.startswith(\"pca_\")]\n",
    "pca_type_df = pca_df.groupby([\"label\"] + meta_cols)[[f\"pca_{i}\" for i in range(pca.n_components_)]].agg([\"mean\", \"std\"]).reset_index()\n",
    "pca_type_df.columns = [\"_\".join(col).strip(\"_\") for col in pca_type_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pca_df, x=\"pca_0\", y=\"pca_1\", hue=\"hyp_0\", s=5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pca_type_df, x=\"pca_2_mean\", y=\"pca_3_mean\", hue=\"hyp_2\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between variance along a PC and word frequency\n",
    "ax = sns.lineplot(data=pca_df.groupby(\"log_frequency_bin\").apply(lambda xs: xs.groupby(\"label\")[[f\"pca_{i}\" for i in range(16)]].std().mean(axis=0)).reset_index().melt(id_vars=\"log_frequency_bin\"),\n",
    "             x=\"log_frequency_bin\", y=\"value\", hue=\"variable\")\n",
    "ax.set_title(\"Variance along PCA components by log frequency bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=pca_type_df, x=\"pca_0_mean\", hue=\"first_phoneme\", kind=\"ecdf\",\n",
    "            hue_order=pca_df.groupby(\"first_phoneme\").pca_0.mean().sort_values().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### study PC 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=pca_type_df, x=\"pca_1_mean\", hue=\"first_phoneme\", kind=\"ecdf\",\n",
    "            hue_order=pca_type_df.groupby(\"first_phoneme\").pca_1_mean.mean().sort_values().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.displot(data=pca_type_df, x=\"pca_1_mean\", hue=\"last_phoneme\", kind=\"ecdf\",\n",
    "            hue_order=pca_type_df.groupby(\"last_phoneme\").pca_1_mean.mean().sort_values().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pca_type_df, x=\"pca_0_mean\", y=\"pca_1_mean\", hue=\"hyp_1\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pca_type_df[(pca_type_df.pca_0_mean > 1) & (pca_type_df.pca_1_mean > 2)],\n",
    "                x=\"pca_0_mean\", y=\"pca_1_mean\", alpha=0.5)\n",
    "for label, row in pca_type_df[(pca_type_df.pca_0_mean > 1) & (pca_type_df.pca_1_mean > 2)].iterrows():\n",
    "    plt.text(row.pca_0_mean, row.pca_1_mean, row.label, fontsize=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
