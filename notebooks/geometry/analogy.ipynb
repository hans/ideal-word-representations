{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs = {agg_fn: aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "                   for agg_fn in tqdm(agg_fns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat = {k: flatten_trajectory(v) for k, v in trajectory_aggs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_analogy(triple, agg_method=\"mean\", num_samples=50, k=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - result_df: a df describing the k nearest neighbors to the analogy vector in each sample\n",
    "    - difference_vectors: the difference vectors used in the analogy for each sample\n",
    "    - analogy_vectors: the analogy vectors for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    word_a, word_b, word_c, expected = triple\n",
    "    assert word_a in state_space_spec.labels\n",
    "    assert word_b in state_space_spec.labels\n",
    "    assert word_c in state_space_spec.labels\n",
    "\n",
    "    # if the expected word isn't in the vocabulary, this isn't really interpretable\n",
    "    assert expected in state_space_spec.labels\n",
    "\n",
    "    word_a_idx = state_space_spec.labels.index(word_a)\n",
    "    word_b_idx = state_space_spec.labels.index(word_b)\n",
    "    word_c_idx = state_space_spec.labels.index(word_c)\n",
    "\n",
    "    # collect results of a - b\n",
    "    difference_vectors = []\n",
    "    # collect results of a - b + c\n",
    "    analogy_vectors = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        word_a_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_a_idx])))\n",
    "        word_b_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_b_idx])))\n",
    "        word_c_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_c_idx])))\n",
    "\n",
    "        word_a_traj = trajectory_aggs[agg_method][word_a_idx][word_a_instance].squeeze()\n",
    "        word_b_traj = trajectory_aggs[agg_method][word_b_idx][word_b_instance].squeeze()\n",
    "        word_c_traj = trajectory_aggs[agg_method][word_c_idx][word_c_instance].squeeze()\n",
    "\n",
    "        difference_vector = word_a_traj - word_b_traj\n",
    "        analogy_vector = difference_vector + word_c_traj\n",
    "\n",
    "        difference_vectors.append(difference_vector)\n",
    "        analogy_vectors.append(analogy_vector)\n",
    "\n",
    "    difference_vectors = np.array(difference_vectors)\n",
    "    analogy_vectors = np.array(analogy_vectors)\n",
    "\n",
    "    references, references_src = trajectory_aggs_flat[agg_method]\n",
    "    dists = cdist(analogy_vectors, references, metric=metric).mean(axis=0)\n",
    "    ranks = dists.argsort()\n",
    "\n",
    "    if verbose:\n",
    "        for dist, (label_idx, instance_idx, _) in zip(dists[ranks[:k]], references_src[ranks[:k]]):\n",
    "            print(dist, state_space_spec.labels[label_idx])\n",
    "\n",
    "    ret = pd.DataFrame(references_src[ranks[:k]], columns=[\"label_idx\", \"instance_idx\", \"frame_idx\"])\n",
    "    ret[\"distance\"] = dists[ranks[:k]]\n",
    "    ret[\"label\"] = [state_space_spec.labels[label_idx] for label_idx in ret[\"label_idx\"]]\n",
    "    return ret, difference_vectors, analogy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_dataset = datasets.load_dataset(\"relbert/analogy_questions\", \"bats\") \\\n",
    "    [\"test\"].filter(lambda x: \"morphology\" in x[\"prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_vectors = []\n",
    "prediction_results = []\n",
    "k = 20\n",
    "for item in tqdm(analogy_dataset):\n",
    "    b, a = item[\"stem\"]\n",
    "    c, d = item[\"choice\"][item[\"answer\"]]\n",
    "    \n",
    "    try:\n",
    "        ret, difference_vectors_i, _ = estimate_analogy((a, b, c, d), num_samples=100, k=k, verbose=False)\n",
    "    except AssertionError:\n",
    "        continue\n",
    "    \n",
    "    nearest_neighbor = ret.iloc[0].label\n",
    "    prediction_results.append(\n",
    "        {\"nearest_neighbor\": nearest_neighbor,\n",
    "         \"expected\": d,\n",
    "         \"correct\": nearest_neighbor == d,\n",
    "         \"correct_topk\": d in ret.iloc[:k].label,\n",
    "         \"correct_position\": ret[ret.label == d].index[0] if d in ret.label.values else None,\n",
    "         **item})\n",
    "    \n",
    "    difference_vectors.append({\"a\": a, \"b\": b, \"prefix\": item[\"prefix\"],\n",
    "                               \"difference_vectors\": difference_vectors_i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(prediction_results).drop(columns=[\"choice\"])\n",
    "results_df.to_csv(Path(output_dir) / \"analogy_results.csv\", index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = results_df.groupby(\"prefix\").correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")\n",
    "summary_df.to_csv(Path(output_dir) / \"analogy_summary.csv\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(difference_vectors, Path(output_dir) / \"analogy_difference_vectors.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
