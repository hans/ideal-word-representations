{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "from lemminflect import getInflection\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs = {agg_fn: aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "                   for agg_fn in tqdm(agg_fns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat = {k: flatten_trajectory(v) for k, v in trajectory_aggs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_analogy(triple, agg_method=\"mean\", num_samples=50, k=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - result_df: a df describing the k nearest neighbors to the analogy vector in each sample\n",
    "    - difference_vectors: the difference vectors used in the analogy for each sample\n",
    "    - analogy_vectors: the analogy vectors for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    word_a, word_b, word_c, expected = triple\n",
    "    assert word_a in state_space_spec.labels\n",
    "    assert word_b in state_space_spec.labels\n",
    "    assert word_c in state_space_spec.labels\n",
    "\n",
    "    # if the expected word isn't in the vocabulary, this isn't really interpretable\n",
    "    assert expected in state_space_spec.labels\n",
    "\n",
    "    word_a_idx = state_space_spec.labels.index(word_a)\n",
    "    word_b_idx = state_space_spec.labels.index(word_b)\n",
    "    word_c_idx = state_space_spec.labels.index(word_c)\n",
    "\n",
    "    # collect results of a - b\n",
    "    difference_vectors = []\n",
    "    # collect results of a - b + c\n",
    "    analogy_vectors = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        word_a_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_a_idx])))\n",
    "        word_b_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_b_idx])))\n",
    "        word_c_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[word_c_idx])))\n",
    "\n",
    "        word_a_traj = trajectory_aggs[agg_method][word_a_idx][word_a_instance].squeeze()\n",
    "        word_b_traj = trajectory_aggs[agg_method][word_b_idx][word_b_instance].squeeze()\n",
    "        word_c_traj = trajectory_aggs[agg_method][word_c_idx][word_c_instance].squeeze()\n",
    "\n",
    "        difference_vector = word_a_traj - word_b_traj\n",
    "        analogy_vector = difference_vector + word_c_traj\n",
    "\n",
    "        difference_vectors.append(difference_vector)\n",
    "        analogy_vectors.append(analogy_vector)\n",
    "\n",
    "    difference_vectors = np.array(difference_vectors)\n",
    "    analogy_vectors = np.array(analogy_vectors)\n",
    "\n",
    "    references, references_src = trajectory_aggs_flat[agg_method]\n",
    "    dists = cdist(analogy_vectors, references, metric=metric).mean(axis=0)\n",
    "    ranks = dists.argsort()\n",
    "\n",
    "    if verbose:\n",
    "        for dist, (label_idx, instance_idx, _) in zip(dists[ranks[:k]], references_src[ranks[:k]]):\n",
    "            print(dist, state_space_spec.labels[label_idx])\n",
    "\n",
    "    ret = pd.DataFrame(references_src[ranks[:k]], columns=[\"label_idx\", \"instance_idx\", \"frame_idx\"])\n",
    "    ret[\"distance\"] = dists[ranks[:k]]\n",
    "    ret[\"label\"] = [state_space_spec.labels[label_idx] for label_idx in ret[\"label_idx\"]]\n",
    "    return ret, difference_vectors, analogy_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial BATS study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_dataset = datasets.load_dataset(\"relbert/analogy_questions\", \"bats\") \\\n",
    "    [\"test\"].filter(lambda x: \"morphology\" in x[\"prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_vectors = []\n",
    "prediction_results = []\n",
    "k = 20\n",
    "for item in tqdm(analogy_dataset):\n",
    "    b, a = item[\"stem\"]\n",
    "    c, d = item[\"choice\"][item[\"answer\"]]\n",
    "    \n",
    "    try:\n",
    "        ret, difference_vectors_i, _ = estimate_analogy((a, b, c, d), num_samples=100, k=k, verbose=False)\n",
    "    except AssertionError:\n",
    "        continue\n",
    "    \n",
    "    nearest_neighbor = ret.iloc[0].label\n",
    "    prediction_results.append(\n",
    "        {\"nearest_neighbor\": nearest_neighbor,\n",
    "         \"expected\": d,\n",
    "         \"correct\": nearest_neighbor == d,\n",
    "         \"correct_topk\": d in ret.iloc[:k].label.tolist(),\n",
    "         \"correct_position\": ret[ret.label == d].index[0] if d in ret.label.values else None,\n",
    "         **item})\n",
    "    \n",
    "    difference_vectors.append({\"a\": a, \"b\": b, \"prefix\": item[\"prefix\"],\n",
    "                               \"difference_vectors\": difference_vectors_i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(prediction_results).drop(columns=[\"choice\"])\n",
    "results_df.to_csv(Path(output_dir) / \"analogy_results.csv\", index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = results_df.groupby(\"prefix\").correct.agg([\"count\", \"mean\"]).sort_values(\"mean\")\n",
    "summary_df.to_csv(Path(output_dir) / \"analogy_summary.csv\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(difference_vectors, Path(output_dir) / \"analogy_difference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homegrown study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_targets = [\n",
    "    (\"VBD\", \"verb_inf - Ved\"),\n",
    "    (\"VBZ\", \"verb_inf - 3pSg\"),\n",
    "    (\"VBG\", \"verb_inf - Ving\"),\n",
    "    (\"NNS\", \"noun - plural_reg\"),\n",
    "]\n",
    "labels = state_space_spec.label_counts\n",
    "labels = set(labels[labels > 15].index)\n",
    "\n",
    "inflection_results = {target: {} for target, _ in inflection_targets}\n",
    "inflection_reverse = defaultdict(set)\n",
    "for target, _ in tqdm(inflection_targets):\n",
    "    for label in labels:\n",
    "        label_inflections = set(getInflection(label, tag=target, inflect_oov=False))\n",
    "        # don't include zero-derived forms\n",
    "        label_inflections -= {label}\n",
    "\n",
    "        covered_inflections = label_inflections & labels\n",
    "        if covered_inflections:\n",
    "            inflection_results[target][label] = covered_inflections\n",
    "\n",
    "            for infl in covered_inflections:\n",
    "                inflection_reverse[infl].add((label, target))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({target: len(v) for target, v in inflection_results.items()})\n",
    "\n",
    "ambiguous_inflected_forms = {k: v for k, v in inflection_reverse.items()\n",
    "                             if len(v) > 1}\n",
    "print(f\"Ambiguous inflected forms ({len(ambiguous_inflected_forms)} total):\")\n",
    "print(\" \".join(ambiguous_inflected_forms.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_pairs = 200\n",
    "k = 5\n",
    "all_prediction_results = {}\n",
    "all_difference_vectors = {}\n",
    "for study_inflection, study_inflection_prefix in tqdm(inflection_targets):\n",
    "    # generate random pairs for analogies\n",
    "    candidate_bases = list(inflection_results[study_inflection].keys())\n",
    "    candidate_pairs = list(itertools.combinations(candidate_bases, 2))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(candidate_pairs)\n",
    "    candidate_pairs = candidate_pairs[:total_num_pairs]\n",
    "\n",
    "    difference_vectors = []\n",
    "    prediction_results = []\n",
    "    for (word1, word2) in tqdm(candidate_pairs, leave=False):\n",
    "        a = next(iter(inflection_results[study_inflection][word1]))\n",
    "        b = word1\n",
    "        c = word2\n",
    "        d = next(iter(inflection_results[study_inflection][word2]))\n",
    "        print(a, b, c, d)\n",
    "        \n",
    "        try:\n",
    "            ret, difference_vectors_i, _ = estimate_analogy((a, b, c, d), num_samples=100, k=k, verbose=False)\n",
    "        except AssertionError:\n",
    "            continue\n",
    "        \n",
    "        nearest_neighbor = ret.iloc[0].label\n",
    "        prediction_results.append(\n",
    "            {\"a\": a, \"b\": b, \"c\": c, \"d\": d,\n",
    "            \"d_pred\": nearest_neighbor,\n",
    "            \"correct\": nearest_neighbor == d,\n",
    "            \"correct_topk\": d in ret.iloc[:k].label.tolist(),\n",
    "            \"correct_position\": ret[ret.label == d].index[0] if d in ret.label.values else None})\n",
    "        \n",
    "        difference_vectors.append({\"a\": a, \"b\": b, \"c\": c, \"prefix\": study_inflection_prefix,\n",
    "                                   \"difference_vectors\": difference_vectors_i})\n",
    "\n",
    "    all_prediction_results[study_inflection] = pd.DataFrame(prediction_results)\n",
    "    all_difference_vectors[study_inflection] = difference_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_prediction_results, names=[\"inflection\"]).to_csv(f\"{output_dir}/broad_inflection_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_difference_vectors, Path(output_dir) / \"broad_inflection_difference_vectors.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
