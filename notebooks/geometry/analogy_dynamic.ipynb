{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the dynamics of morphemic processing and understand if/how they relate to the computed geometries from static word-level embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "from lemminflect import getInflection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fn = (\"mean_within_cut\", \"phoneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(max_samples_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load difference vectors for comparison\n",
    "difference_vectors = torch.load(f\"outputs/notebooks/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/geometry/analogy/analogy_difference_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_difference_vectors = np.concatenate([x['difference_vectors'] for x in difference_vectors if \"noun - plural_reg\" in x[\"prefix\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_agg = aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "agg_flat, agg_src = flatten_trajectory(traj_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = state_space_spec.cuts.xs(\"phoneme\", level=\"level\").drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\").map({l: i for i, l in enumerate(state_space_spec.labels)})\n",
    "cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "\n",
    "agg_flat_idxs = pd.Series({tuple(agg_src_i): i for i, agg_src_i in enumerate(agg_src)})\n",
    "agg_flat_idxs.index.names = [\"label_idx\", \"instance_idx\", \"frame_idx\"]\n",
    "cuts_df = pd.merge(cuts_df, agg_flat_idxs.rename(\"traj_flat_idx\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_df = cuts_df.set_index(\"label\", append=True).reorder_levels([\"label\", \"label_idx\", \"instance_idx\", \"frame_idx\"]).sort_index()\n",
    "cuts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection_targets = [\n",
    "    (\"VBD\", \"verb_inf - Ved\"),\n",
    "    (\"VBZ\", \"verb_inf - 3pSg\"),\n",
    "    (\"VBG\", \"verb_inf - Ving\"),\n",
    "    (\"NNS\", \"noun - plural_reg\"),\n",
    "]\n",
    "labels = state_space_spec.label_counts\n",
    "labels = set(labels[labels > 15].index)\n",
    "\n",
    "inflection_results = {target: {} for target, _ in inflection_targets}\n",
    "inflection_reverse = defaultdict(set)\n",
    "for target, _ in tqdm(inflection_targets):\n",
    "    for label in labels:\n",
    "        label_inflections = set(getInflection(label, tag=target, inflect_oov=False))\n",
    "        # don't include zero-derived forms\n",
    "        label_inflections -= {label}\n",
    "\n",
    "        covered_inflections = label_inflections & labels\n",
    "        if covered_inflections:\n",
    "            inflection_results[target][label] = covered_inflections\n",
    "\n",
    "            for infl in covered_inflections:\n",
    "                inflection_reverse[infl].add((label, target))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({target: len(v) for target, v in inflection_results.items()})\n",
    "\n",
    "ambiguous_inflected_forms = {k: v for k, v in inflection_reverse.items()\n",
    "                             if len(v) > 1}\n",
    "print(f\"Ambiguous inflected forms ({len(ambiguous_inflected_forms)} total):\")\n",
    "print(\" \".join(ambiguous_inflected_forms.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inflection_study(study_inflection, study_inflection_difference_vector_prefix):\n",
    "    study_triples, study_metadata = defaultdict(list), {}\n",
    "    for base_form, inflected_forms in tqdm(inflection_results[study_inflection].items()):\n",
    "        for inflected_form in inflected_forms:\n",
    "            is_regular = inflected_form[:len(base_form)] == base_form \\\n",
    "                or inflected_form[-3:] == \"ies\" and base_form[-1] == \"y\" \\\n",
    "                or inflected_form[-3:] == \"ves\" and (base_form[-1] == \"f\" or base_form[-2:] == \"fe\")\n",
    "            \n",
    "            # orthographic divergence point\n",
    "            ortho_divergence_point = sum(1 for idx in range(min(len(base_form), len(inflected_form)))\n",
    "                                        if inflected_form[:idx] == base_form[:idx])\n",
    "            \n",
    "            base_cuts = cuts_df.loc[base_form]\n",
    "            inflected_cuts = cuts_df.loc[inflected_form]\n",
    "\n",
    "            # all attested phonological forms of base\n",
    "            base_phono_forms = set(base_cuts.groupby(\"instance_idx\").apply(\n",
    "                lambda xs: tuple(xs.description)))\n",
    "\n",
    "            for instance_idx, inflected_instance in inflected_cuts.groupby(\"instance_idx\"):\n",
    "                # phonological divergence point: latest point at which the inflected form overlaps with\n",
    "                # any pronunciation of the base form\n",
    "                inflected_phones = tuple(inflected_instance.description)\n",
    "                phono_divergence_points = []\n",
    "                for base_phones in base_phono_forms:\n",
    "                    for idx in range(len(inflected_phones) + 1):\n",
    "                        if inflected_phones[:idx] != base_phones[:idx]:\n",
    "                            break\n",
    "                    phono_divergence_points.append(idx - 1)\n",
    "                phono_divergence_point = max(phono_divergence_points)\n",
    "\n",
    "                # print(f\"{base_phono_forms} -> {inflected_phones} (regular: {is_regular}, ortho_divergence: {ortho_divergence_point}, phono_divergence: {phono_divergence_point})\")\n",
    "\n",
    "                if phono_divergence_point == 0:\n",
    "                    print(f\"Ignoring {base_form} -> {inflected_form} due to phono_divergence_point == 0\")\n",
    "                    continue\n",
    "                pre_diverging_frame = inflected_instance.iloc[phono_divergence_point - 1].traj_flat_idx\n",
    "                diverging_frame = inflected_instance.iloc[phono_divergence_point].traj_flat_idx\n",
    "                final_frame = inflected_instance.iloc[-1].traj_flat_idx\n",
    "                \n",
    "                study_triples[base_form].append((pre_diverging_frame, diverging_frame, final_frame))\n",
    "                if base_form not in study_metadata:\n",
    "                    study_metadata[base_form] = {\n",
    "                        \"label\": base_form,\n",
    "                        \"is_regular\": is_regular,\n",
    "                        \"post_divergence\": Counter([inflected_phones[phono_divergence_point:]]),\n",
    "                    }\n",
    "                else:\n",
    "                    study_metadata[base_form][\"post_divergence\"].update([inflected_phones[phono_divergence_point:]])\n",
    "\n",
    "    # for each base -> inflected, get average in each of the three states\n",
    "    study_triple_means = []\n",
    "    study_triple_metadata = []\n",
    "    for label, label_triples in tqdm(study_triples.items()):\n",
    "        study_triple_means.append(agg_flat[label_triples].mean(axis=0))\n",
    "\n",
    "        label_metadata = study_metadata[label]\n",
    "        # get most common post-divergence phonological form\n",
    "        label_metadata[\"post_divergence\"] = \" \".join(label_metadata[\"post_divergence\"].most_common(1)[0][0])\n",
    "        study_triple_metadata.append(label_metadata)\n",
    "\n",
    "    study_triple_metadata = pd.DataFrame(study_triple_metadata)\n",
    "\n",
    "    # get num_lemmata * 3 * model_dim representations\n",
    "    inflected_states = np.stack(study_triple_means)\n",
    "    inflected_states.shape\n",
    "\n",
    "    assert len(study_triple_metadata) == len(inflected_states)\n",
    "\n",
    "    #######\n",
    "\n",
    "    inflection_updates = inflected_states[:, 2, :] - inflected_states[:, 0, :]\n",
    "\n",
    "    regular_inflected_states = inflected_states[study_triple_metadata[study_triple_metadata[\"is_regular\"]].index]\n",
    "    regular_inflection_updates = regular_inflected_states[:, 2, :] - regular_inflected_states[:, 0, :]\n",
    "\n",
    "    reference_difference_vectors = np.stack([x[\"difference_vectors\"].mean(0) for x in difference_vectors\n",
    "                                                   if study_inflection_difference_vector_prefix in x[\"prefix\"]])\n",
    "    counterfactual_difference_vectors = np.stack([x['difference_vectors'].mean(0) for x in difference_vectors\n",
    "                                                        if study_inflection_difference_vector_prefix not in x[\"prefix\"]])\n",
    "\n",
    "    return inflection_updates, regular_inflection_updates, \\\n",
    "        reference_difference_vectors, counterfactual_difference_vectors, \\\n",
    "        study_triple_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_results = {}\n",
    "study_results_df = {}\n",
    "infl_updates = {}\n",
    "\n",
    "for target, prefix in tqdm(inflection_targets):\n",
    "    infl_updates[target], infl_updates_regular, reference_diff_vectors, reference_diff_vectors_counterfactual, metadata = \\\n",
    "        run_inflection_study(target, prefix)\n",
    "\n",
    "    study_results_df[target] = {\n",
    "        \"within_inflection\": cdist(infl_updates[target], infl_updates[target], metric=metric).mean(),\n",
    "        \"within_inflection_regular\": cdist(infl_updates_regular, infl_updates_regular, metric=metric).mean(),\n",
    "        \"reference_diff\": cdist(infl_updates[target], reference_diff_vectors, metric=metric).mean(),\n",
    "        \"reference_diff_counterfactual\": cdist(infl_updates[target], reference_diff_vectors_counterfactual, metric=metric).mean(),\n",
    "    }\n",
    "\n",
    "    study_results[target] = {\n",
    "        \"infl_updates\": infl_updates[target],\n",
    "        \"metadata\": metadata,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have vectors from all inflections computed, estimate between-inflection distances\n",
    "for target in study_results_df:\n",
    "    study_results_df[target][\"between_inflection\"] = cdist(infl_updates[target], np.concatenate([infl_updates[t] for t in study_results if t != target]), metric=metric).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_results_df = pd.DataFrame.from_dict(study_results_df, orient=\"index\")\n",
    "study_results_df.to_csv(f\"{output_dir}/inflection_study.csv\")\n",
    "study_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=study_results_df.reset_index().melt(id_vars=[\"index\"]),\n",
    "            x=\"index\", y=\"value\", hue=\"variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNS sub-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_updates, nns_metadata = study_results[\"NNS\"][\"infl_updates\"], study_results[\"NNS\"][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_metadata.post_divergence.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_divergence_categories = [\"Z\", \"S\", \"IH Z\"]\n",
    "nns_category_updates = {category: nns_updates[nns_metadata[nns_metadata.post_divergence == category].index]\n",
    "                        for category in compare_divergence_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_distances_within = {category: cdist(nns_category_updates[category], nns_category_updates[category], metric=metric).mean()\n",
    "                        for category in compare_divergence_categories}\n",
    "nns_distances_between = {category: cdist(nns_category_updates[category], np.concatenate([nns_category_updates[c] for c in compare_divergence_categories if c != category]), metric=metric).mean()\n",
    "                         for category in compare_divergence_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_study_df = pd.DataFrame({\n",
    "    \"distance_within\": nns_distances_within,\n",
    "    \"distance_between\": nns_distances_between,\n",
    "})\n",
    "nns_study_df[\"distance_counterfactual\"] = cdist(nns_updates, reference_diff_vectors_counterfactual, metric=metric).mean()\n",
    "nns_study_df.to_csv(f\"{output_dir}/nns_study.csv\")\n",
    "nns_study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=nns_study_df.reset_index().melt(id_vars=\"index\"), x=\"index\", y=\"value\", hue=\"variable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
