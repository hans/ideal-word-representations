{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import coherence\n",
    "from src.analysis.state_space import prepare_state_trajectory, StateSpaceAnalysisSpec\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/timit/w2v2_6/rnn_8/phoneme\"\n",
    "output_dir = \"outputs/notebooks/timit/w2v2_6/rnn_8/phoneme/plot\"\n",
    "dataset_path = \"outputs/preprocessed_data/timit\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/timit/w2v2_6/phoneme/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/timit/w2v2_6/hidden_states.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/timit/w2v2_6/state_space_specs.h5\"\n",
    "embeddings_path = \"outputs/model_embeddings/timit/w2v2_6/rnn_8/phoneme/embeddings.npy\"\n",
    "\n",
    "output_dir = \".\"\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Retain phonemes with N or more instances\n",
    "retain_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"phoneme\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idxs = [idx for idx, target_frames in enumerate(state_space_spec.target_frame_spans)\n",
    "               if len(target_frames) < retain_n]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate within-phoneme distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance, within_distance_offset = \\\n",
    "    coherence.estimate_within_distance(trajectory, lengths, state_space_spec, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(within_distance, center=1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_df = pd.DataFrame(within_distance, index=pd.Index(state_space_spec.labels, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_offset_df = pd.DataFrame(within_distance_offset, index=pd.Index(state_space_spec.labels, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate between-phoneme distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distance, between_distance_offset = \\\n",
    "    coherence.estimate_between_distance(trajectory, lengths, state_space_spec, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_df = pd.DataFrame(np.nanmean(between_distance, axis=-1),\n",
    "                                    index=pd.Index(state_space_spec.labels, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_offset_df = pd.DataFrame(np.nanmean(between_distance_offset, axis=-1),\n",
    "                                    index=pd.Index(state_space_spec.labels, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([within_distance_df.assign(type=\"within\"), between_distances_df.assign(type=\"between\")])\n",
    "merged_df.to_csv(Path(output_dir) / \"distances.csv\", index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=merged_df.dropna(), x=\"frame\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance within- and between-phoneme\")\n",
    "ax.set_xlabel(\"Frames since phoneme onset\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_offset_df = pd.concat([within_distance_offset_df.assign(type=\"within\"), between_distances_offset_df.assign(type=\"between\")])\n",
    "merged_offset_df.to_csv(Path(output_dir) / \"distances_aligned_offset.csv\", index=False)\n",
    "merged_offset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=merged_offset_df.dropna(), x=\"frame\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance within- and between-phoneme\")\n",
    "ax.set_xlabel(\"Frames before phoneme offset\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block by phoneme categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorization = {\n",
    "    \"consonant\": \"B CH D DH F G HH JH K L M N NG P R S SH T TH V W Y Z ZH\".split(\" \"),\n",
    "    \"vowel\": \"AA AE AH AO AW AY EH ER EY IH IY OW OY UH UW\".split(\" \"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = trajectory[0].shape[1]\n",
    "\n",
    "for phoneme_list in categorization.values():\n",
    "    for phoneme in phoneme_list:\n",
    "        assert phoneme in state_space_spec.labels, f\"Phoneme {phoneme} missing from state space spec\"\n",
    "\n",
    "# Prepare balanced sample of representations for each phoneme in each category\n",
    "num_instances = min(len(state_space_spec.target_frame_spans[i]) for i in range(len(state_space_spec.labels)))\n",
    "# HACK this is just to make the number of instnaces different than the number of frames, to make sure I don't make debuggnig mistakes\n",
    "num_instances -= 1\n",
    "\n",
    "all_phonemes = sorted(set(itertools.chain.from_iterable(categorization.values())))\n",
    "phoneme_representations, phoneme_representation_lengths = {}, {}\n",
    "for phoneme in all_phonemes:\n",
    "    sample_instance_idxs = np.random.choice(len(state_space_spec.target_frame_spans[state_space_spec.labels.index(phoneme)]),\n",
    "                                            num_instances, replace=False)\n",
    "    phoneme_representations[phoneme] = np.array([trajectory[state_space_spec.labels.index(phoneme)][idx]\n",
    "                                                 for idx in sample_instance_idxs])\n",
    "    phoneme_representation_lengths[phoneme] = lengths[state_space_spec.labels.index(phoneme)][sample_instance_idxs]\n",
    "\n",
    "# Compute between-phoneme distances\n",
    "distances = np.zeros((len(all_phonemes), len(all_phonemes), trajectory[0].shape[1]))\n",
    "for p1, p2 in itertools.product(list(range(len(all_phonemes))), repeat=2):\n",
    "    for k in range(num_frames):\n",
    "        mask1 = phoneme_representation_lengths[all_phonemes[p1]] >= k\n",
    "        mask2 = phoneme_representation_lengths[all_phonemes[p2]] >= k\n",
    "        if mask1.sum() == 0 or mask2.sum() == 0:\n",
    "            break\n",
    "\n",
    "        distances[p1, p2, k] = coherence.get_mean_distance(phoneme_representations[all_phonemes[p1]][mask1, k, :],\n",
    "                                                           phoneme_representations[all_phonemes[p2]][mask2, k, :], metric=metric)\n",
    "        \n",
    "# Compute between- and within-category distance trajectory\n",
    "within_distances, between_distances, within_comparisons, between_comparisons = {}, {}, {}, {}\n",
    "for category, phonemes in categorization.items():\n",
    "    within_comparisons[category] = list(itertools.combinations(phonemes, 2))\n",
    "    between_comparisons[category] = [(p1, p2) for p1, p2 in itertools.product(phonemes, all_phonemes) if p1 in phonemes and p2 not in phonemes]\n",
    "    within_distances[category] = np.stack([distances[all_phonemes.index(p1), all_phonemes.index(p2)] for p1, p2 in within_comparisons[category]], axis=0)\n",
    "    between_distances[category] = np.stack([distances[all_phonemes.index(p1), all_phonemes.index(p2)] for p1, p2 in between_comparisons[category]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_within_distances = pd.concat(\n",
    "    {category: pd.DataFrame(within_distances[category], index=pd.Index(within_comparisons[category], name=(\"p1\", \"p2\")), columns=pd.Index(range(num_frames), name=\"frame\")) \\\n",
    "                .melt(ignore_index=False, var_name=\"frame\", value_name=\"distance\")\n",
    "     for category in categorization},\n",
    "    names=[\"category\"]\n",
    ")\n",
    "\n",
    "all_between_distances = pd.concat(\n",
    "    {category: pd.DataFrame(between_distances[category], index=pd.Index(between_comparisons[category], name=(\"p1\", \"p2\")), columns=pd.Index(range(num_frames), name=\"frame\")) \\\n",
    "                .melt(ignore_index=False, var_name=\"frame\", value_name=\"distance\")\n",
    "     for category in categorization},\n",
    "    names=[\"category\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = pd.concat([all_within_distances, all_between_distances], keys=[\"within\", \"between\"], names=[\"type\"])\n",
    "all_distances.to_csv(Path(output_dir) / \"grouped_distances.csv\")\n",
    "all_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=all_distances.reset_index(), x=\"frame\", y=\"distance\", hue=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(pd.DataFrame(np.nanmean(distances, -1), index=all_phonemes, columns=all_phonemes),\n",
    "               center=1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
