{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.analysis import coherence\n",
    "from src.analysis.pwcca import solve_cca\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset\n",
    "from src.utils import ndarray_to_long_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/librispeech-train-clean-100/w2v2_8/rnn_32-aniso3/word_broad_10frames\"\n",
    "output_dir = \".\"\n",
    "dataset_path = \"outputs/preprocessed_data/librispeech-train-clean-100\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/librispeech-train-clean-100/w2v2_8/word_broad_10frames/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/librispeech-train-clean-100/w2v2_8/librispeech-train-clean-100.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/librispeech-train-clean-100/w2v2_8/state_space_specs.pkl\"\n",
    "embeddings_path = \"outputs/model_embeddings/librispeech-train-clean-100/w2v2_8/rnn_32-aniso3/word_broad_10frames/librispeech-train-clean-100.npy\"\n",
    "\n",
    "# name -> (agg_spec, length_grouping_level)\n",
    "# CCA will be estimated and evaluated on words within length groups; the unit of this length count\n",
    "# is determined by `length_grouping_level`. This is because it makes more sense to talk about syllable-by-syllable\n",
    "# representation within words matched in syllable count.\n",
    "# The `length_grouping_level` should correspond to a `level` in the state space spec cuts.\n",
    "agg_methods = {\n",
    "    \"mean_within_phoneme\": ((\"mean_within_cut\", \"phoneme\"), \"phoneme\"),\n",
    "    \"mean_within_syllable\": ((\"mean_within_cut\", \"syllable\"), \"syllable\"),\n",
    "    \"mean\": (\"mean\", \"phoneme\"),\n",
    "    \"last_frame\": (\"last_frame\", \"phoneme\"),\n",
    "    \"max\": (\"max\", \"phoneme\"),\n",
    "    \"none\": (None, \"phoneme\"),\n",
    "}\n",
    "\n",
    "# Keep just the K most frequent words\n",
    "k = 500\n",
    "\n",
    "# Keep at most N instances of each word\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the K most frequent words\n",
    "state_space_spec_small = state_space_spec.keep_top_k(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep at most N instances per word\n",
    "state_space_spec_small = state_space_spec_small.subsample_instances(n, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec_small, pad=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cca(trajectory, state_space_spec, agg_method, cv=4):\n",
    "    \"\"\"\n",
    "    Evaluate CCA alignment between model representations and one-hot word embeddings.\n",
    "    \"\"\"\n",
    "    if agg_method is not None:\n",
    "        trajectory_agg = aggregate_state_trajectory(trajectory, state_space_spec, agg_method, keepdims=True)\n",
    "    else:\n",
    "        trajectory_agg = trajectory\n",
    "    flat_traj, flat_traj_src = flatten_trajectory(trajectory_agg)\n",
    "\n",
    "    # Z-score\n",
    "    flat_traj = (flat_traj - flat_traj.mean(0)) / flat_traj.std(0)\n",
    "\n",
    "    # Target values\n",
    "    Y = np.zeros((len(flat_traj), k), dtype=int)\n",
    "    Y[np.arange(len(flat_traj)), flat_traj_src[:, 0]] = 1\n",
    "\n",
    "    cv = KFold(cv, shuffle=True) if isinstance(cv, int) else cv\n",
    "    # NB here \"frame\" depends on the aggregation method; this may correspond to a model frame,\n",
    "    # phoneme, syllable, etc.\n",
    "    max_num_frames = flat_traj_src[:, 2].max() + 1\n",
    "\n",
    "    # store the images of all instances in the aligned space\n",
    "    # keys are (frame_idx, fold_idx)\n",
    "    cca_images = {}\n",
    "    cca_scores = np.zeros((max_num_frames, cv.get_n_splits(), 4)) * np.nan\n",
    "    for frame_idx in trange(max_num_frames, desc=\"Estimating CCA\", unit=\"frame\", leave=False):\n",
    "        sample_idxs = np.where(flat_traj_src[:, 2] == frame_idx)[0]\n",
    "        if len(sample_idxs) / cv.get_n_splits() < flat_traj.shape[1]:\n",
    "            # Not enough samples\n",
    "            continue\n",
    "\n",
    "        for fold_idx, (train_idxs, test_idxs) in enumerate(cv.split(sample_idxs)):\n",
    "            x_src = flat_traj_src[sample_idxs[train_idxs]]\n",
    "            x, y = flat_traj[sample_idxs[train_idxs]].T, Y[sample_idxs[train_idxs]].T\n",
    "            try:\n",
    "                cca = solve_cca(x, y)\n",
    "            except AssertionError:\n",
    "                continue\n",
    "            else:\n",
    "                cca_scores[frame_idx, fold_idx, 0] = cca[\"pwcca_sim_x\"]\n",
    "                cca_scores[frame_idx, fold_idx, 1] = cca[\"pwcca_sim_y\"]\n",
    "                cca_scores[frame_idx, fold_idx, 2] = cca[\"ewcca_sim_x\"]\n",
    "                cca_scores[frame_idx, fold_idx, 3] = cca[\"ewcca_sim_y\"]\n",
    "\n",
    "                cca_images[frame_idx, fold_idx] = cca[\"cca_pos_x\"] @ flat_traj.T\n",
    "\n",
    "    cca_scores_df = ndarray_to_long_dataframe(cca_scores, [\"frame_idx\", \"fold_idx\", \"measure\"]).reset_index()\n",
    "    cca_scores_df[\"measure\"] = cca_scores_df[\"measure\"].map({0: \"pw_x\", 1: \"pw_y\", 2: \"ew_x\", 3: \"ew_y\"})\n",
    "\n",
    "    return flat_traj, flat_traj_src, cca_scores_df, cca_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this analysis grouped by # of phonemes in word tokens\n",
    "state_space_spec_small.cuts[\"grouping_value\"] = \\\n",
    "    state_space_spec_small.cuts.xs(\"phoneme\", level=\"level\").groupby([\"label\", \"instance_idx\"]).size()\n",
    "\n",
    "for name, (agg_spec, grouping_level) in tqdm(agg_methods.items(), unit=\"method\"):\n",
    "    state_space_spec_small.cuts[\"grouping_value\"] = state_space_spec_small.cuts \\\n",
    "        .xs(grouping_level, level=\"level\") \\\n",
    "        .groupby([\"label\", \"instance_idx\"]).size()\n",
    "    total_num_groups = state_space_spec_small.cuts[\"grouping_value\"].nunique()\n",
    "\n",
    "    for length, group in tqdm(state_space_spec_small.groupby(\"grouping_value\"), total=total_num_groups, unit=\"length group\", leave=False):\n",
    "        trajectory = prepare_state_trajectory(model_representations, group, pad=np.nan)\n",
    "        flat_traj, flat_traj_src, cca_scores_df, cca_images = evaluate_cca(trajectory, group, agg_spec, cv=5)\n",
    "        cca_scores_df.to_csv(f\"{output_dir}/cca_scores-{name}-len{length}.csv\", index=False)\n",
    "        # with open(f\"{output_dir}/cca_images-{name}-len{length}.pkl\", \"wb\") as f:\n",
    "        #     pickle.dump(cca_images, f)\n",
    "\n",
    "        cca_scores_df = cca_scores_df.dropna()\n",
    "        if cca_scores_df.empty:\n",
    "            # there is no hope\n",
    "            continue\n",
    "        max_num_frames = cca_scores_df[\"frame_idx\"].max() + 1\n",
    "        min_value = min(0.5, cca_scores_df[\"value\"].min())\n",
    "        max_value = cca_scores_df[\"value\"].max()\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(12, 6))\n",
    "        if max_num_frames > 1:\n",
    "            sns.lineplot(data=cca_scores_df, x=\"frame_idx\", y=\"value\", hue=\"measure\", ax=ax)\n",
    "            ax.set_title(f\"CCA alignment scores (aggregation: {name}; max {grouping_level} length: {length})\")\n",
    "            ax.set_xlabel(\"Frame index\")\n",
    "            ax.set_ylim((min_value, max_value))\n",
    "        else:\n",
    "            sns.barplot(data=cca_scores_df, x=\"measure\", y=\"value\", ax=ax)\n",
    "            ax.set_title(f\"CCA alignment scores ({name})\")\n",
    "            ax.set_ylim((min_value, max_value))\n",
    "        f.savefig(Path(output_dir) / f\"cca_scores-{name}-len{length}.png\")\n",
    "\n",
    "        # plot PCA of resulting image space for a spectrum of frames\n",
    "        num_plots = 5\n",
    "        # pick a random fold\n",
    "        fold_idx = np.random.randint(cca_scores_df.fold_idx.max())\n",
    "        # pick random words to sample\n",
    "        plot_sample_idxs = np.random.choice(len(flat_traj), min(100, len(flat_traj)), replace=False)\n",
    "        frame_points = np.unique(np.linspace(0, max_num_frames - 1, num_plots, dtype=int))\n",
    "\n",
    "        for frame_idx in frame_points:\n",
    "            cca_image_i = cca_images[frame_idx, fold_idx]\n",
    "            pca = PCA(2).fit(cca_image_i.T)\n",
    "\n",
    "            plot_points = pca.transform(cca_image_i[:, plot_sample_idxs].T)\n",
    "            plot_label_idxs = flat_traj_src[plot_sample_idxs, 0]\n",
    "            \n",
    "            f, ax = plt.subplots(figsize=(12, 12))\n",
    "            ax.scatter(*plot_points.T)\n",
    "            ax.set_title(f\"PCA of CCA image space (aggregation: {name},\\nmax {grouping_level} length: {length}; frame {frame_idx})\")\n",
    "            for i, label_idx in enumerate(plot_label_idxs):\n",
    "                ax.text(*plot_points[i], state_space_spec.labels[label_idx], fontsize=8)\n",
    "\n",
    "            f.savefig(Path(output_dir) / f\"pca_image-{name}-len{length}-frame{frame_idx}.png\")\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
