{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.pkl\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "# Add 4 frames prior to onset to each trajectory\n",
    "expand_frame_window = (4, 0)\n",
    "\n",
    "# Only use plot words for PCA or use whole vocabulary?\n",
    "pca_plot_words_only = False\n",
    "# Use words with this many or more instances to estimate embedding PCA space\n",
    "pca_freq_min = 15\n",
    "# Ignore words with this many or more instances when estimating embedding PCA space\n",
    "pca_freq_max = 10000\n",
    "\n",
    "# Use at most this many samples of each word in computing PCA (for computational efficiency)\n",
    "pca_max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"word\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all words with frequency greater than cutoff to compute PCA\n",
    "word_freqs = {label: len(trajs) for trajs, label in\n",
    "            zip(state_space_spec.target_frame_spans, state_space_spec.labels)}\n",
    "\n",
    "min_suffix_type_count = 10\n",
    "min_prefix_type_count = 10\n",
    "plot_suffixes = 3\n",
    "plot_prefixes = 3\n",
    "\n",
    "# find longest possible suffixes for which we have at least min_suffix_type_count word types matching frequency constraint\n",
    "best_suffixes = []\n",
    "for suffix_length in range(9):\n",
    "    suffix_counts = Counter()\n",
    "    for word in state_space_spec.labels:\n",
    "        if len(word) < suffix_length or word_freqs[word] < pca_freq_min or word_freqs[word] >= pca_freq_max:\n",
    "            continue\n",
    "        suffix_counts[word[-suffix_length:]] += 1\n",
    "\n",
    "    for suffix, count in suffix_counts.most_common():\n",
    "        if count < min_suffix_type_count:\n",
    "            break\n",
    "        best_suffixes.append((suffix_length, count, suffix))\n",
    "best_suffixes = sorted(best_suffixes, reverse=True)[:plot_suffixes]\n",
    "\n",
    "# same for prefixes\n",
    "best_prefixes = []\n",
    "for prefix_length in range(9):\n",
    "    prefix_counts = Counter()\n",
    "    for word in state_space_spec.labels:\n",
    "        if len(word) < prefix_length or word_freqs[word] < pca_freq_min or word_freqs[word] >= pca_freq_max:\n",
    "            continue\n",
    "        prefix_counts[word[:prefix_length]] += 1\n",
    "\n",
    "    for prefix, count in prefix_counts.most_common():\n",
    "        if count < min_prefix_type_count:\n",
    "            break\n",
    "        best_prefixes.append((prefix_length, count, prefix))\n",
    "best_prefixes = sorted(best_prefixes, reverse=True)[:plot_prefixes]\n",
    "\n",
    "print(\"Best prefixes\", best_prefixes)\n",
    "print(\"Best suffixes\", best_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sets = {\n",
    "    \"exploratory\": [\"allow\", \"about\", \"around\",\n",
    "                    \"before\", \"black\", \"barely\",\n",
    "                    \"small\", \"said\", \"such\",\n",
    "                    \"please\", \"people\", \"problem\"],    \n",
    "}\n",
    "plot_sets.update({f\"prefix_{prefix}\": [word for word in state_space_spec.labels if word.startswith(prefix)\n",
    "                                       and word_freqs[word] >= pca_freq_min and word_freqs[word] < pca_freq_max]\n",
    "                  for _, _, prefix in best_prefixes})\n",
    "plot_sets.update({f\"suffix_{suffix}\": [word for word in state_space_spec.labels if word.endswith(suffix)\n",
    "                                       and word_freqs[word] >= pca_freq_min and word_freqs[word] < pca_freq_max]\n",
    "                  for _, _, suffix in best_suffixes})\n",
    "\n",
    "plot_words = set(word for words in plot_sets.values() for word in words)\n",
    "if any(word not in state_space_spec.labels for word in plot_words):\n",
    "    raise ValueError(f\"Plot words not found in state space: {plot_words}\")\n",
    "\n",
    "if pca_plot_words_only:\n",
    "    pca_words = plot_words\n",
    "else:\n",
    "    # use all words with frequency between cutoffs to compute PCA\n",
    "    pca_words = sorted([(freq, label) for label, freq in word_freqs.items()\n",
    "                        if freq >= pca_freq_min and freq < pca_freq_max], reverse=True)\n",
    "    pca_words = [label for _, label in pca_words]\n",
    "\n",
    "    for plot_word in plot_words:\n",
    "        if plot_word not in pca_words:\n",
    "            L.warn(f\"Plot word {plot_word} not found in PCA words\")\n",
    "            pca_words.append(plot_word)\n",
    "\n",
    "drop_idxs = [idx for idx, word in enumerate(state_space_spec.labels)\n",
    "             if word not in pca_words]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PCA on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(\n",
    "    model_representations,\n",
    "    state_space_spec,\n",
    "    expand_window=expand_frame_window,\n",
    "    pad=np.nan\n",
    ")\n",
    "# Subsample trajectories to reduce computation time\n",
    "for i in range(len(trajectory)):\n",
    "    if len(trajectory[i]) > pca_max_samples_per_word:\n",
    "        subsample_idxs = np.random.choice(len(trajectory[i]), pca_max_samples_per_word, replace=False)\n",
    "        trajectory[i] = trajectory[i][subsample_idxs]\n",
    "\n",
    "all_trajectories_full = np.concatenate(trajectory)\n",
    "all_trajectories_src = np.array(list(np.ndindex(all_trajectories_full.shape[:2])))\n",
    "\n",
    "# flatten & retain non-padding\n",
    "all_trajectories = all_trajectories_full.reshape(-1, all_trajectories_full.shape[-1])\n",
    "retain_idxs = ~np.isnan(all_trajectories).any(axis=1)\n",
    "all_trajectories = all_trajectories[retain_idxs]\n",
    "all_trajectories_src = all_trajectories_src[retain_idxs]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_trajectories)\n",
    "scaler = StandardScaler().fit(all_trajectories)\n",
    "\n",
    "all_trajectories_pca = pca.transform(scaler.transform(all_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space analysis over plot sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_space(plot_key):\n",
    "    plot_words = plot_sets[plot_key]\n",
    "    state_space_spec_sub = state_space_spec.drop_labels([idx for idx, word in enumerate(state_space_spec.labels) if word not in plot_words])\n",
    "\n",
    "    trajectory = prepare_state_trajectory(\n",
    "        model_representations,\n",
    "        state_space_spec_sub,\n",
    "        expand_window=expand_frame_window,\n",
    "        pad=np.nan\n",
    "    )\n",
    "\n",
    "    # Subsample trajectories to reduce computation time\n",
    "    for i in range(len(trajectory)):\n",
    "        if len(trajectory[i]) > pca_max_samples_per_word:\n",
    "            subsample_idxs = np.random.choice(len(trajectory[i]), pca_max_samples_per_word, replace=False)\n",
    "            trajectory[i] = trajectory[i][subsample_idxs]\n",
    "\n",
    "    all_trajectories_full = np.concatenate(trajectory)\n",
    "    all_trajectories_src = np.array(list(np.ndindex(all_trajectories_full.shape[:2])))\n",
    "\n",
    "    # flatten & retain non-padding\n",
    "    all_trajectories = all_trajectories_full.reshape(-1, all_trajectories_full.shape[-1])\n",
    "    retain_idxs = ~np.isnan(all_trajectories).any(axis=1)\n",
    "    all_trajectories = all_trajectories[retain_idxs]\n",
    "    all_trajectories_src = all_trajectories_src[retain_idxs]\n",
    "\n",
    "    # use previously fit scaler+PCA to transform these representations\n",
    "    all_trajectories_pca = pca.transform(scaler.transform(all_trajectories))\n",
    "\n",
    "    all_trajectories_pca_padded = np.full(all_trajectories_full.shape[:2] + (2,), np.nan)\n",
    "    all_trajectories_pca_padded[all_trajectories_src[:, 0], all_trajectories_src[:, 1]] = all_trajectories_pca\n",
    "\n",
    "    # get index of first nan in each item; back-fill with last value\n",
    "    for idx, nan_onset in enumerate(np.isnan(all_trajectories_pca_padded)[:, :, 0].argmax(axis=1)):\n",
    "        if nan_onset == 0:\n",
    "            continue\n",
    "        all_trajectories_pca_padded[idx, nan_onset:] = all_trajectories_pca_padded[idx, nan_onset - 1]\n",
    "\n",
    "    # plotting helpers\n",
    "    trajectory_dividers = np.cumsum([traj.shape[0] for traj in trajectory])\n",
    "    trajectory_dividers = np.concatenate([[0], trajectory_dividers])\n",
    "    # get just the dividers for plot_words\n",
    "    plot_word_dividers = []\n",
    "    for word in plot_words:\n",
    "        class_idx = state_space_spec_sub.labels.index(word)\n",
    "        left_edge = trajectory_dividers[class_idx]\n",
    "        right_edge = trajectory_dividers[class_idx + 1] if class_idx + 1 < len(trajectory_dividers) else len(all_trajectories_pca_padded)\n",
    "        plot_word_dividers.append((left_edge, right_edge))\n",
    "    \n",
    "    #####\n",
    "\n",
    "    min, max = all_trajectories_pca.min(), all_trajectories_pca.max()\n",
    "\n",
    "    # Animate\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(np.floor(min), np.ceil(max))\n",
    "    ax.set_ylim(np.floor(min), np.ceil(max))\n",
    "    annot_frame = ax.text(-0.75, 0.75, \"-1\")\n",
    "\n",
    "    color_classes = sorted(set(word for word in plot_words))\n",
    "    cmap = sns.color_palette(\"Set1\", len(color_classes))\n",
    "    color_values = {class_: cmap[i] for i, class_ in enumerate(color_classes)}\n",
    "    marker_values = {class_: \"o\" if i % 2 == 0 else \"x\" for i, class_ in enumerate(color_classes)}\n",
    "\n",
    "    scats = [ax.scatter(np.zeros(end - start + 1), np.zeros(end - start + 1),\n",
    "                        alpha=0.5,\n",
    "                        marker=marker_values[word],\n",
    "                        color=color_values[word],\n",
    "                    ) for i, (word, (start, end)) in enumerate(zip(plot_words, plot_word_dividers))]\n",
    "    ax.legend(scats, plot_words, loc=1)\n",
    "\n",
    "    def init():\n",
    "        for scat in scats:\n",
    "            scat.set_offsets(np.zeros((0, 2)))\n",
    "        return tuple(scats)\n",
    "\n",
    "    def update(frame):\n",
    "        for scat, (idx_start, idx_end) in zip(scats, plot_word_dividers):\n",
    "            traj_i = all_trajectories_pca_padded[idx_start:idx_end, frame]\n",
    "            scat.set_offsets(traj_i)\n",
    "            # scat.set_array(np.arange(traj_i.shape[0]))\n",
    "        annot_frame.set_text(str(frame))\n",
    "        return tuple(scats) + (annot_frame,)\n",
    "\n",
    "    # Animate by model frame\n",
    "    num_frames = all_trajectories_pca_padded.shape[1]\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=500,\n",
    "                                init_func=init)\n",
    "    ani.save(Path(output_dir) / f\"state_space-{plot_key}.gif\", writer=\"ffmpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(plot_sets):\n",
    "    plot_state_space(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on trajectory aggregates\n",
    "\n",
    "The brain encoding linking analysis assumes that the mean over representational trajectory is meaningful. Is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fns = [\n",
    "    \"mean\", \"max\", \"last_frame\",\n",
    "    (\"mean_last_k\", 2), (\"mean_last_k\", 5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_aggs = {}\n",
    "pca_agg_results = {}\n",
    "for name in tqdm(agg_fns):\n",
    "    pca_aggs[name] = PCA(n_components=2)\n",
    "    pca_data = np.concatenate(aggregate_state_trajectory(trajectory, state_space_spec, name))\n",
    "    pca_agg_results[name] = pca_aggs[name].fit_transform(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories_pca_padded = np.full(all_trajectories_full.shape[:2] + (2,), np.nan)\n",
    "all_trajectories_pca_padded[all_trajectories_src[:, 0], all_trajectories_src[:, 1]] = all_trajectories_pca\n",
    "\n",
    "# plotting helpers\n",
    "trajectory_dividers = np.cumsum([traj.shape[0] for traj in trajectory])\n",
    "trajectory_dividers = np.concatenate([[0], trajectory_dividers])\n",
    "# get just the dividers for plot_words\n",
    "plot_word_dividers = []\n",
    "for word in plot_words:\n",
    "    class_idx = state_space_spec.labels.index(word)\n",
    "    left_edge = trajectory_dividers[class_idx]\n",
    "    right_edge = trajectory_dividers[class_idx + 1] if class_idx + 1 < len(trajectory_dividers) else len(all_trajectories_pca_padded)\n",
    "    plot_word_dividers.append((left_edge, right_edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_classes = sorted(set(word for word in plot_words))\n",
    "cmap = sns.color_palette(\"Set1\", len(color_classes))\n",
    "color_values = {class_: cmap[i] for i, class_ in enumerate(color_classes)}\n",
    "marker_values = {class_: \"o\" if i % 2 == 0 else \"x\" for i, class_ in enumerate(color_classes)}\n",
    "\n",
    "f, axs = plt.subplots(len(agg_fns), 1, figsize=(7, 5 * len(agg_fns)))\n",
    "for ax, (agg_fn, trajectory_agg_pca) in zip(axs.ravel(), pca_agg_results.items()):\n",
    "    for word in plot_words:\n",
    "        class_idx = state_space_spec.labels.index(word)\n",
    "        traj_i = trajectory_agg_pca[trajectory_dividers[class_idx]:trajectory_dividers[class_idx + 1]]\n",
    "        ax.scatter(*traj_i.T, color=color_values[word], marker=marker_values[word], alpha=0.5,\n",
    "                   label=word)\n",
    "    ax.set_title(agg_fn)\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
