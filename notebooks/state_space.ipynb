{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Union\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "# Add 4 frames prior to onset to each trajectory\n",
    "expand_frame_window = (4, 0)\n",
    "\n",
    "# Only use plot words for PCA or use whole vocabulary?\n",
    "pca_plot_words_only = False\n",
    "# Use words with this many or more instances to estimate embedding PCA space\n",
    "pca_freq_min = 15\n",
    "# Ignore words with this many or more instances when estimating embedding PCA space\n",
    "pca_freq_max = 10000\n",
    "\n",
    "# Use at most this many samples of each word in computing PCA (for computational efficiency)\n",
    "pca_max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best prefixes [(5, 11, 'inter'), (5, 10, 'count'), (4, 23, 'cons'), (4, 22, 'inte'), (4, 20, 'comp'), (4, 18, 'cont'), (4, 17, 'comm'), (4, 12, 'pres')]\n",
      "Best suffixes [(6, 11, 'ection'), (5, 46, 'ation'), (5, 19, 'tions'), (5, 19, 'ction'), (5, 14, 'ssion'), (5, 14, 'ently'), (5, 14, 'ement'), (5, 12, 'ering')]\n"
     ]
    }
   ],
   "source": [
    "# use all words with frequency greater than cutoff to compute PCA\n",
    "word_freqs = {label: len(trajs) for trajs, label in\n",
    "            zip(state_space_spec.target_frame_spans, state_space_spec.labels)}\n",
    "\n",
    "min_suffix_type_count = 10\n",
    "min_prefix_type_count = 10\n",
    "plot_suffixes = 8\n",
    "plot_prefixes = 8\n",
    "\n",
    "# find longest possible suffixes for which we have at least min_suffix_type_count word types matching frequency constraint\n",
    "best_suffixes = []\n",
    "for suffix_length in range(9):\n",
    "    suffix_counts = Counter()\n",
    "    for word in state_space_spec.labels:\n",
    "        if len(word) < suffix_length or word_freqs[word] < pca_freq_min or word_freqs[word] >= pca_freq_max:\n",
    "            continue\n",
    "        suffix_counts[word[-suffix_length:]] += 1\n",
    "\n",
    "    for suffix, count in suffix_counts.most_common():\n",
    "        if count < min_suffix_type_count:\n",
    "            break\n",
    "        best_suffixes.append((suffix_length, count, suffix))\n",
    "best_suffixes = sorted(best_suffixes, reverse=True)[:plot_suffixes]\n",
    "\n",
    "# same for prefixes\n",
    "best_prefixes = []\n",
    "for prefix_length in range(9):\n",
    "    prefix_counts = Counter()\n",
    "    for word in state_space_spec.labels:\n",
    "        if len(word) < prefix_length or word_freqs[word] < pca_freq_min or word_freqs[word] >= pca_freq_max:\n",
    "            continue\n",
    "        prefix_counts[word[:prefix_length]] += 1\n",
    "\n",
    "    for prefix, count in prefix_counts.most_common():\n",
    "        if count < min_prefix_type_count:\n",
    "            break\n",
    "        best_prefixes.append((prefix_length, count, prefix))\n",
    "best_prefixes = sorted(best_prefixes, reverse=True)[:plot_prefixes]\n",
    "\n",
    "print(\"Best prefixes\", best_prefixes)\n",
    "print(\"Best suffixes\", best_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sets = {\n",
    "    \"exploratory\": [\"allow\", \"about\", \"around\",\n",
    "                    \"before\", \"black\", \"barely\",\n",
    "                    \"small\", \"said\", \"such\",\n",
    "                    \"please\", \"people\", \"problem\"],    \n",
    "}\n",
    "plot_sets.update({f\"prefix_{prefix}\": [word for word in state_space_spec.labels if word.startswith(prefix)\n",
    "                                       and word_freqs[word] >= pca_freq_min and word_freqs[word] < pca_freq_max]\n",
    "                  for _, _, prefix in best_prefixes})\n",
    "plot_sets.update({f\"suffix_{suffix}\": [word for word in state_space_spec.labels if word.endswith(suffix)\n",
    "                                       and word_freqs[word] >= pca_freq_min and word_freqs[word] < pca_freq_max]\n",
    "                  for _, _, suffix in best_suffixes})\n",
    "\n",
    "plot_words = set(word for words in plot_sets.values() for word in words)\n",
    "if any(word not in state_space_spec.labels for word in plot_words):\n",
    "    raise ValueError(f\"Plot words not found in state space: {plot_words}\")\n",
    "\n",
    "if pca_plot_words_only:\n",
    "    pca_words = plot_words\n",
    "else:\n",
    "    # use all words with frequency between cutoffs to compute PCA\n",
    "    pca_words = sorted([(freq, label) for label, freq in word_freqs.items()\n",
    "                        if freq >= pca_freq_min and freq < pca_freq_max], reverse=True)\n",
    "    pca_words = [label for _, label in pca_words]\n",
    "\n",
    "    for plot_word in plot_words:\n",
    "        if plot_word not in pca_words:\n",
    "            L.warn(f\"Plot word {plot_word} not found in PCA words\")\n",
    "            pca_words.append(plot_word)\n",
    "\n",
    "drop_idxs = [idx for idx, word in enumerate(state_space_spec.labels)\n",
    "             if word not in pca_words]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space_spec = state_space_spec.subsample_instances(pca_max_samples_per_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PCA on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(\n",
    "    model_representations,\n",
    "    state_space_spec,\n",
    "    expand_window=expand_frame_window,\n",
    "    pad=np.nan\n",
    ")\n",
    "all_trajectories, all_trajectories_src = flatten_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), PCA(n_components=2))\n",
    "all_trajectories_pca = pipeline.fit_transform(all_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space analysis over plot sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_space(plot_key):\n",
    "    plot_words = plot_sets[plot_key]\n",
    "    state_space_spec_sub = state_space_spec.drop_labels([idx for idx, word in enumerate(state_space_spec.labels) if word not in plot_words])\n",
    "\n",
    "    trajectory = prepare_state_trajectory(\n",
    "        model_representations,\n",
    "        state_space_spec_sub,\n",
    "        expand_window=expand_frame_window,\n",
    "        pad=np.nan\n",
    "    )\n",
    "\n",
    "    # Subsample trajectories to reduce computation time\n",
    "    for i in range(len(trajectory)):\n",
    "        if len(trajectory[i]) > pca_max_samples_per_word:\n",
    "            subsample_idxs = np.random.choice(len(trajectory[i]), pca_max_samples_per_word, replace=False)\n",
    "            trajectory[i] = trajectory[i][subsample_idxs]\n",
    "\n",
    "    all_trajectories_full = np.concatenate(trajectory)\n",
    "    all_trajectories_src = np.array(list(np.ndindex(all_trajectories_full.shape[:2])))\n",
    "\n",
    "    # flatten & retain non-padding\n",
    "    all_trajectories = all_trajectories_full.reshape(-1, all_trajectories_full.shape[-1])\n",
    "    retain_idxs = ~np.isnan(all_trajectories).any(axis=1)\n",
    "    all_trajectories = all_trajectories[retain_idxs]\n",
    "    all_trajectories_src = all_trajectories_src[retain_idxs]\n",
    "\n",
    "    # use previously fit scaler+PCA to transform these representations\n",
    "    all_trajectories_pca = pipeline.transform(all_trajectories)\n",
    "\n",
    "    all_trajectories_pca_padded = np.full(all_trajectories_full.shape[:2] + (2,), np.nan)\n",
    "    all_trajectories_pca_padded[all_trajectories_src[:, 0], all_trajectories_src[:, 1]] = all_trajectories_pca\n",
    "\n",
    "    # get index of first nan in each item; back-fill with last value\n",
    "    for idx, nan_onset in enumerate(np.isnan(all_trajectories_pca_padded)[:, :, 0].argmax(axis=1)):\n",
    "        if nan_onset == 0:\n",
    "            continue\n",
    "        all_trajectories_pca_padded[idx, nan_onset:] = all_trajectories_pca_padded[idx, nan_onset - 1]\n",
    "\n",
    "    # plotting helpers\n",
    "    trajectory_dividers = np.cumsum([traj.shape[0] for traj in trajectory])\n",
    "    trajectory_dividers = np.concatenate([[0], trajectory_dividers])\n",
    "    # get just the dividers for plot_words\n",
    "    plot_word_dividers = []\n",
    "    for word in plot_words:\n",
    "        class_idx = state_space_spec_sub.labels.index(word)\n",
    "        left_edge = trajectory_dividers[class_idx]\n",
    "        right_edge = trajectory_dividers[class_idx + 1] if class_idx + 1 < len(trajectory_dividers) else len(all_trajectories_pca_padded)\n",
    "        plot_word_dividers.append((left_edge, right_edge))\n",
    "    \n",
    "    #####\n",
    "\n",
    "    min, max = all_trajectories_pca.min(), all_trajectories_pca.max()\n",
    "\n",
    "    # Animate\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(np.floor(min), np.ceil(max))\n",
    "    ax.set_ylim(np.floor(min), np.ceil(max))\n",
    "    annot_frame = ax.text(-0.75, 0.75, \"-1\")\n",
    "\n",
    "    color_classes = sorted(set(word for word in plot_words))\n",
    "    cmap = sns.color_palette(\"Set1\", len(color_classes))\n",
    "    color_values = {class_: cmap[i] for i, class_ in enumerate(color_classes)}\n",
    "    marker_values = {class_: \"o\" if i % 2 == 0 else \"x\" for i, class_ in enumerate(color_classes)}\n",
    "\n",
    "    scats = [ax.scatter(np.zeros(end - start + 1), np.zeros(end - start + 1),\n",
    "                        alpha=0.5,\n",
    "                        marker=marker_values[word],\n",
    "                        color=color_values[word],\n",
    "                    ) for i, (word, (start, end)) in enumerate(zip(plot_words, plot_word_dividers))]\n",
    "    ax.legend(scats, plot_words, loc=1)\n",
    "\n",
    "    def init():\n",
    "        for scat in scats:\n",
    "            scat.set_offsets(np.zeros((0, 2)))\n",
    "        return tuple(scats)\n",
    "\n",
    "    def update(frame):\n",
    "        for scat, (idx_start, idx_end) in zip(scats, plot_word_dividers):\n",
    "            traj_i = all_trajectories_pca_padded[idx_start:idx_end, frame]\n",
    "            scat.set_offsets(traj_i)\n",
    "            # scat.set_array(np.arange(traj_i.shape[0]))\n",
    "        annot_frame.set_text(str(frame))\n",
    "        return tuple(scats) + (annot_frame,)\n",
    "\n",
    "    # Animate by model frame\n",
    "    num_frames = all_trajectories_pca_padded.shape[1]\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=500,\n",
    "                                init_func=init)\n",
    "    ani.save(Path(output_dir) / f\"state_space-{plot_key}.gif\", writer=\"ffmpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(plot_sets):\n",
    "    plot_state_space(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_agg_phoneme = aggregate_state_trajectory(trajectory, state_space_spec, (\"mean_within_cut\", \"phoneme\"), keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_agg_phoneme_flat, traj_agg_phoneme_src = flatten_trajectory(traj_agg_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), PCA(n_components=min(traj_agg_phoneme_flat.shape[1], 4)))\n",
    "traj_agg_phoneme_pca = pipeline.fit_transform(traj_agg_phoneme_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quiver(group_spec: Union[list[str], dict[str, list[str]]],\n",
    "                traj_flat, traj_flat_src, state_space_spec, plot_cut_description=\"phoneme\",\n",
    "                legend=True, ax=None):\n",
    "    if isinstance(group_spec, list):\n",
    "        quiver_groups = {key: plot_sets[key] for key in group_spec}\n",
    "    else:\n",
    "        quiver_groups = group_spec\n",
    "\n",
    "    palette = sns.color_palette(\"Set1\")\n",
    "    get_color = lambda idx: palette[idx % len(palette)]\n",
    "\n",
    "    if plot_cut_description is not None:\n",
    "        # Prepare cuts annotation\n",
    "        cuts_df = state_space_spec.cuts.xs(plot_cut_description, level=\"level\") \\\n",
    "            .drop(columns=[\"onset_frame_idx\", \"offset_frame_idx\"])\n",
    "        cuts_df[\"label_idx\"] = cuts_df.index.get_level_values(\"label\") \\\n",
    "            .map({label: idx for idx, label in enumerate(state_space_spec.labels)})\n",
    "        cuts_df[\"frame_idx\"] = cuts_df.groupby([\"label\", \"instance_idx\"]).cumcount()\n",
    "        cuts_df = cuts_df.reset_index().set_index([\"label_idx\", \"instance_idx\", \"frame_idx\"])\n",
    "\n",
    "    max_num_frames = traj_flat_src[:, 2].max() + 1\n",
    "    quiver_data, quiver_data_src = {}, {}\n",
    "    for group, words in tqdm(quiver_groups.items(), unit=\"group\", leave=False):\n",
    "        quiver_data_i, quiver_data_src_i = [], []\n",
    "        word_idxs = [state_space_spec.labels.index(word) for word in words]\n",
    "        mask = np.isin(traj_flat_src[:, 0], word_idxs)\n",
    "        for j in trange(max_num_frames):\n",
    "            mask_j = mask & (traj_flat_src[:, 2] == j)\n",
    "            if not mask_j.any():\n",
    "                break\n",
    "            quiver_data_i.append(traj_flat[mask_j])\n",
    "            quiver_data_src_i.append(traj_flat_src[mask_j])\n",
    "\n",
    "        quiver_data[group] = quiver_data_i\n",
    "        quiver_data_src[group] = quiver_data_src_i\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    for i, group in enumerate(quiver_data):\n",
    "        data = quiver_data[group]\n",
    "        group_src = quiver_data_src[group]\n",
    "\n",
    "        frame_means = np.array([np.nanmean(frame, axis=0) for frame in data])\n",
    "        frame_sems = np.array([np.nanstd(frame, axis=0) / np.sqrt(np.sum(~np.isnan(frame).any(axis=1), axis=0))\n",
    "                               for frame in data])\n",
    "        frame_counts = np.array([np.sum(~np.isnan(frame).any(axis=1), axis=0) for frame in data])\n",
    "        \n",
    "        # quiver using means\n",
    "        ax.quiver(frame_means[:-1, 0], frame_means[:-1, 1],\n",
    "                frame_means[1:, 0] - frame_means[:-1, 0],\n",
    "                frame_means[1:, 1] - frame_means[:-1, 1],\n",
    "                linewidths=frame_counts[:-1] / frame_counts.max() * 3,\n",
    "                edgecolors=get_color(i),\n",
    "                angles=\"xy\", scale_units=\"xy\", scale=1.2,\n",
    "                color=get_color(i),\n",
    "                label=group)\n",
    "        \n",
    "        if plot_cut_description is not None:\n",
    "            # transform axis coordinate jitter to data space\n",
    "            jitter_magnitude = 0.05\n",
    "            jitter_x_scale = 0.05 # (np.nanmax(frame_means[:, 0]) - np.nanmin(frame_means[:, 0])) * jitter_magnitude\n",
    "            jitter_y_scale = 0.05 # (np.nanmax(frame_means[:, 1]) - np.nanmin(frame_means[:, 1])) * jitter_magnitude\n",
    "            \n",
    "            for j, group_src_j in enumerate(group_src):\n",
    "                descriptions_j = cuts_df.loc[group_src_j[:, 0], group_src_j[:, 1], group_src_j[:, 2]].description.value_counts()\n",
    "                descriptions_j /= descriptions_j.sum()\n",
    "                # plot each description with jitter centered around frame mean; size proportional to relative frequency\n",
    "                for k, (description, freq) in enumerate(descriptions_j.head(3).items()):\n",
    "                    ax.text(frame_means[j, 0] + np.random.randn() * jitter_x_scale,\n",
    "                            frame_means[j, 1] + np.random.randn() * jitter_y_scale,\n",
    "                            description,\n",
    "                            size=4 + 8 * freq,\n",
    "                            color=get_color(i),\n",
    "                            transform=ax.transData)\n",
    "        \n",
    "        # plot shaded region using sem\n",
    "        for j1, j2 in zip(range(len(data) - 1), range(1, len(data))):\n",
    "            polygon_edges = np.array([\n",
    "                [frame_means[j1, 0] - frame_sems[j1, 0], frame_means[j1, 1]],\n",
    "                [frame_means[j2, 0] - frame_sems[j2, 0], frame_means[j2, 1]],\n",
    "                [frame_means[j2, 0] + frame_sems[j2, 0], frame_means[j2, 1]],\n",
    "                [frame_means[j1, 0] + frame_sems[j1, 0], frame_means[j1, 1]],\n",
    "            ])\n",
    "            polygon = plt.Polygon(polygon_edges, alpha=0.2, color=get_color(i))\n",
    "            ax.add_patch(polygon)\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "\n",
    "    return quiver_data, quiver_data_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quiver([x for x in plot_sets.keys() if x.startswith(\"prefix\")],\n",
    "            traj_agg_phoneme_pca[:, 0:2], traj_agg_phoneme_src, state_space_spec)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prefixes = [key for key in plot_sets.keys() if key.startswith(\"prefix\")]\n",
    "num_cols = 3\n",
    "num_rows = int(np.ceil(len(all_prefixes) / num_cols))\n",
    "f, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))\n",
    "\n",
    "for prefix_key, ax in zip(tqdm(all_prefixes), axs.flat):\n",
    "    plot_quiver({word: [word] for word in plot_sets[prefix_key]},\n",
    "                traj_agg_phoneme_pca[:, 0:2], traj_agg_phoneme_src, state_space_spec, ax=ax, legend=False)\n",
    "    ax.set_title(prefix_key)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traj_agg_phoneme_pca.shape[1] > 2:\n",
    "    start_pc = min(2, traj_agg_phoneme_pca.shape[1] - 2)\n",
    "\n",
    "    all_prefixes = [key for key in plot_sets.keys() if key.startswith(\"prefix\")]\n",
    "    num_cols = 3\n",
    "    num_rows = int(np.ceil(len(all_prefixes) / num_cols))\n",
    "    f, axs = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))\n",
    "\n",
    "    for prefix_key, ax in zip(tqdm(all_prefixes), axs.flat):\n",
    "        plot_quiver({word: [word] for word in plot_sets[prefix_key]},\n",
    "                    traj_agg_phoneme_pca[:, start_pc:start_pc + 2], traj_agg_phoneme_src, state_space_spec, ax=ax, legend=False)\n",
    "        ax.set_title(prefix_key)\n",
    "        ax.set_xlabel(f\"PC{start_pc}\")\n",
    "        ax.set_ylabel(f\"PC{start_pc + 1}\")\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison: plot K equivalent groups of random words\n",
    "baseline_group_spec = {\n",
    "    f\"random_{i}\": np.random.choice(state_space_spec.labels, 12, replace=False)\n",
    "    for i in range(8)\n",
    "}\n",
    "plot_quiver(baseline_group_spec,\n",
    "            traj_agg_phoneme_pca[:, 0:2], traj_agg_phoneme_src, state_space_spec)\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
