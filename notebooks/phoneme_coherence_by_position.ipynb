{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import replace\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis import coherence\n",
    "from src.analysis.state_space import prepare_state_trajectory, StateSpaceAnalysisSpec\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_dir = \"outputs/models/timit/w2v2_6/rnn_8/phoneme\"\n",
    "output_dir = \"outputs/notebooks/timit/w2v2_6/rnn_8/phoneme/plot\"\n",
    "dataset_path = \"outputs/preprocessed_data/timit\"\n",
    "equivalence_path = \"outputs/equivalence_datasets/timit/w2v2_6/phoneme/equivalence.pkl\"\n",
    "hidden_states_path = \"outputs/hidden_states/timit/w2v2_6/hidden_states.h5\"\n",
    "state_space_specs_path = \"outputs/state_space_specs/timit/w2v2_6/state_space_specs.pkl\"\n",
    "embeddings_path = \"outputs/model_embeddings/timit/w2v2_6/rnn_8/phoneme/embeddings.npy\"\n",
    "\n",
    "output_dir = \".\"\n",
    "\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "with open(state_space_specs_path, \"rb\") as f:\n",
    "    state_space_spec: StateSpaceAnalysisSpec = torch.load(f)[\"phoneme_by_syllable_position_and_identity\"]\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for well-attested phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build representation of all phoneme identities/positions\n",
    "all_phonemes = sorted(set(phoneme for phoneme, position in state_space_spec.labels))\n",
    "all_positions = sorted(set(position for phoneme, position in state_space_spec.labels if position is not None))\n",
    "phoneme_mat = np.zeros((len(all_phonemes), len(all_positions)), dtype=int)\n",
    "for i, (phoneme, position) in enumerate(state_space_spec.labels):\n",
    "    if position is None:\n",
    "        continue\n",
    "    phoneme_mat[all_phonemes.index(phoneme), all_positions.index(position)] = \\\n",
    "        len(state_space_spec.target_frame_spans[i])\n",
    "phoneme_df = pd.DataFrame(phoneme_mat, index=all_phonemes, columns=all_positions)\n",
    "phoneme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find phonemes which appear in every ordinal position at least twice up to `min_number_positions`\n",
    "min_number_positions = 3\n",
    "phoneme_max_position = (phoneme_df >= 2).idxmin(axis=1)\n",
    "match_phonemes = phoneme_max_position.loc[phoneme_max_position >= min_number_positions].index.tolist()\n",
    "len(match_phonemes), match_phonemes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=phoneme_df.loc[match_phonemes, :min_number_positions] \\\n",
    "                    .melt(var_name=\"ordinal_position\", value_name=\"frequency\"),\n",
    "            x=\"ordinal_position\", y=\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_labels = [(phoneme, ordinal) for phoneme in match_phonemes\n",
    "                 for ordinal in range(min_number_positions)]\n",
    "drop_idxs = [idx for idx, label in enumerate(state_space_spec.labels)\n",
    "             if label not in retain_labels]\n",
    "state_space_spec = state_space_spec.drop_labels(drop_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_label_strs = [f\"{phone} {ordinal}\" for phone, ordinal in state_space_spec.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]\n",
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate within-phoneme, within-position distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance, within_distance_offset = \\\n",
    "    coherence.estimate_within_distance(trajectory, lengths, state_space_spec, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(within_distance, center=1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_df = pd.DataFrame(within_distance, index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_distance_offset_df = pd.DataFrame(within_distance_offset, index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate within-phoneme, between-position distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between1_samples = [[state_space_spec.labels.index((phoneme_i, ordinal_j))\n",
    "                     for ordinal_j in range(min_number_positions)\n",
    "                     if ordinal_j != ordinal_i]\n",
    "                    for phoneme_i, ordinal_i in state_space_spec.labels]\n",
    "\n",
    "between1_distances, between1_distances_offset = \\\n",
    "    coherence.estimate_between_distance(trajectory, lengths, state_space_spec,\n",
    "                                        between1_samples, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between1_distances_df = pd.DataFrame(np.nanmean(between1_distances, axis=-1),\n",
    "                                     index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between1_distances_offset_df = pd.DataFrame(np.nanmean(between1_distances_offset, axis=-1),\n",
    "                                     index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate between-phoneme distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the number of between-samples with the earlier analysis\n",
    "num_samples = min_number_positions\n",
    "\n",
    "between_distances, between_distances_offset = \\\n",
    "    coherence.estimate_between_distance(trajectory, lengths, state_space_spec,\n",
    "                                        num_samples=num_samples, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_df = pd.DataFrame(np.nanmean(between_distances, axis=-1),\n",
    "                                    index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_distances_offset_df = pd.DataFrame(np.nanmean(between_distances_offset, axis=-1),\n",
    "                                    index=pd.Index(spec_label_strs, name=\"phoneme\")) \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=[\"phoneme\"], var_name=\"frame\", value_name=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([within_distance_df.assign(type=\"within\"),\n",
    "                       between1_distances_df.assign(type=\"different_position\"),\n",
    "                       between_distances_df.assign(type=\"between\")])\n",
    "merged_df.to_csv(Path(output_dir) / \"distances.csv\", index=False)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=merged_df.dropna().replace({\"type\": {\"within\": \"Same identity, same position\",\n",
    "                                                            \"different_position\": \"Same identity, different position\",\n",
    "                                                            \"between\": \"Different identity, different position\"}}),\n",
    "                  x=\"frame\", y=\"distance\", hue=\"type\")\n",
    "ax.set_title(\"Representational distance within- and between-phoneme\")\n",
    "ax.set_xlabel(\"Frames since phoneme onset\")\n",
    "ax.set_ylabel(f\"{metric.capitalize()} distance\")\n",
    "ax.set_xlim((0, np.percentile(np.concatenate(lengths), 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_offset_df = pd.concat([within_distance_offset_df.assign(type=\"within\"),\n",
    "                       between1_distances_offset_df.assign(type=\"different_position\"),\n",
    "                       between_distances_offset_df.assign(type=\"between\")])\n",
    "merged_offset_df.to_csv(Path(output_dir) / \"distances_aligned_offset.csv\", index=False)\n",
    "merged_offset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
