{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import replace\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.analysis.state_space import StateSpaceAnalysisSpec, \\\n",
    "    prepare_state_trajectory, aggregate_state_trajectory, flatten_trajectory\n",
    "from src.datasets.speech_equivalence import SpeechEquivalenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_model = \"w2v2_8\"\n",
    "model_class = \"rnn_32-hinge-mAP4\"\n",
    "model_name = \"word_broad\"\n",
    "train_dataset = \"librispeech-train-clean-100\"\n",
    "model_dir = f\"outputs/models/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames\"\n",
    "output_dir = f\".\"\n",
    "dataset_path = f\"outputs/preprocessed_data/{train_dataset}\"\n",
    "equivalence_path = f\"outputs/equivalence_datasets/{train_dataset}/{base_model}/{model_name}_10frames/equivalence.pkl\"\n",
    "hidden_states_path = f\"outputs/hidden_states/{train_dataset}/{base_model}/{train_dataset}.h5\"\n",
    "state_space_specs_path = f\"outputs/state_space_specs/{train_dataset}/{base_model}/state_space_specs.h5\"\n",
    "embeddings_path = f\"outputs/model_embeddings/{train_dataset}/{base_model}/{model_class}/{model_name}_10frames/{train_dataset}.npy\"\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "max_samples_per_word = 100\n",
    "\n",
    "metric = \"cosine\"\n",
    "\n",
    "agg_fns = [\n",
    "    \"mean\", \"max\", \"last_frame\",\n",
    "    (\"mean_last_k\", 2), (\"mean_last_k\", 5),\n",
    "    (\"mean_first_k\", 10),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_path, \"rb\") as f:\n",
    "    model_representations: np.ndarray = np.load(f)\n",
    "state_space_spec = StateSpaceAnalysisSpec.from_hdf5(state_space_specs_path, \"word\")\n",
    "assert state_space_spec.is_compatible_with(model_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = prepare_state_trajectory(model_representations, state_space_spec, pad=np.nan)\n",
    "\n",
    "# Subsample trajectories to reduce computation time\n",
    "for i in range(len(trajectory)):\n",
    "    if len(trajectory[i]) > max_samples_per_word:\n",
    "        subsample_idxs = np.random.choice(len(trajectory[i]), max_samples_per_word, replace=False)\n",
    "        trajectory[i] = trajectory[i][subsample_idxs]\n",
    "\n",
    "lengths = [np.isnan(traj_i[:, :, 0]).argmax(axis=1) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs = {agg_fn: aggregate_state_trajectory(trajectory, state_space_spec, agg_fn, keepdims=True)\n",
    "                   for agg_fn in tqdm(agg_fns)}\n",
    "dummy_lengths = [np.ones(len(traj_i), dtype=int) for traj_i in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat = {k: flatten_trajectory(v) for k, v in trajectory_aggs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_aggs_flat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare quantitative tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_clustering_coefficients(xs, agg_method, k=5, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Compute local clustering coefficients for each point in the collection `xs`.\n",
    "\n",
    "    The local clustering coefficient measures the density of connections between the\n",
    "    neighbors of a point.\n",
    "    \n",
    "    Args:\n",
    "    - xs: np.ndarray, shape (n_samples, n_features)\n",
    "        Collection of points to evaluate.\n",
    "    - agg_method: str\n",
    "    \"\"\"\n",
    "    assert xs.ndim == 2\n",
    "    references, references_src = trajectory_aggs_flat[agg_method]\n",
    "    assert xs.shape[1] == references.shape[1]\n",
    "\n",
    "    # compute K nearest neighbors for each point in `xs`\n",
    "    dists = cdist(xs, references, metric=metric)\n",
    "    neighbors = np.argsort(dists, axis=1)[:, :k]\n",
    "\n",
    "    # find neighbors of each of these neighbors\n",
    "    neighbor_embeddings = references[neighbors]\n",
    "    neighbor_dists = cdist(neighbor_embeddings.reshape(-1, neighbor_embeddings.shape[-1]),\n",
    "                           references, metric=metric)\n",
    "    # skip the first neighbor since we know that the distance is 0\n",
    "    neighbor_neighbors = np.argsort(neighbor_dists, axis=1)[:, 1:k+1] \\\n",
    "        .reshape(neighbor_embeddings.shape[0], k, k)\n",
    "\n",
    "    # compute local clustering coefficients\n",
    "    local_clustering_coeffs = np.zeros(xs.shape[0])\n",
    "    for i, neighbors_i, neighbor_neighbors_i in zip(range(xs.shape[0]), neighbors, neighbor_neighbors):\n",
    "        n_triangles = 0\n",
    "        for j, (neighbor_ij, neighbor_neighbors_ij) in enumerate(zip(neighbors_i, neighbor_neighbors_i)):\n",
    "            n_triangles += len(np.intersect1d(neighbor_neighbors_ij, neighbors_i[:j]))\n",
    "        \n",
    "        n_neighbors = len(neighbors_i)\n",
    "        max_possible_triangles = n_neighbors * (n_neighbors - 1)\n",
    "        local_clustering_coeffs[i] = n_triangles / max_possible_triangles\n",
    "\n",
    "    return local_clustering_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate KDEs for each trajectory agg\n",
    "kde_models = {agg_method: KernelDensity(bandwidth=0.1).fit(trajectory_aggs_flat[agg_method][0] / np.linalg.norm(trajectory_aggs_flat[agg_method][0], axis=1, keepdims=True))\n",
    "              for agg_method in tqdm(trajectory_aggs_flat)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search for hand-picked pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"the\", \"pomegranate\"),\n",
    "    (\"ice\", \"nice\"),\n",
    "    (\"eyes\", \"yes\"),\n",
    "    (\"supervision\", \"vision\"),\n",
    "    (\"eyes\", \"eye\"),\n",
    "    (\"boys\", \"boy\"),\n",
    "    (\"girls\", \"girl\"),\n",
    "    (\"say\", \"ace\"),\n",
    "    (\"says\", \"say\"),\n",
    "    (\"reign\", \"rain\"),\n",
    "]\n",
    "\n",
    "agg_method = ('mean_first_k', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_line_search(word_pair, agg_method, step_norm=0.2, k=5, verbose=False, ax=None):\n",
    "    start_word, end_word = word_pair\n",
    "\n",
    "    assert start_word in state_space_spec.labels\n",
    "    assert end_word in state_space_spec.labels\n",
    "\n",
    "    start_word_idx = state_space_spec.labels.index(start_word)\n",
    "    end_word_idx = state_space_spec.labels.index(end_word)\n",
    "\n",
    "    start_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[start_word_idx])))\n",
    "    end_instance = np.random.choice(min(max_samples_per_word, len(state_space_spec.target_frame_spans[end_word_idx])))\n",
    "\n",
    "    start_traj = trajectory_aggs[agg_method][start_word_idx][start_instance].squeeze()\n",
    "    end_traj = trajectory_aggs[agg_method][end_word_idx][end_instance].squeeze()\n",
    "\n",
    "    # navigate from start_traj to end_traj by steps of norm step_norm\n",
    "    step_vector = (end_traj - start_traj) / np.linalg.norm(end_traj - start_traj)\n",
    "    xs = [start_traj]\n",
    "    while np.linalg.norm(xs[-1] - end_traj) > step_norm:\n",
    "        xs.append(xs[-1] + step_vector * step_norm)\n",
    "    assert np.allclose(xs[0], start_traj)\n",
    "    xs = np.array(xs)\n",
    "\n",
    "    assert xs.ndim == 2\n",
    "    references, references_src = trajectory_aggs_flat[agg_method]\n",
    "\n",
    "    # compute nearest neighbors at each step\n",
    "    dists = cdist(xs, references, metric=metric)\n",
    "    ranks = dists.argsort(axis=1)\n",
    "\n",
    "    # compute local clustering coefficients at each step\n",
    "    local_clustering_coeffs = get_local_clustering_coefficients(\n",
    "        xs, agg_method, k=10, metric=\"cosine\")\n",
    "    # # DEV\n",
    "    # local_clustering_coeffs = np.zeros(len(xs))\n",
    "\n",
    "    # compute density estimates at each step\n",
    "    log_densities = kde_models[agg_method].score_samples(xs / np.linalg.norm(xs, axis=1, keepdims=True))\n",
    "\n",
    "    # prepare labeled results at each step\n",
    "    word_results = {}\n",
    "    # prepare a long dataframe as well\n",
    "    neighbors_df = []\n",
    "    metrics_df = []\n",
    "    for i, step_results in enumerate(references_src[ranks]):\n",
    "        word_dist_results = defaultdict(list)\n",
    "        for j, (label_idx, instance_idx, _) in enumerate(step_results):\n",
    "            word_dist_results[state_space_spec.labels[label_idx]].append(dists[i, j])\n",
    "\n",
    "            if len(word_dist_results) > k:\n",
    "                break\n",
    "\n",
    "        word_results[i] = word_dist_results\n",
    "\n",
    "        for word, dists_ij in word_dist_results.items():\n",
    "            for dist in dists_ij:\n",
    "                neighbors_df.append({\n",
    "                    \"step\": i,\n",
    "                    \"word\": word,\n",
    "                    \"dist\": dist,\n",
    "                })\n",
    "        metrics_df.append({\"step\": i,\n",
    "                           \"log_density\": log_densities[i],\n",
    "                           \"local_clustering_coeff\": local_clustering_coeffs[i]})\n",
    "\n",
    "    if verbose:\n",
    "        for i, step_word_results in enumerate(word_results):\n",
    "            print(i, log_densities[i], local_clustering_coeffs[i])\n",
    "\n",
    "            for word, dists in word_results[step_word_results].items():\n",
    "                print(word, len(dists), np.median(dists))\n",
    "            print()\n",
    "\n",
    "    if ax is not None:\n",
    "        ax.plot(local_clustering_coeffs, color=\"blue\")\n",
    "        ax.set_ylabel(\"local clustering coeff\", color=\"blue\")\n",
    "        ax.set_xlabel(\"step\")\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(log_densities, color=\"red\")\n",
    "        ax2.set_ylabel(\"log density\", color=\"red\")\n",
    "\n",
    "        # plot start and end word label\n",
    "        # use ax.transAxes\n",
    "        ax.text(0.05, 0.05, start_word, transform=ax.transAxes)\n",
    "        ax.text(0.95, 0.95, end_word, transform=ax.transAxes)\n",
    "\n",
    "        # at each step, plot the label of the nearest neighbor\n",
    "        for i, step_results in word_results.items():\n",
    "            ax.text(i, 0.5, list(step_results.keys())[0], ha=\"center\", va=\"center\",\n",
    "                    color=\"black\", rotation=90,\n",
    "                    transform=transforms.blended_transform_factory(ax.transData, ax.transAxes))\n",
    "        \n",
    "    return pd.DataFrame(neighbors_df), pd.DataFrame(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = [None] * len(word_pairs)\n",
    "\n",
    "all_neighbors_df, all_metrics_df = [], []\n",
    "\n",
    "for i, (ax, word_pair) in enumerate(zip(axs, tqdm(word_pairs))):\n",
    "    try:\n",
    "        neighbors_df, metrics_df = run_line_search(word_pair, agg_method)#, ax=ax)\n",
    "    except AssertionError:\n",
    "        # missing word\n",
    "        continue\n",
    "\n",
    "    all_neighbors_df.append(neighbors_df)\n",
    "    all_metrics_df.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_keys = keys=[f\"{start_word}-{end_word}\" for start_word, end_word in word_pairs]\n",
    "all_neighbors_df = pd.concat(all_neighbors_df, names=[\"word_pair\"], keys=concat_keys)\n",
    "all_metrics_df = pd.concat(all_metrics_df, names=[\"word_pair\"], keys=concat_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_df = all_metrics_df.droplevel(-1).reset_index().melt(id_vars=[\"word_pair\", \"step\"])\n",
    "# normalize values within variable\n",
    "plot_metrics_df[\"value\"] = plot_metrics_df.groupby(\"variable\")[\"value\"].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "top_neighbors_df = all_neighbors_df.droplevel(-1).groupby([\"word_pair\", \"step\"]).head(1)\n",
    "\n",
    "with sns.plotting_context(\"talk\", font_scale=2):\n",
    "    g = sns.relplot(data=plot_metrics_df,\n",
    "                col=\"word_pair\", col_wrap=2, aspect=2.5, height=6,\n",
    "                hue=\"variable\", x=\"step\", y=\"value\", kind=\"line\")\n",
    "    \n",
    "    for (row, col, hue), data in g.facet_data():\n",
    "        word_pair = data.iloc[0].word_pair\n",
    "        ax = g.facet_axis(row, col)\n",
    "\n",
    "        top_neighbors_i = top_neighbors_df.loc[word_pair]\n",
    "        if isinstance(top_neighbors_i, pd.Series):\n",
    "            top_neighbors_i = pd.DataFrame([top_neighbors_i])\n",
    "        for j, (_, row) in enumerate(top_neighbors_i.iterrows()):\n",
    "            ax.text(j, 0.5, row[\"word\"], ha=\"center\", va=\"center\",\n",
    "                    color=\"black\", rotation=90, alpha=0.4,\n",
    "                    transform=transforms.blended_transform_factory(ax.transData, ax.transAxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom-up line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pairs = 100\n",
    "# min_word_freq = 100\n",
    "\n",
    "# word_freqs = state_space_spec.label_counts\n",
    "# candidate_words = word_freqs[word_freqs >= min_word_freq].index\n",
    "\n",
    "# word_pairs_bu = [np.random.choice(len(candidate_words), 2, replace=False) for _ in range(num_pairs)]\n",
    "# word_pairs_bu = [(candidate_words[start_word_idx], candidate_words[end_word_idx]) for start_word_idx, end_word_idx in word_pairs_bu]\n",
    "# word_pairs_bu[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do the above but with Dask\n",
    "# if \"client\" not in locals():\n",
    "#     cluster = LocalCluster(n_workers=16)\n",
    "#     client = Client(cluster)\n",
    "\n",
    "# @dask.delayed\n",
    "# def run_line_search_dask(word_pair, agg_method, step_norm=0.2, k=5, verbose=False):\n",
    "#     return run_line_search(word_pair, agg_method, step_norm, k, verbose)\n",
    "\n",
    "# promises = [run_line_search_dask(word_pair, agg_method) for word_pair in word_pairs_bu]\n",
    "# results = dask.compute(*promises, scheduler=\"processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_keys = keys=[f\"{start_word}-{end_word}\" for start_word, end_word in word_pairs_bu]\n",
    "# all_metrics_bu_df = pd.concat([metrics_df for _, metrics_df in results], names=[\"word_pair\"], keys=concat_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # block line search trajectories by length, then cluster trajectories according to their metrics\n",
    "# # (local clustering coefficient, log density)\n",
    "# # then visualize the trajectories of each cluster\n",
    "# line_search_lengths = all_metrics_bu_df.groupby(\"word_pair\").size()\n",
    "# _, length_blocks = np.histogram(line_search_lengths, bins=3)\n",
    "# length_blocks = np.ceil(length_blocks).astype(int)\n",
    "# length_block_assignments = np.digitize(line_search_lengths, bins=length_blocks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
